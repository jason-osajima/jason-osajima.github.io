<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="author" content="Jason Osajima">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width">
        <title>word2vec | Jason {osa-jima}</title>

	<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
	<link rel="icon" href="/favicon.ico" type="image/x-icon">
        <link rel="alternate" type="application/atom+xml" title="Jason {osa-jima} blog atom feed" href="/feeds/all.atom.xml" />
        <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700' rel='stylesheet' type='text/css'>

        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
        <link rel="stylesheet" type="text/css" href="/theme/css/icons.css"/>
        <style>.highlight .hll { background-color: #ffffcc }
.highlight .c { color: #60a0b0; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #007020; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .cm { color: #60a0b0; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #007020 } /* Comment.Preproc */
.highlight .c1 { color: #60a0b0; font-style: italic } /* Comment.Single */
.highlight .cs { color: #60a0b0; background-color: #fff0f0 } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #808080 } /* Generic.Output */
.highlight .gp { color: #c65d09; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0040D0 } /* Generic.Traceback */
.highlight .kc { color: #007020; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #007020; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #007020; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #007020 } /* Keyword.Pseudo */
.highlight .kr { color: #007020; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #902000 } /* Keyword.Type */
.highlight .m { color: #40a070 } /* Literal.Number */
.highlight .s { color: #4070a0 } /* Literal.String */
.highlight .na { color: #4070a0 } /* Name.Attribute */
.highlight .nb { color: #007020 } /* Name.Builtin */
.highlight .nc { color: #0e84b5; font-weight: bold } /* Name.Class */
.highlight .no { color: #60add5 } /* Name.Constant */
.highlight .nd { color: #555555; font-weight: bold } /* Name.Decorator */
.highlight .ni { color: #d55537; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #007020 } /* Name.Exception */
.highlight .nf { color: #06287e } /* Name.Function */
.highlight .nl { color: #002070; font-weight: bold } /* Name.Label */
.highlight .nn { color: #0e84b5; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #062873; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #bb60d5 } /* Name.Variable */
.highlight .ow { color: #007020; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mf { color: #40a070 } /* Literal.Number.Float */
.highlight .mh { color: #40a070 } /* Literal.Number.Hex */
.highlight .mi { color: #40a070 } /* Literal.Number.Integer */
.highlight .mo { color: #40a070 } /* Literal.Number.Oct */
.highlight .sb { color: #4070a0 } /* Literal.String.Backtick */
.highlight .sc { color: #4070a0 } /* Literal.String.Char */
.highlight .sd { color: #4070a0; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #4070a0 } /* Literal.String.Double */
.highlight .se { color: #4070a0; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #4070a0 } /* Literal.String.Heredoc */
.highlight .si { color: #70a0d0; font-style: italic } /* Literal.String.Interpol */
.highlight .sx { color: #c65d09 } /* Literal.String.Other */
.highlight .sr { color: #235388 } /* Literal.String.Regex */
.highlight .s1 { color: #4070a0 } /* Literal.String.Single */
.highlight .ss { color: #517918 } /* Literal.String.Symbol */
.highlight .bp { color: #007020 } /* Name.Builtin.Pseudo */
.highlight .vc { color: #bb60d5 } /* Name.Variable.Class */
.highlight .vg { color: #bb60d5 } /* Name.Variable.Global */
.highlight .vi { color: #bb60d5 } /* Name.Variable.Instance */
.highlight .il { color: #40a070 } /* Literal.Number.Integer.Long */</style>
        <style>body {
  margin: 0;
  padding: 0;
  font: 15px 'Source Sans Pro', sans-serif;
  line-height: 1.6em;
  color: #222;
  text-rendering: optimizeLegibility;
  -webkit-font-smoothing: antialiased;
}
a {
  color: #007EE5;
  text-decoration: none;
}
a:hover {
  color: #007EE5;
  text-decoration: none;
}
header.main-header {
  background: none repeat scroll 0% 0% #205F29;
  margin-bottom: 0px;
}
header.main-header a {
  color: #fff;
}
header.main-header .container {
  max-width: 1000px;
}
header.main-header .container nav a:hover {
  background-color: #5C881C;
}
article {
  margin: 0;
}
article header.about {
  margin-bottom: 0px;
  padding-bottom: 0px;
}
article header {
  margin-bottom: 20px;
  padding-bottom: 20px;
}
article header h1 {
  margin-bottom: 2px;
  font-weight: 700;
  color: #000;
}
article header time {
  color: #9E9E9E;
  font-size: 0.85em;
  float: right;
}
article header time.left {
  color: #9E9E9E;
  font-size: 0.85em;
  float: left;
}
article div.social-links ul {
  padding: 0px;
}
article div.social-links li {
  display: inline;
  font-size: 20px;
}
article div.social-links li a {
  color: #000;
  padding: 10px;
}
article div.social-links li a:hover {
  color: #666;
  text-decoration: none;
}
article p {
  font-size: 16px;
  margin-bottom: 20px;
  line-height: 1.6em;
}
article p.note {
  background: #f5f5f5;
  border: 1px solid #ddd;
  padding: 0.533em 0.733em;
}
article p.update {
  background-color: #FEEFB3;
  border: 1px solid #e6e68a;
  padding: 0.533em 0.733em;
}
article p.alert {
  background-color: #ffe2e2;
  border: 1px solid #ffb2b2;
  padding: 0.533em 0.733em;
}
article ul,
article ol {
  margin-top: 0px;
  margin-bottom: 25px;
}
article li {
  font-size: 16px;
  line-height: 1.6em;
}
article a:hover {
  text-decoration: underline;
}
article blockquote {
  border-left: 2px solid #c7c7cc;
  color: #666;
  margin: 30px 0;
  padding: 0 0 0 25px;
}
article img {
  max-width: 100%;
}
article code {
  color: #333;
  background-color: #EEE;
  border-radius: 0;
  font-size: 13px;
}
article .meta {
  font-size: 11px;
}
article .meta a:hover {
  text-decoration: none;
}
article .meta div {
  margin-bottom: 20px;
  display: block;
}
article .meta a.tag {
  margin: 0 10px 10px 0;
  padding: 1px 12px;
  display: inline-block;
  font-size: 14px;
  color: rgba(0, 0, 0, 0.8);
  background: rgba(0, 0, 0, 0.05);
}
article .meta a.tag:hover {
  background: rgba(0, 0, 0, 0.15);
}
article .meta a.read_more,
article .meta a.comments_btn {
  font-size: 14px;
  font-weight: 800;
  padding: 10px 20px;
  color: #205F29;
  background: #FFF;
  border: 1px solid #205F29;
}
article .meta a.read_more:hover,
article .meta a.comments_btn:hover {
  color: #FFF;
  background: #5C881C;
}
.index {
  max-width: 700px;
}
.index article header h2 {
  font-size: 36px;
  margin-bottom: 2px;
  font-weight: 700;
}
.index article header h2 a {
  color: #000;
}
.index article header h2 a:hover {
  color: #007EE5;
  text-decoration: none;
}
.index .separator {
  padding: 40px 0 0 0;
  margin: 0 0 40px 0;
  height: 10px;
  border-bottom: solid 1px #CCC;
}
.index .pagination {
  display: block;
  margin-bottom: 100px;
}
.index .pagination .left {
  text-align: right;
}
.index .pagination .right {
  text-align: left;
}
.index .pagination a {
  display: inline-block;
  border: 2px solid #5C881C;
  margin: 0 5px;
  padding: 8px 20px;
  font-weight: bold;
  color: #5C881C;
}
.index .pagination a:hover {
  color: #FFF;
  background: #5C881C;
}
.post {
  max-width: 700px;
}
.post h2:before {
  content: "# ";
  font-weight: bold;
  color: #DDD;
}
.post h3:before {
  content: "## ";
  font-weight: bold;
  color: #DDD;
}
.post h4:before {
  content: "### ";
  font-weight: bold;
  color: #DDD;
}
.post article .meta {
  margin: 50px 0 100px;
}
.list {
  max-width: 700px;
}
.list ul.double-list {
  margin: 0 auto 60px;
  padding: 0;
  list-style-type: none;
}
.list ul.double-list li {
  padding: 5px 0;
}
.list ul.double-list li h2 {
  font-size: 1em;
  display: inline;
  font-weight: normal;
}
.list ul.double-list li span {
  font-family: sans-serif;
  text-transform: uppercase;
  text-align: right;
  float: right;
  padding-top: 3px;
  font-size: 12px;
  color: #999;
}
.full-width-content {
  padding-top: 10px;
  padding-left: 0px;
  padding-right: 0px;
  margin-left: -20px;
  margin-right: -20px;
}
.col-xs-1,
.col-sm-1,
.col-md-1,
.col-lg-1,
.col-xs-2,
.col-sm-2,
.col-md-2,
.col-lg-2,
.col-xs-3,
.col-sm-3,
.col-md-3,
.col-lg-3,
.col-xs-4,
.col-sm-4,
.col-md-4,
.col-lg-4,
.col-xs-5,
.col-sm-5,
.col-md-5,
.col-lg-5,
.col-xs-6,
.col-sm-6,
.col-md-6,
.col-lg-6,
.col-xs-7,
.col-sm-7,
.col-md-7,
.col-lg-7,
.col-xs-8,
.col-sm-8,
.col-md-8,
.col-lg-8,
.col-xs-9,
.col-sm-9,
.col-md-9,
.col-lg-9,
.col-xs-10,
.col-sm-10,
.col-md-10,
.col-lg-10,
.col-xs-11,
.col-sm-11,
.col-md-11,
.col-lg-11,
.col-xs-12,
.col-sm-12,
.col-md-12,
.col-lg-12 {
  padding-right: 0px;
  padding-left: 0px;
}</style>

	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>


    </head>

    <body>
        <header class="navbar navbar-inverse bs-docs-nav">
            <div class="container-fluid">
                <div class="navbar-header">
		  <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#theNavbar">
		    <span class="icon-bar"></span>
		    <span class="icon-bar"></span>
		    <span class="icon-bar"></span> 
		  </button>
                  <a class="navbar-brand" href="/" title="Home" class="title">Jason {osa-jima}</a>
                </div>
                <nav class="collapse navbar-collapse bs-navbar-collapse" role="navigation" id="theNavbar">
		    <ul class="nav navbar-nav navbar-right">
                            <li><a href="/pages/about.html" title="About">About</a></li>
                    </ul>
                </nav>
            </div>
        </header>

        <div id="wrap">
<div class="container post">
    <article>
        <header>
            <h1>word2vec</h1>
            <time datetime="article.date.isoformat()" pubdate>Fri 22 November 2019</time>
        </header>

        <div class="article_content">
            <h2>Introduction</h2>




<p>word2vec is an iterative model that can be used to create embeddings of words (or embeddings of <a href="https://medium.com/airbnb-engineering/listing-embeddings-for-similar-listing-recommendations-and-real-time-personalization-in-search-601172f7603e">pretty much anything</a>). In this post, we will talk briefly about why you would want to use word2vec, break down the Continuous Bag of Words (CBOW) and skip gram word2vec model, and implement it in tensorflow.</p>
<h3>Why use word2vec?</h3>
<p>For a lot of machine learning tasks, we need to figure out the relationship between items. Take for example an NLP task where we use words as items. We somehow need to learn numerical representations of words so that the model can understand the relationship between words. The most straightforward way of representing words numerically would be to represent them as vectors. So, imagine that we trained a model and learned vectors for each word. We'd expect that the word "cat" and "dog" would be close in distance since they are both pets, and that "cat" would be far from "gym", because there isn't much of a relationship between these two words. We sometimes call these vectors embeddings.</p>
<h3>What is word2vec?</h3>
<p>word2vec is a model that attempts to learn these embeddings based on a corpus of text (or a group of items). The basic idea is that we initialize a vector for each word in the corpus with random numbers. We then iterate through each word of each document (a document is just a group of words that are related), grab the vectors of the closest n-words on either side of our target word, concatenate these vectors, forward propagate it through a linear layer + softmax function, and attempt to predict what our target word was. We then backpropagate the error between our prediction and the actual target word, and update not only the weights of the linear layer, but also the vectors (or embeddings) of our neighbor words.</p>
<p>Imagine that we have a corpus of two documents:</p>
<div class="highlight"><pre><span></span><span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span><span class="n">document_1</span><span class="p">,</span> <span class="n">document_2</span><span class="p">]</span>
</pre></div>


<p>The documents are just one sentence long:</p>
<div class="highlight"><pre><span></span><span class="n">document1</span> <span class="o">=</span> <span class="p">[</span><span class="ss">&quot;the&quot;</span><span class="p">,</span> <span class="ss">&quot;cat&quot;</span><span class="p">,</span> <span class="ss">&quot;loves&quot;</span><span class="p">,</span> <span class="ss">&quot;fish&quot;</span><span class="p">]</span>
<span class="n">document2</span> <span class="o">=</span> <span class="p">[</span><span class="ss">&quot;the&quot;</span><span class="p">,</span> <span class="ss">&quot;person&quot;</span><span class="p">,</span> <span class="ss">&quot;hates&quot;</span><span class="p">,</span> <span class="ss">&quot;fish&quot;</span><span class="p">]</span>
</pre></div>


<p>So the goal of using word2vec is to learn embeddings for all of the words in our corpus. In this case, the words in our corpus are:</p>
<div class="highlight"><pre><span></span><span class="err">{</span><span class="s1">&#39;the&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;cat&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;loves&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;fish&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">&#39;person&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;hates&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="err">}</span>
</pre></div>


<p>We can create our vocab <code>word_to_ix</code> using the following code:</p>
<div class="highlight"><pre><span></span><span class="nv">def</span> <span class="nv">corpus_to_vocab</span><span class="ss">(</span><span class="nv">corpus</span><span class="ss">)</span>:
    <span class="s2">&quot;&quot;&quot;</span>
    <span class="nv">Takes</span> <span class="nv">a</span> <span class="nv">corpus</span> <span class="ss">(</span><span class="nv">list</span> <span class="nv">of</span> <span class="nv">documents</span><span class="ss">)</span> <span class="nv">and</span> <span class="nv">converts</span>
    <span class="nv">it</span> <span class="nv">to</span> <span class="nv">two</span> <span class="nv">dictionaries</span>:
      <span class="o">-</span> <span class="nv">word_to_ix</span>: <span class="nv">key</span> <span class="nv">are</span> <span class="nv">words</span> <span class="nv">in</span> <span class="nv">vocab</span>, <span class="nv">values</span>
        <span class="nv">are</span> <span class="nv">the</span> <span class="nv">unique</span> <span class="nv">indices</span>
      <span class="o">-</span> <span class="nv">ix_to_word</span>: <span class="nv">key</span> <span class="nv">are</span> <span class="nv">the</span> <span class="nv">unique</span> <span class="nv">indices</span>,
        <span class="nv">values</span> <span class="nv">are</span> <span class="nv">the</span> <span class="nv">words</span> <span class="nv">in</span> <span class="nv">vocab</span>
    <span class="s2">&quot;&quot;&quot;</span>
    <span class="nv">word_to_ix</span>, <span class="nv">ix_to_word</span> <span class="o">=</span> {}, {}
    <span class="nv">ix</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="nv">document</span> <span class="nv">in</span> <span class="nv">corpus</span>:
        <span class="k">for</span> <span class="nv">word</span> <span class="nv">in</span> <span class="nv">document</span>:
            <span class="k">if</span> <span class="nv">word</span> <span class="nv">not</span> <span class="nv">in</span> <span class="nv">word_to_ix</span>.<span class="nv">keys</span><span class="ss">()</span>:
                <span class="nv">word_to_ix</span>[<span class="nv">word</span>], <span class="nv">ix_to_word</span>[<span class="nv">ix</span>] <span class="o">=</span> <span class="nv">ix</span>, <span class="nv">word</span>
                <span class="nv">ix</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="nv">word_to_ix</span>, <span class="nv">ix_to_word</span>

<span class="nv">EMBEDDING_DIM</span> <span class="o">=</span> <span class="mi">3</span>

<span class="nv">document1</span> <span class="o">=</span> [<span class="s2">&quot;</span><span class="s">the</span><span class="s2">&quot;</span>, <span class="s2">&quot;</span><span class="s">cat</span><span class="s2">&quot;</span>, <span class="s2">&quot;</span><span class="s">loves</span><span class="s2">&quot;</span>, <span class="s2">&quot;</span><span class="s">fish</span><span class="s2">&quot;</span>]
<span class="nv">document2</span> <span class="o">=</span> [<span class="s2">&quot;</span><span class="s">the</span><span class="s2">&quot;</span>, <span class="s2">&quot;</span><span class="s">person</span><span class="s2">&quot;</span>, <span class="s2">&quot;</span><span class="s">hates</span><span class="s2">&quot;</span>, <span class="s2">&quot;</span><span class="s">fish</span><span class="s2">&quot;</span>]
<span class="nv">corpus</span> <span class="o">=</span> [<span class="nv">document1</span>, <span class="nv">document2</span>]

# <span class="nv">vocab</span>
<span class="nv">word_to_ix</span>, <span class="nv">ix_to_word</span> <span class="o">=</span> <span class="nv">corpus_to_vocab</span><span class="ss">(</span><span class="nv">corpus</span><span class="ss">)</span>
</pre></div>


<p>And we can instantiate embeddings for each of these words in a matrix where the number of rows are equal to the number of words in our vocab and the number of columns is the number of dimensions of our embedding vector. Let's make it simple and work with 3 dimensional embeddings.</p>
<div class="math">$$\textbf{V} = \begin{bmatrix}
  1 &amp; 1 &amp; 4 \\
   5 &amp; 5 &amp; 1 \\
   2 &amp; 1 &amp; 5
\end{bmatrix}$$</div>
<p>If we were interested in looking up the embedding for the word <code>the</code>, we would lookup <code>the</code> in our <code>vocab</code>, get the index <span class="math">\(0\)</span>, and return the embedding <code>[1, 1, 4]</code>.</p>
<p>Now that we've explained the setup of word embeddings, how do we learn them? There are two main ways to learn embeddings using word2vec: Continuous Bag of Words (CBOW), and skipgram. We'll start with explaining CBOW.</p>
<h2>Continuous Bag of Words (CBOW)</h2>
<p>The first step for implementing CBOW is to instantiate the embedding matrix described above. Let's create an embedding matrix for the words in our vocab using tensorflow.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>

<span class="n">N_WORDS</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_to_ix</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">N_WORDS</span><span class="p">,</span> <span class="n">EMBEDDING_DIM</span><span class="p">,</span> 
                                   <span class="n">embeddings_initializer</span><span class="o">=</span><span class="s2">&quot;RandomNormal&quot;</span><span class="p">,</span>
                                   <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
</pre></div>


<p>We define an <code>input_shape</code> because the embedding layer is the first layer of our model. The reason why it has an input shape of <span class="math">\((2,)\)</span> is that for each target word, we pass in as input two context words, represented as indices. So if we wanted to pass in "the" and "loves", we would pass in the vector <code>[0,2]</code>. </p>
<p>Great, so now we have an embedding matrix. As an example, we can look up the embedding for <code>the</code> by passing a <code>0</code>:</p>
<div class="highlight"><pre><span></span><span class="n">embedding_layer</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>


<p>Next we need to setup our training set for CBOW. Our output is a word in a document, and the input is the context of the word, which is just the n-words to the left and right of the output word. So for example, if we were converting <code>document1</code> to our training set, we would end up with:</p>
<div class="highlight"><pre><span></span><span class="p">[([</span><span class="ss">&quot;the&quot;</span><span class="p">,</span> <span class="ss">&quot;loves&quot;</span><span class="p">],</span> <span class="ss">&quot;cat&quot;</span><span class="p">),</span>
 <span class="p">([</span><span class="ss">&quot;cat&quot;</span><span class="p">,</span> <span class="ss">&quot;fish&quot;</span><span class="p">],</span> <span class="ss">&quot;loves&quot;</span><span class="p">)]</span>
</pre></div>


<p>Notice we don't include <code>"the"</code> and <code>"fish"</code> in our train set, because we wouldn't have enough context words to construct the training example for them. This isn't a problem when we have a large corpus (but might look like a problem with our small corpus of two document sentences).</p>
<p>We'll convert the words in the train set to indices so it's easy to lookup the word's embeddings:</p>
<div class="highlight"><pre><span></span><span class="p">[([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span>
 <span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="mi">2</span><span class="p">)]</span>
</pre></div>


<p>In code, this looks like:</p>
<div class="highlight"><pre><span></span><span class="nv">train_set</span> <span class="o">=</span> []
<span class="k">for</span> <span class="nv">document</span> <span class="nv">in</span> <span class="nv">corpus</span>:
    <span class="k">for</span> <span class="nv">i</span> <span class="nv">in</span> <span class="nv">range</span><span class="ss">(</span><span class="mi">1</span>, <span class="nv">len</span><span class="ss">(</span><span class="nv">document</span><span class="ss">)</span><span class="o">-</span><span class="mi">1</span><span class="ss">)</span>:
        <span class="nv">target_word</span> <span class="o">=</span> <span class="nv">word_to_ix</span>[<span class="nv">document</span>[<span class="nv">i</span>]]
        <span class="nv">context</span> <span class="o">=</span> [<span class="nv">word_to_ix</span>[<span class="nv">document</span>[<span class="nv">i</span><span class="o">-</span><span class="mi">1</span>]], <span class="nv">word_to_ix</span>[<span class="nv">document</span>[<span class="nv">i</span><span class="o">+</span><span class="mi">1</span>]]]
        <span class="nv">train_set</span>.<span class="nv">append</span><span class="ss">((</span><span class="nv">context</span>, <span class="nv">target_word</span><span class="ss">))</span>

<span class="nv">X</span> <span class="o">=</span> <span class="nv">np</span>.<span class="nv">array</span><span class="ss">(</span>[<span class="nv">example</span>[<span class="mi">0</span>] <span class="k">for</span> <span class="nv">example</span> <span class="nv">in</span> <span class="nv">train_set</span>]<span class="ss">)</span>
<span class="nv">y</span> <span class="o">=</span> <span class="nv">np</span>.<span class="nv">array</span><span class="ss">(</span>[<span class="nv">example</span>[<span class="mi">1</span>] <span class="k">for</span> <span class="nv">example</span> <span class="nv">in</span> <span class="nv">train_set</span>]<span class="ss">)</span>
<span class="nv">y</span> <span class="o">=</span> <span class="nv">keras</span>.<span class="nv">utils</span>.<span class="nv">to_categorical</span><span class="ss">(</span><span class="nv">y</span>, <span class="nv">num_classes</span><span class="o">=</span><span class="nv">N_WORDS</span><span class="ss">)</span>
</pre></div>


<p>Now that we constructed our train set, let's setup our model.</p>
<div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">([</span>
  <span class="n">embedding_layer</span><span class="p">,</span>
  <span class="n">layers</span><span class="p">.</span><span class="n">GlobalAveragePooling1D</span><span class="p">(),</span>
  <span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">N_WORDS</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">),</span>
<span class="p">])</span>
</pre></div>


<p>Let's go through the three layers of this model. <code>embedding_layer</code> was already described above, and takes as input two indices for the two context words and returns their two embeddings. The output therefore has shape <code>(None,2,3)</code>. The first dimension is <code>None</code> because it depends on how many words we pass into the model during training. So it could be <span class="math">\(1\)</span> if we pass in one training example, and <span class="math">\(5\)</span> if we pass in five training examples. <code>GlobalAveragePooling1d</code> takes the average of the two embeddings, and its output is  <code>(None,1,3)</code>. Finally, <code>Dense</code> is a fully-connected layer that multiplies the output of <code>GlobalAveragePooling1d</code> by a weight matrix and adds a bias. The resulting vector is then passed through a <code>softmax</code> activation, so that we end up with a vector of probabilities for the index of the target output word.</p>
<div class="highlight"><pre><span></span><span class="n">model</span><span class="p">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>


<p>Next, we compile the model, using <code>categorical_crossentropy</code> as our loss function since this is a multi-category problem, and then we are done! Obviously, our word embeddings won't be very "good" because we only trained a model on  two sentences. To check how the embeddings for the word <code>the</code> changed, we just pass:</p>
<div class="highlight"><pre><span></span><span class="nf">embedding_layer</span><span class="p">(</span><span class="n">word_to_ix</span><span class="p">[</span><span class="ss">`the`</span><span class="p">])</span>
</pre></div>


<p>So what changes when we use skipgram?</p>
<h2>Skip-Gram</h2>
<p>The main change is that we switch the output and input for the model. So the input into the model is now the target word, and the output are the context words. For example, the input could now be the word <code>cat</code>, and the output could be the words <code>love</code> and <code>the</code>. We represent the input word as its index, so we'd feed in <span class="math">\(1\)</span> for the word <code>cat</code>. What would the output be? If the context is <code>love</code> and <code>the</code>, the output would be <code>[1,1,0,0,0]</code>. I like to call this multi-hot encoding, but I'm not sure if that's the best term for it. We also change the loss to <code>binary_crossentropy</code>, since we now have <a href="https://github.com/keras-team/keras/issues/2166">converted our problem to multi-label</a>.</p>
<div class="highlight"><pre><span></span><span class="nv">train_set</span> <span class="o">=</span> []

<span class="k">for</span> <span class="nv">document</span> <span class="nv">in</span> <span class="nv">corpus</span>:
    <span class="k">for</span> <span class="nv">i</span> <span class="nv">in</span> <span class="nv">range</span><span class="ss">(</span><span class="mi">1</span>, <span class="nv">len</span><span class="ss">(</span><span class="nv">document</span><span class="ss">)</span><span class="o">-</span><span class="mi">1</span><span class="ss">)</span>:
        <span class="nv">target_word</span> <span class="o">=</span> <span class="nv">word_to_ix</span>[<span class="nv">document</span>[<span class="nv">i</span>]]
        <span class="nv">context</span> <span class="o">=</span> [<span class="nv">word_to_ix</span>[<span class="nv">document</span>[<span class="nv">i</span><span class="o">-</span><span class="mi">1</span>]], <span class="nv">word_to_ix</span>[<span class="nv">document</span>[<span class="nv">i</span><span class="o">+</span><span class="mi">1</span>]]]
        <span class="nv">train_set</span>.<span class="nv">append</span><span class="ss">((</span><span class="nv">target_word</span>, <span class="nv">context</span><span class="ss">))</span>

<span class="nv">N_WORDS</span> <span class="o">=</span> <span class="nv">len</span><span class="ss">(</span><span class="nv">word_to_ix</span>.<span class="nv">keys</span><span class="ss">())</span>
<span class="nv">embedding_layer</span> <span class="o">=</span> <span class="nv">layers</span>.<span class="nv">Embedding</span><span class="ss">(</span><span class="nv">N_WORDS</span>, <span class="nv">EMBEDDING_DIM</span>, 
                                   <span class="nv">embeddings_initializer</span><span class="o">=</span><span class="s2">&quot;</span><span class="s">RandomNormal</span><span class="s2">&quot;</span>,
                                   <span class="nv">input_shape</span><span class="o">=</span><span class="ss">(</span><span class="mi">1</span>,<span class="ss">))</span>

<span class="nv">X</span> <span class="o">=</span> <span class="nv">np</span>.<span class="nv">array</span><span class="ss">(</span>[<span class="nv">example</span>[<span class="mi">0</span>] <span class="k">for</span> <span class="nv">example</span> <span class="nv">in</span> <span class="nv">train_set</span>]<span class="ss">)</span>
<span class="nv">y</span> <span class="o">=</span> <span class="nv">np</span>.<span class="nv">array</span><span class="ss">(</span>[<span class="nv">example</span>[<span class="mi">1</span>] <span class="k">for</span> <span class="nv">example</span> <span class="nv">in</span> <span class="nv">train_set</span>]<span class="ss">)</span>
<span class="nv">y</span> <span class="o">=</span> <span class="nv">keras</span>.<span class="nv">utils</span>.<span class="nv">to_categorical</span><span class="ss">(</span><span class="nv">y</span>, <span class="nv">num_classes</span><span class="o">=</span><span class="nv">N_WORDS</span><span class="ss">)</span>
<span class="nv">y</span> <span class="o">=</span> <span class="nv">np</span>.<span class="nv">sum</span><span class="ss">(</span><span class="nv">y</span>, <span class="nv">axis</span><span class="o">=</span><span class="mi">1</span><span class="ss">)</span>.<span class="nv">astype</span><span class="ss">(</span><span class="s1">&#39;</span><span class="s">int</span><span class="s1">&#39;</span><span class="ss">)</span>

<span class="nv">model</span> <span class="o">=</span> <span class="nv">keras</span>.<span class="nv">Sequential</span><span class="ss">(</span>[
  <span class="nv">embedding_layer</span>,
  <span class="nv">layers</span>.<span class="nv">GlobalAveragePooling1D</span><span class="ss">()</span>,
  <span class="nv">layers</span>.<span class="nv">Dense</span><span class="ss">(</span><span class="nv">N_WORDS</span>, <span class="nv">activation</span><span class="o">=</span><span class="s1">&#39;</span><span class="s">softmax</span><span class="s1">&#39;</span><span class="ss">)</span>,
]<span class="ss">)</span>

<span class="nv">model</span>.<span class="nv">compile</span><span class="ss">(</span><span class="nv">optimizer</span><span class="o">=</span><span class="s1">&#39;</span><span class="s">adam</span><span class="s1">&#39;</span>,
              <span class="nv">loss</span><span class="o">=</span><span class="s1">&#39;</span><span class="s">binary_crossentropy</span><span class="s1">&#39;</span>,
              <span class="nv">metrics</span><span class="o">=</span>[<span class="s1">&#39;</span><span class="s">accuracy</span><span class="s1">&#39;</span>]<span class="ss">)</span>

<span class="nv">history</span> <span class="o">=</span> <span class="nv">model</span>.<span class="nv">fit</span><span class="ss">(</span><span class="nv">X</span>,<span class="nv">y</span>, <span class="nv">batch_size</span><span class="o">=</span><span class="nv">X</span>.<span class="nv">shape</span>[<span class="mi">0</span>]<span class="ss">)</span>
</pre></div>


<h3>Conclusion</h3>
<p>So we now know how to implement CBOW and SkipGram word2vec in tensorflow. Hooray! Don't get too excited though: these implementations are not very practical. The reason is because the number of words will not typically be 6, like in our example. Let's imagine that we implemented word2vec using all of wikipedia. Wikipedia has 2.9 billion words. We can't have a 2.9 billion one-hot or multi-hot encoded output. Luckily, we have a way to get around this by using negative sampling, which was introduced in the <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">original word2vec paper</a>.</p>
<h3>Resources</h3>
<p>1) <a href="https://github.com/stanfordnlp/cs224n-winter17-notes/blob/master/notes1.pdf">Notes from Stanford NLP course</a></p>
<p>2) <a href="https://arxiv.org/pdf/1301.3781.pdf">word2vec paper</a></p>
<p>3) <a href="https://stackoverflow.com/questions/38287772/cbow-v-s-skip-gram-why-invert-context-and-target-words">difference between skipgram and cbow</a></p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
        </div>

        <div class="meta">
            <div>
                    <a href="http://www.jasonosajima.com/tag/machine-learning.html" class="tag">machine learning</a>
                    <a href="http://www.jasonosajima.com/tag/linear-algebra.html" class="tag">linear algebra</a>
                    <a href="http://www.jasonosajima.com/tag/natural-language-processing.html" class="tag">natural language processing</a>
            </div>
        </div>
    </article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div>
    <script type="text/javascript">
      var disqus_shortname = 'www-jasonosajima-com';
      var disqus_identifier = '/word2vec.html';
      var disqus_url = 'http://www.jasonosajima.com/word2vec.html';
      var disqus_title = 'word2vec';
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = "//" + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
  </section>

</div>

<style type="text/css">
{
    max-width: 700px;
}

.text_cell .prompt {
    display: none;
}

div.cell {
    padding: 0;
}

div.text_cell_render {
    padding: 0;
}

div.prompt {
    font-size: 13px;
}

div.input_prompt {
    padding: .7em 0.2em;
}

div.output_prompt {
    padding: .4em .2em;
}

div.input_area {
    margin: .2em 0.4em;
    max-width: 580px;
}

table.dataframe {
    font-family: Arial, sans-serif;
    font-size: 13px;
    line-height: 20px;
}

table.dataframe th, td {
    padding: 4px;
    text-align: left;
}

pre code {
    background-color: inherit;
}</style>

        </div>
<!--
    <footer>
      <p>
        © 2012-2017 Jason Osajima, license <a href=""> </a>
        unless otherwise noted.
        Generated by <a href= "http://docs.getpelican.com/">Pelican</a>.
      </p>
    </footer>
-->
    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-15195255-2']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();

    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-15195255-2');
    ga('send', 'pageview');
</script>
    </body>
</html>