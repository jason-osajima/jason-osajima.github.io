<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Jason {osa-jima} - posts</title><link href="http://www.jasonosajima.com/" rel="alternate"></link><link href="http://www.jasonosajima.com/feeds/posts.atom.xml" rel="self"></link><id>http://www.jasonosajima.com/</id><updated>2019-12-10T00:00:00-08:00</updated><subtitle>Machine Learning concepts, deconstructed.</subtitle><entry><title>word2vec - Negative Sampling</title><link href="http://www.jasonosajima.com/ns.html" rel="alternate"></link><published>2019-12-10T00:00:00-08:00</published><updated>2019-12-10T00:00:00-08:00</updated><author><name>Jason Osajima</name></author><id>tag:www.jasonosajima.com,2019-12-10:/ns.html</id><summary type="html">
&lt;p&gt;In the &lt;a href="https://papers.nips.cc/ paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"&gt;original word2vec paper&lt;/a&gt;, the authors introduced Negative Sampling, which is a technique to overcome the computational limitations of vanilla Skip-Gram. Recall that in the &lt;a href="/ns"&gt;previous post&lt;/a&gt;, we had a vocabulary of 6 words, so the output of Skip-Gram was a vector of 6 binary elements. However, if we had a vocabulary of, say 170,000 words, we'd find it difficult to compute our loss function for every step of training the model. &lt;/p&gt;
&lt;p&gt;In this post, we will discuss the changes to Skip-Gram using negative sampling and update our Tensorflow word2vec implementation to use it.
&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;This is part two in a two-part series on the word2vec. Part one is about CBOW and Skip-Gram and can be found &lt;a href="/word2vec"&gt;here&lt;/a&gt;. Part two is about negative sampling.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;


&lt;p&gt;In the &lt;a href="https://papers.nips.cc/ paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"&gt;original word2vec paper&lt;/a&gt;, the authors introduced Negative Sampling, which is a technique to overcome the computational limitations of vanilla Skip-Gram. Recall that in the &lt;a href="/ns"&gt;previous post&lt;/a&gt;, we had a vocabulary of 6 words, so the output of Skip-Gram was a vector of 6 binary elements. However, if we had a vocabulary of, say 170,000 words, we'd find it difficult to compute our loss function for every step of training the model. &lt;/p&gt;
&lt;p&gt;In this post, we will discuss the changes to Skip-Gram using negative sampling and update our Tensorflow word2vec implementation to use it.
&lt;/p&gt;
&lt;h2&gt;Problem Setup&lt;/h2&gt;
&lt;p&gt;Let's use the same corpus of documents that we had in the &lt;a href="/ns"&gt;previous post&lt;/a&gt;: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;corpus&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;document_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;document_2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The documents are just one sentence long:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;document1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;the&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;cat&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;loves&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;fish&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;document2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;the&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;person&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;hates&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;fish&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And we are still trying to learn the embeddings for the following words in our vocab:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;word_to_ix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;the&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;cat&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;loves&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;fish&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;person&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;hates&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The input to Skip-Gram is a word, and the output is the words that surround that word (called the context). So we would expect to see the following as a training example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;cat&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;the&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;loves&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We convert these to indices:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And convert the output to a binary encoded vector:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If we use negative sampling, we will convert this training example into two training examples like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;cat&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;the&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;cat&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;loves&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So now the model takes as input a word, context pair &lt;span class="math"&gt;\((w, c)\)&lt;/span&gt; and attempts to predict whether or not this pair came from the training data (1 if it is, 0 if it is not). If we used this approach, the training data would be quite imbalanced (since it only has positive examples). So how do we get the negative examples? We sample them (hence, negative sampling)!&lt;/p&gt;
&lt;p&gt;Which distribution do we sample them from? The &lt;a href="https://papers.nips.cc/ paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"&gt;original paper&lt;/a&gt; recommends using the Unigram Model raised to the &lt;span class="math"&gt;\(3/4\)&lt;/span&gt; power. The rationale behind using &lt;span class="math"&gt;\(3/4\)&lt;/span&gt; this can be explained using the example from this &lt;a href="https://github.com/stanfordnlp/cs224n-winter17-notes/blob/master/notes1.pdf"&gt;lecture&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;is: &lt;span class="math"&gt;\(0.9^{3/4}\)&lt;/span&gt; = 0.92
Constitution: &lt;span class="math"&gt;\(0.09^{3/4}\)&lt;/span&gt; = 0.16
bombastic: &lt;span class="math"&gt;\(0.01^{3/4}\)&lt;/span&gt; = 0.032&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So the original probability from the Unigram Model for the word "is" is &lt;span class="math"&gt;\(0.9\)&lt;/span&gt;. After taking that probability to the &lt;span class="math"&gt;\(3/4\)&lt;/span&gt; power, its new probability is &lt;span class="math"&gt;\(0.92\)&lt;/span&gt;. Not much of a difference. But now look at the word "bombastic". Its probability increases from &lt;span class="math"&gt;\(0.01\)&lt;/span&gt; to &lt;span class="math"&gt;\(0.032\)&lt;/span&gt;, a 3x difference. So taking the probabilities of words to the &lt;span class="math"&gt;\(3/4\)&lt;/span&gt; power is a way to normalize probabilities, so that words that show up more infrequently have a higher probability of being sampled.&lt;/p&gt;
&lt;h3&gt;What is the Unigram Model?&lt;/h3&gt;
&lt;p&gt;The Unigram Model is a probability distribution for words that makes the assumption that the words in a sentence are completely independent from one another. So a sentence's probability of occurring is dependent on the probabilities of each of the words in that sentence. Under the unigram model, we'd expect this sentence to have a higher probability:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"is and and she"&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Compared to this sentence:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"The cantankerous curmudgeon is irascible."&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Why does the first sentence have a higher probability of occurring according to the Unigram Model? We want to calculate the probability of the sentence, which is the probability that the sequence of words will occur, &lt;span class="math"&gt;\(P(is, and, and she)\)&lt;/span&gt;. Remember that the Unigram Model assumes that words occurences in a sequence are independent of one another, so this probability becomes:&lt;/p&gt;
&lt;div class="math"&gt;$$P(is, and, and, she) = P(is)P(and)P(and)P(she)$$&lt;/div&gt;
&lt;p&gt;Comparing this to the probability of our second sentence:&lt;/p&gt;
&lt;div class="math"&gt;$$P(the, cantankerous, curmudgeon, is, irascible) = P(the)P(cantankerous)P(curmudgeon)P(is)P(irascible)$$&lt;/div&gt;
&lt;p&gt;And it becomes clear that the probability of the first sentence occurring is much higher, because we would expect the probabilities of the rarer words in the second sentence to be much lower than all of the words in the first sentence.&lt;/p&gt;
&lt;p&gt;Let's implement the Unigram Model using python. We start by counting the frequency for each word and saving this in a &lt;code&gt;dict&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;collections&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;defaultdict&lt;/span&gt;

&lt;span class="n"&gt;wordFreq&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;defaultdict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;document&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;corpus&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;document&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;wordFreq&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The result is a frequency dict, which shows the number of times a word showed up in our corpus:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;wordFreq&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;the&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;cat&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;loves&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;fish&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;person&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;hates&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Next, let's convert these frequencies to probabilities. If for a given word &lt;span class="math"&gt;\(w_i\)&lt;/span&gt; the frequency that it shows up in the corpus is &lt;span class="math"&gt;\(f(w_i)\)&lt;/span&gt;, then the sample probability for &lt;span class="math"&gt;\(w_i\)&lt;/span&gt; for our distribution will be:&lt;/p&gt;
&lt;div class="math"&gt;$$P(w_i) = \dfrac{f(w_i)}{\sum^n_{j=0}f(w_j)}$$&lt;/div&gt;
&lt;p&gt;Taking the advice from the word2vec authors, we replace &lt;span class="math"&gt;\(f(w_i)\)&lt;/span&gt; with &lt;span class="math"&gt;\(f(w_i)^{3/4}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$P(w_i) = \dfrac{f(w_i)^{3/4}}{\sum^n_{j=0}f(w_j)^{3/4}}$$&lt;/div&gt;
&lt;p&gt;Implementing this in python:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;totalWords&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;sum&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;[&lt;span class="nv"&gt;freq&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;freq&lt;/span&gt; &lt;span class="nv"&gt;in&lt;/span&gt; &lt;span class="nv"&gt;wordFreq&lt;/span&gt;.&lt;span class="nv"&gt;values&lt;/span&gt;&lt;span class="ss"&gt;()&lt;/span&gt;]&lt;span class="ss"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;wordProb&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; {&lt;span class="nv"&gt;word&lt;/span&gt;:&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;freq&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="nv"&gt;totalWords&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;word&lt;/span&gt;, &lt;span class="nv"&gt;freq&lt;/span&gt; &lt;span class="nv"&gt;in&lt;/span&gt; &lt;span class="nv"&gt;wordFreq&lt;/span&gt;.&lt;span class="nv"&gt;items&lt;/span&gt;&lt;span class="ss"&gt;()&lt;/span&gt;}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Great! Now we can use &lt;code&gt;np.random.choice&lt;/code&gt; to sample from this probability distribution, and we can use that to generate our negative word, context pairs to use to train our model.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;generate_negative_sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;wordProb&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;    This function takes as input a dict with keys as the &lt;/span&gt;
&lt;span class="sd"&gt;    words in the vocab and values as the probabilities.&lt;/span&gt;
&lt;span class="sd"&gt;    Probabilities must sum to 1.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;    
    &lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;context&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;wordProb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;()),&lt;/span&gt; 
                     &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;wordProb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;context&lt;/span&gt;

&lt;span class="n"&gt;word&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;context&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;generate_negative_sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;wordProb&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;How many should we generate? Good question. &lt;code&gt;gensim&lt;/code&gt;, the most popular NLP library in Python, uses a rate of &lt;span class="math"&gt;\(0.75\)&lt;/span&gt; but that might not be the &lt;a href="https://github.com/RaRe-Technologies/gensim/issues/2090"&gt;best rate for all word2vec applications&lt;/a&gt;. Let's stick with 50% negative samples for this simple example.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;posTrainSet&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; []

# &lt;span class="nv"&gt;add&lt;/span&gt; &lt;span class="nv"&gt;positive&lt;/span&gt; &lt;span class="nv"&gt;examples&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;document&lt;/span&gt; &lt;span class="nv"&gt;in&lt;/span&gt; &lt;span class="nv"&gt;corpus&lt;/span&gt;:
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;i&lt;/span&gt; &lt;span class="nv"&gt;in&lt;/span&gt; &lt;span class="nv"&gt;range&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;, &lt;span class="nv"&gt;len&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;document&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;:
        &lt;span class="nv"&gt;word&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;word_to_ix&lt;/span&gt;[&lt;span class="nv"&gt;document&lt;/span&gt;[&lt;span class="nv"&gt;i&lt;/span&gt;]]
        &lt;span class="nv"&gt;context_words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; [&lt;span class="nv"&gt;word_to_ix&lt;/span&gt;[&lt;span class="nv"&gt;document&lt;/span&gt;[&lt;span class="nv"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;]], &lt;span class="nv"&gt;word_to_ix&lt;/span&gt;[&lt;span class="nv"&gt;document&lt;/span&gt;[&lt;span class="nv"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;]]]
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;context&lt;/span&gt; &lt;span class="nv"&gt;in&lt;/span&gt; &lt;span class="nv"&gt;context_words&lt;/span&gt;:
            &lt;span class="nv"&gt;posTrainSet&lt;/span&gt;.&lt;span class="nv"&gt;append&lt;/span&gt;&lt;span class="ss"&gt;((&lt;/span&gt;&lt;span class="nv"&gt;word&lt;/span&gt;, &lt;span class="nv"&gt;context&lt;/span&gt;&lt;span class="ss"&gt;))&lt;/span&gt;

&lt;span class="nv"&gt;n_pos_examples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;len&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;posTrainSet&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;

# &lt;span class="nv"&gt;add&lt;/span&gt; &lt;span class="nv"&gt;the&lt;/span&gt; &lt;span class="nv"&gt;same&lt;/span&gt; &lt;span class="nv"&gt;number&lt;/span&gt; &lt;span class="nv"&gt;of&lt;/span&gt; &lt;span class="nv"&gt;negative&lt;/span&gt; &lt;span class="nv"&gt;examples&lt;/span&gt;
&lt;span class="nv"&gt;n_neg_examples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="nv"&gt;negTrainSet&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; []

&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="nv"&gt;n_neg_examples&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="nv"&gt;n_pos_examples&lt;/span&gt;:
    &lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;word&lt;/span&gt;, &lt;span class="nv"&gt;context&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;generate_negative_sample&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;wordProb&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
    # &lt;span class="nv"&gt;convert&lt;/span&gt; &lt;span class="nv"&gt;to&lt;/span&gt; &lt;span class="nv"&gt;indicies&lt;/span&gt;
    &lt;span class="nv"&gt;word&lt;/span&gt;, &lt;span class="nv"&gt;context&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;word_to_ix&lt;/span&gt;[&lt;span class="nv"&gt;word&lt;/span&gt;], &lt;span class="nv"&gt;word_to_ix&lt;/span&gt;[&lt;span class="nv"&gt;context&lt;/span&gt;]
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;word&lt;/span&gt;, &lt;span class="nv"&gt;context&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt; &lt;span class="nv"&gt;not&lt;/span&gt; &lt;span class="nv"&gt;in&lt;/span&gt; &lt;span class="nv"&gt;posTrainSet&lt;/span&gt;:
        &lt;span class="nv"&gt;negTrainSet&lt;/span&gt;.&lt;span class="nv"&gt;append&lt;/span&gt;&lt;span class="ss"&gt;((&lt;/span&gt;&lt;span class="nv"&gt;word&lt;/span&gt;, &lt;span class="nv"&gt;context&lt;/span&gt;&lt;span class="ss"&gt;))&lt;/span&gt;
        &lt;span class="nv"&gt;n_neg_examples&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="nv"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;np&lt;/span&gt;.&lt;span class="nv"&gt;concatenate&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;[&lt;span class="nv"&gt;np&lt;/span&gt;.&lt;span class="nv"&gt;array&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;posTrainSet&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;, &lt;span class="nv"&gt;np&lt;/span&gt;.&lt;span class="nv"&gt;array&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;negTrainSet&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;], &lt;span class="nv"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;np&lt;/span&gt;.&lt;span class="nv"&gt;concatenate&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;[[&lt;span class="mi"&gt;1&lt;/span&gt;]&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nv"&gt;n_pos_examples&lt;/span&gt;, [&lt;span class="mi"&gt;0&lt;/span&gt;]&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nv"&gt;n_neg_examples&lt;/span&gt;]&lt;span class="ss"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Notice that when we generate negative examples, we check if that negative example is the same as a positive example. If it is, we discard it. That makes sense, since we don't want a word, context pair to be both a positive and negative example.&lt;/p&gt;
&lt;p&gt;Now, let's initialize the embeddings. We'll change the &lt;code&gt;input_shape&lt;/code&gt; parameter from &lt;code&gt;1&lt;/code&gt; to &lt;code&gt;2&lt;/code&gt;, since we are taking a word and context as inputs into the model instead of just the word.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;N_WORDS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word_to_ix&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="n"&gt;embedding_layer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N_WORDS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;EMBEDDING_DIM&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                                   &lt;span class="n"&gt;embeddings_initializer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;RandomNormal&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                   &lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Next, let's define the model, compile it, and fit it. The only change we make is that we want the output to be a probability that the word, context pair came from the train dataset, so we change the output dimensions to be 1 and the activation to be sigmoid.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;keras&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;tensorflow.keras&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;layers&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;keras.models&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Model&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;keras&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Sequential&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
  &lt;span class="n"&gt;embedding_layer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GlobalAveragePooling1D&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
  &lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sigmoid&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;adam&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
              &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;binary_crossentropy&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
              &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;accuracy&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;history&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And that's a basic implementation of negative sampling. We typically don't want to do negative sampling manually, so luckily gensim and tensorflow do it automatically (however at the time of this post we are &lt;a href="https://github.com/tensorflow/tensorflow/issues/34131"&gt;still waiting&lt;/a&gt; for an implementation in the tensorflow keras api).&lt;/p&gt;
&lt;h3&gt;Resources&lt;/h3&gt;
&lt;p&gt;1) &lt;a href="https://github.com/stanfordnlp/cs224n-winter17-notes/blob/master/notes1.pdf"&gt;Notes from Stanford NLP course&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;2) &lt;a href="https://arxiv.org/pdf/1301.3781.pdf"&gt;word2vec paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;3) &lt;a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/"&gt;word2vec Tutorial Part 2 - Negative Sampling&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="posts"></category><category term="machine learning"></category><category term="natural language processing"></category><category term="word2vec"></category></entry><entry><title>word2vec - CBOW and Skip-Gram</title><link href="http://www.jasonosajima.com/word2vec.html" rel="alternate"></link><published>2019-12-09T00:00:00-08:00</published><updated>2019-12-09T00:00:00-08:00</updated><author><name>Jason Osajima</name></author><id>tag:www.jasonosajima.com,2019-12-09:/word2vec.html</id><summary type="html">
&lt;p&gt;word2vec is an iterative model that can be used to create embeddings of words (or embeddings of &lt;a href="https://medium.com/airbnb-engineering/listing-embeddings-for-similar-listing-recommendations-and-real-time-personalization-in-search-601172f7603e"&gt;pretty much anything&lt;/a&gt;). In this post, we will talk briefly about why you would want to use word2vec, break down the Continuous Bag of Words (CBOW) and skip gram word2vec model, and implement it in tensorflow.
&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;This is part one in a two-part series on the word2vec. Part one is about CBOW and Skip-Gram. Part two is about negative sampling and can be found &lt;a href="/ns"&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;


&lt;p&gt;word2vec is an iterative model that can be used to create embeddings of words (or embeddings of &lt;a href="https://medium.com/airbnb-engineering/listing-embeddings-for-similar-listing-recommendations-and-real-time-personalization-in-search-601172f7603e"&gt;pretty much anything&lt;/a&gt;). In this post, we will talk briefly about why you would want to use word2vec, break down the Continuous Bag of Words (CBOW) and skip gram word2vec model, and implement it in tensorflow.
&lt;/p&gt;
&lt;h3&gt;Why use word2vec?&lt;/h3&gt;
&lt;p&gt;For a lot of machine learning tasks, we need to figure out the relationship between items. Take for example an NLP task where we use words as items. We somehow need to learn numerical representations of words so that the model can understand the relationship between words. The most straightforward way of representing words numerically would be to represent them as vectors. So, imagine that we trained a model and learned vectors for each word. We'd expect that the word "cat" and "dog" would be close in distance since they are both pets, and that "cat" would be far from "gym", because there isn't much of a relationship between these two words. We sometimes call these vectors embeddings.&lt;/p&gt;
&lt;h3&gt;What is word2vec?&lt;/h3&gt;
&lt;p&gt;word2vec is a model that attempts to learn these embeddings based on a corpus of text (or a group of items). The basic idea is that we initialize a vector for each word in the corpus with random numbers. We then iterate through each word of each document (a document is just a group of words that are related), grab the vectors of the closest n-words on either side of our target word, concatenate these vectors, forward propagate it through a linear layer + softmax function, and attempt to predict what our target word was. We then backpropagate the error between our prediction and the actual target word, and update not only the weights of the linear layer, but also the vectors (or embeddings) of our neighbor words.&lt;/p&gt;
&lt;p&gt;Imagine that we have a corpus of two documents:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;corpus&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;document_1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;document_2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The documents are just one sentence long:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;document1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;the&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;cat&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;loves&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;fish&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;document2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;the&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;person&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;hates&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;fish&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So the goal of using word2vec is to learn embeddings for all of the words in our corpus. In this case, the words in our corpus are:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;word_to_ix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;the&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;cat&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;loves&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;fish&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;person&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;hates&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can create our vocab &lt;code&gt;word_to_ix&lt;/code&gt; using the following code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;def&lt;/span&gt; &lt;span class="nv"&gt;corpus_to_vocab&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;corpus&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;:
    &lt;span class="s2"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="nv"&gt;Takes&lt;/span&gt; &lt;span class="nv"&gt;a&lt;/span&gt; &lt;span class="nv"&gt;corpus&lt;/span&gt; &lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;list&lt;/span&gt; &lt;span class="nv"&gt;of&lt;/span&gt; &lt;span class="nv"&gt;documents&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt; &lt;span class="nv"&gt;and&lt;/span&gt; &lt;span class="nv"&gt;converts&lt;/span&gt;
    &lt;span class="nv"&gt;it&lt;/span&gt; &lt;span class="nv"&gt;to&lt;/span&gt; &lt;span class="nv"&gt;two&lt;/span&gt; &lt;span class="nv"&gt;dictionaries&lt;/span&gt;:
      &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nv"&gt;word_to_ix&lt;/span&gt;: &lt;span class="nv"&gt;key&lt;/span&gt; &lt;span class="nv"&gt;are&lt;/span&gt; &lt;span class="nv"&gt;words&lt;/span&gt; &lt;span class="nv"&gt;in&lt;/span&gt; &lt;span class="nv"&gt;vocab&lt;/span&gt;, &lt;span class="nv"&gt;values&lt;/span&gt;
        &lt;span class="nv"&gt;are&lt;/span&gt; &lt;span class="nv"&gt;the&lt;/span&gt; &lt;span class="nv"&gt;unique&lt;/span&gt; &lt;span class="nv"&gt;indices&lt;/span&gt;
      &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nv"&gt;ix_to_word&lt;/span&gt;: &lt;span class="nv"&gt;key&lt;/span&gt; &lt;span class="nv"&gt;are&lt;/span&gt; &lt;span class="nv"&gt;the&lt;/span&gt; &lt;span class="nv"&gt;unique&lt;/span&gt; &lt;span class="nv"&gt;indices&lt;/span&gt;,
        &lt;span class="nv"&gt;values&lt;/span&gt; &lt;span class="nv"&gt;are&lt;/span&gt; &lt;span class="nv"&gt;the&lt;/span&gt; &lt;span class="nv"&gt;words&lt;/span&gt; &lt;span class="nv"&gt;in&lt;/span&gt; &lt;span class="nv"&gt;vocab&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="nv"&gt;word_to_ix&lt;/span&gt;, &lt;span class="nv"&gt;ix_to_word&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; {}, {}
    &lt;span class="nv"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;document&lt;/span&gt; &lt;span class="nv"&gt;in&lt;/span&gt; &lt;span class="nv"&gt;corpus&lt;/span&gt;:
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;word&lt;/span&gt; &lt;span class="nv"&gt;in&lt;/span&gt; &lt;span class="nv"&gt;document&lt;/span&gt;:
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nv"&gt;word&lt;/span&gt; &lt;span class="nv"&gt;not&lt;/span&gt; &lt;span class="nv"&gt;in&lt;/span&gt; &lt;span class="nv"&gt;word_to_ix&lt;/span&gt;.&lt;span class="nv"&gt;keys&lt;/span&gt;&lt;span class="ss"&gt;()&lt;/span&gt;:
                &lt;span class="nv"&gt;word_to_ix&lt;/span&gt;[&lt;span class="nv"&gt;word&lt;/span&gt;], &lt;span class="nv"&gt;ix_to_word&lt;/span&gt;[&lt;span class="nv"&gt;ix&lt;/span&gt;] &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;ix&lt;/span&gt;, &lt;span class="nv"&gt;word&lt;/span&gt;
                &lt;span class="nv"&gt;ix&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nv"&gt;word_to_ix&lt;/span&gt;, &lt;span class="nv"&gt;ix_to_word&lt;/span&gt;

&lt;span class="nv"&gt;EMBEDDING_DIM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;

&lt;span class="nv"&gt;document1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; [&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s"&gt;the&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s"&gt;cat&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s"&gt;loves&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s"&gt;fish&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;]
&lt;span class="nv"&gt;document2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; [&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s"&gt;the&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s"&gt;person&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s"&gt;hates&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;, &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s"&gt;fish&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;]
&lt;span class="nv"&gt;corpus&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; [&lt;span class="nv"&gt;document1&lt;/span&gt;, &lt;span class="nv"&gt;document2&lt;/span&gt;]

# &lt;span class="nv"&gt;vocab&lt;/span&gt;
&lt;span class="nv"&gt;word_to_ix&lt;/span&gt;, &lt;span class="nv"&gt;ix_to_word&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;corpus_to_vocab&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;corpus&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And we can instantiate embeddings for each of these words in a matrix where the number of rows are equal to the number of words in our vocab and the number of columns is the number of dimensions of our embedding vector. Let's make it simple and work with 3 dimensional embeddings.&lt;/p&gt;
&lt;div class="math"&gt;$$\textbf{V} = \begin{bmatrix}
  1 &amp;amp; 1 &amp;amp; 4 \\
   5 &amp;amp; 5 &amp;amp; 1 \\
   2 &amp;amp; 1 &amp;amp; 5
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;If we were interested in looking up the embedding for the word &lt;code&gt;the&lt;/code&gt;, we would lookup &lt;code&gt;the&lt;/code&gt; in our &lt;code&gt;vocab&lt;/code&gt;, get the index &lt;span class="math"&gt;\(0\)&lt;/span&gt;, and return the embedding &lt;code&gt;[1, 1, 4]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now that we've explained the setup of word embeddings, how do we learn them? There are two main ways to learn embeddings using word2vec: Continuous Bag of Words (CBOW), and skipgram. We'll start with explaining CBOW.&lt;/p&gt;
&lt;h2&gt;Continuous Bag of Words (CBOW)&lt;/h2&gt;
&lt;p&gt;The first step for implementing CBOW is to instantiate the embedding matrix described above. Let's create an embedding matrix for the words in our vocab using tensorflow.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;tensorflow&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;keras&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;tensorflow.keras&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;layers&lt;/span&gt;

&lt;span class="n"&gt;N_WORDS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word_to_ix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="n"&gt;embedding_layer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Embedding&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N_WORDS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;EMBEDDING_DIM&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
                                   &lt;span class="n"&gt;embeddings_initializer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;RandomNormal&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                   &lt;span class="n"&gt;input_shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We define an &lt;code&gt;input_shape&lt;/code&gt; because the embedding layer is the first layer of our model. The reason why it has an input shape of &lt;span class="math"&gt;\((2,)\)&lt;/span&gt; is that for each target word, we pass in as input two context words, represented as indices. So if we wanted to pass in "the" and "loves", we would pass in the vector &lt;code&gt;[0,2]&lt;/code&gt;. &lt;/p&gt;
&lt;p&gt;Great, so now we have an embedding matrix. As an example, we can look up the embedding for &lt;code&gt;the&lt;/code&gt; by passing a &lt;code&gt;0&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;embedding_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Next we need to setup our training set for CBOW. Our output is a word in a document, and the input is the context of the word, which is just the n-words to the left and right of the output word. So for example, if we were converting &lt;code&gt;document1&lt;/code&gt; to our training set, we would end up with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;[([&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;the&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;loves&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;cat&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
 &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="ss"&gt;&amp;quot;cat&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;fish&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;loves&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Notice we don't include &lt;code&gt;"the"&lt;/code&gt; and &lt;code&gt;"fish"&lt;/code&gt; in our train set, because we wouldn't have enough context words to construct the training example for them. This isn't a problem when we have a large corpus (but might look like a problem with our small corpus of two document sentences).&lt;/p&gt;
&lt;p&gt;We'll convert the words in the train set to indices so it's easy to lookup the word's embeddings:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;[([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
 &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In code, this looks like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;train_set&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; []
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;document&lt;/span&gt; &lt;span class="nv"&gt;in&lt;/span&gt; &lt;span class="nv"&gt;corpus&lt;/span&gt;:
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;i&lt;/span&gt; &lt;span class="nv"&gt;in&lt;/span&gt; &lt;span class="nv"&gt;range&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;, &lt;span class="nv"&gt;len&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;document&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;:
        &lt;span class="nv"&gt;target_word&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;word_to_ix&lt;/span&gt;[&lt;span class="nv"&gt;document&lt;/span&gt;[&lt;span class="nv"&gt;i&lt;/span&gt;]]
        &lt;span class="nv"&gt;context&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; [&lt;span class="nv"&gt;word_to_ix&lt;/span&gt;[&lt;span class="nv"&gt;document&lt;/span&gt;[&lt;span class="nv"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;]], &lt;span class="nv"&gt;word_to_ix&lt;/span&gt;[&lt;span class="nv"&gt;document&lt;/span&gt;[&lt;span class="nv"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;]]]
        &lt;span class="nv"&gt;train_set&lt;/span&gt;.&lt;span class="nv"&gt;append&lt;/span&gt;&lt;span class="ss"&gt;((&lt;/span&gt;&lt;span class="nv"&gt;context&lt;/span&gt;, &lt;span class="nv"&gt;target_word&lt;/span&gt;&lt;span class="ss"&gt;))&lt;/span&gt;

&lt;span class="nv"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;np&lt;/span&gt;.&lt;span class="nv"&gt;array&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;[&lt;span class="nv"&gt;example&lt;/span&gt;[&lt;span class="mi"&gt;0&lt;/span&gt;] &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;example&lt;/span&gt; &lt;span class="nv"&gt;in&lt;/span&gt; &lt;span class="nv"&gt;train_set&lt;/span&gt;]&lt;span class="ss"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;np&lt;/span&gt;.&lt;span class="nv"&gt;array&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;[&lt;span class="nv"&gt;example&lt;/span&gt;[&lt;span class="mi"&gt;1&lt;/span&gt;] &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;example&lt;/span&gt; &lt;span class="nv"&gt;in&lt;/span&gt; &lt;span class="nv"&gt;train_set&lt;/span&gt;]&lt;span class="ss"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;keras&lt;/span&gt;.&lt;span class="nv"&gt;utils&lt;/span&gt;.&lt;span class="nv"&gt;to_categorical&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;y&lt;/span&gt;, &lt;span class="nv"&gt;num_classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;N_WORDS&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now that we constructed our train set, let's setup our model.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;keras&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Sequential&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
  &lt;span class="n"&gt;embedding_layer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GlobalAveragePooling1D&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
  &lt;span class="n"&gt;layers&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dense&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;N_WORDS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;softmax&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Let's go through the three layers of this model. &lt;code&gt;embedding_layer&lt;/code&gt; was already described above, and takes as input two indices for the two context words and returns their two embeddings. The output therefore has shape &lt;code&gt;(None,2,3)&lt;/code&gt;. The first dimension is &lt;code&gt;None&lt;/code&gt; because it depends on how many words we pass into the model during training. So it could be &lt;span class="math"&gt;\(1\)&lt;/span&gt; if we pass in one training example, and &lt;span class="math"&gt;\(5\)&lt;/span&gt; if we pass in five training examples. &lt;code&gt;GlobalAveragePooling1d&lt;/code&gt; takes the average of the two embeddings, and its output is  &lt;code&gt;(None,1,3)&lt;/code&gt;. Finally, &lt;code&gt;Dense&lt;/code&gt; is a fully-connected layer that multiplies the output of &lt;code&gt;GlobalAveragePooling1d&lt;/code&gt; by a weight matrix and adds a bias. The resulting vector is then passed through a &lt;code&gt;softmax&lt;/code&gt; activation, so that we end up with a vector of probabilities for the index of the target output word.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;adam&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
              &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;categorical_crossentropy&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
              &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;accuracy&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;history&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Next, we compile the model, using &lt;code&gt;categorical_crossentropy&lt;/code&gt; as our loss function since this is a multi-category problem, and then we are done! Obviously, our word embeddings won't be very "good" because we only trained a model on  two sentences. To check how the embeddings for the word &lt;code&gt;the&lt;/code&gt; changed, we just pass:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nf"&gt;embedding_layer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word_to_ix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="ss"&gt;`the`&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So what changes when we use skipgram?&lt;/p&gt;
&lt;h2&gt;Skip-Gram&lt;/h2&gt;
&lt;p&gt;The main change is that we switch the output and input for the model. So the input into the model is now the target word, and the output are the context words. For example, the input could now be the word &lt;code&gt;cat&lt;/code&gt;, and the output could be the words &lt;code&gt;love&lt;/code&gt; and &lt;code&gt;the&lt;/code&gt;. We represent the input word as its index, so we'd feed in &lt;span class="math"&gt;\(1\)&lt;/span&gt; for the word &lt;code&gt;cat&lt;/code&gt;. What would the output be? If the context is &lt;code&gt;love&lt;/code&gt; and &lt;code&gt;the&lt;/code&gt;, the output would be &lt;code&gt;[1,1,0,0,0]&lt;/code&gt;. I like to call this multi-hot encoding, but I'm not sure if that's the best term for it. We also change the loss to &lt;code&gt;binary_crossentropy&lt;/code&gt;, since we now have &lt;a href="https://github.com/keras-team/keras/issues/2166"&gt;converted our problem to multi-label&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;train_set&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; []

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;document&lt;/span&gt; &lt;span class="nv"&gt;in&lt;/span&gt; &lt;span class="nv"&gt;corpus&lt;/span&gt;:
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;i&lt;/span&gt; &lt;span class="nv"&gt;in&lt;/span&gt; &lt;span class="nv"&gt;range&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;, &lt;span class="nv"&gt;len&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;document&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;:
        &lt;span class="nv"&gt;target_word&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;word_to_ix&lt;/span&gt;[&lt;span class="nv"&gt;document&lt;/span&gt;[&lt;span class="nv"&gt;i&lt;/span&gt;]]
        &lt;span class="nv"&gt;context&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; [&lt;span class="nv"&gt;word_to_ix&lt;/span&gt;[&lt;span class="nv"&gt;document&lt;/span&gt;[&lt;span class="nv"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;]], &lt;span class="nv"&gt;word_to_ix&lt;/span&gt;[&lt;span class="nv"&gt;document&lt;/span&gt;[&lt;span class="nv"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;]]]
        &lt;span class="nv"&gt;train_set&lt;/span&gt;.&lt;span class="nv"&gt;append&lt;/span&gt;&lt;span class="ss"&gt;((&lt;/span&gt;&lt;span class="nv"&gt;target_word&lt;/span&gt;, &lt;span class="nv"&gt;context&lt;/span&gt;&lt;span class="ss"&gt;))&lt;/span&gt;

&lt;span class="nv"&gt;N_WORDS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;len&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;word_to_ix&lt;/span&gt;.&lt;span class="nv"&gt;keys&lt;/span&gt;&lt;span class="ss"&gt;())&lt;/span&gt;
&lt;span class="nv"&gt;embedding_layer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;layers&lt;/span&gt;.&lt;span class="nv"&gt;Embedding&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;N_WORDS&lt;/span&gt;, &lt;span class="nv"&gt;EMBEDDING_DIM&lt;/span&gt;, 
                                   &lt;span class="nv"&gt;embeddings_initializer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="s"&gt;RandomNormal&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;,
                                   &lt;span class="nv"&gt;input_shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;,&lt;span class="ss"&gt;))&lt;/span&gt;

&lt;span class="nv"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;np&lt;/span&gt;.&lt;span class="nv"&gt;array&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;[&lt;span class="nv"&gt;example&lt;/span&gt;[&lt;span class="mi"&gt;0&lt;/span&gt;] &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;example&lt;/span&gt; &lt;span class="nv"&gt;in&lt;/span&gt; &lt;span class="nv"&gt;train_set&lt;/span&gt;]&lt;span class="ss"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;np&lt;/span&gt;.&lt;span class="nv"&gt;array&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;[&lt;span class="nv"&gt;example&lt;/span&gt;[&lt;span class="mi"&gt;1&lt;/span&gt;] &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nv"&gt;example&lt;/span&gt; &lt;span class="nv"&gt;in&lt;/span&gt; &lt;span class="nv"&gt;train_set&lt;/span&gt;]&lt;span class="ss"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;keras&lt;/span&gt;.&lt;span class="nv"&gt;utils&lt;/span&gt;.&lt;span class="nv"&gt;to_categorical&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;y&lt;/span&gt;, &lt;span class="nv"&gt;num_classes&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;N_WORDS&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;np&lt;/span&gt;.&lt;span class="nv"&gt;sum&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;y&lt;/span&gt;, &lt;span class="nv"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;.&lt;span class="nv"&gt;astype&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;int&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;

&lt;span class="nv"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;keras&lt;/span&gt;.&lt;span class="nv"&gt;Sequential&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;[
  &lt;span class="nv"&gt;embedding_layer&lt;/span&gt;,
  &lt;span class="nv"&gt;layers&lt;/span&gt;.&lt;span class="nv"&gt;GlobalAveragePooling1D&lt;/span&gt;&lt;span class="ss"&gt;()&lt;/span&gt;,
  &lt;span class="nv"&gt;layers&lt;/span&gt;.&lt;span class="nv"&gt;Dense&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;N_WORDS&lt;/span&gt;, &lt;span class="nv"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;softmax&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;,
]&lt;span class="ss"&gt;)&lt;/span&gt;

&lt;span class="nv"&gt;model&lt;/span&gt;.&lt;span class="nv"&gt;compile&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;adam&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;,
              &lt;span class="nv"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;binary_crossentropy&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;,
              &lt;span class="nv"&gt;metrics&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;[&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="s"&gt;accuracy&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;]&lt;span class="ss"&gt;)&lt;/span&gt;

&lt;span class="nv"&gt;history&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;model&lt;/span&gt;.&lt;span class="nv"&gt;fit&lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;X&lt;/span&gt;,&lt;span class="nv"&gt;y&lt;/span&gt;, &lt;span class="nv"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;X&lt;/span&gt;.&lt;span class="nv"&gt;shape&lt;/span&gt;[&lt;span class="mi"&gt;0&lt;/span&gt;]&lt;span class="ss"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;So we now know how to implement CBOW and SkipGram word2vec in tensorflow. Hooray! Don't get too excited though: these implementations are not very practical. The reason is because the number of words will not typically be 6, like in our example. Let's imagine that we implemented word2vec using all of wikipedia. There are around 170,000 words currently used in the English language. We can't have a 170,000 one-hot or multi-hot encoded output. Luckily, we have a way to get around this by using negative sampling], which was introduced in the &lt;a href="https://papers.nips.cc/ paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"&gt;original word2vec paper&lt;/a&gt;and will be discussed in &lt;a href="/ns"&gt;part two&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Resources&lt;/h3&gt;
&lt;p&gt;1) &lt;a href="https://github.com/stanfordnlp/cs224n-winter17-notes/blob/master/notes1.pdf"&gt;Notes from Stanford NLP course&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;2) &lt;a href="https://arxiv.org/pdf/1301.3781.pdf"&gt;word2vec paper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;3) &lt;a href="https://stackoverflow.com/questions/38287772/cbow-v-s-skip-gram-why-invert-context-and-target-words"&gt;difference between skipgram and cbow&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="posts"></category><category term="machine learning"></category><category term="natural language processing"></category><category term="word2vec"></category></entry><entry><title>Singular Value Decomposition</title><link href="http://www.jasonosajima.com/svd.html" rel="alternate"></link><published>2019-11-22T00:00:00-08:00</published><updated>2019-11-22T00:00:00-08:00</updated><author><name>Jason Osajima</name></author><id>tag:www.jasonosajima.com,2019-11-22:/svd.html</id><summary type="html">
&lt;p&gt;Some quick notes about Singular Value Decomposition (SVD) to develop an intuition that will help solve problems related to collaborative filtering, natural language processing (NLP), dimensionality reduction, image compression, denoising data etc.&lt;/p&gt;
</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;


&lt;p&gt;Some quick notes about Singular Value Decomposition (SVD) to develop an intuition that will help solve problems related to collaborative filtering, natural language processing (NLP), dimensionality reduction, image compression, denoising data etc.&lt;/p&gt;


&lt;p&gt;Let's imagine we have a matrix &lt;span class="math"&gt;\(\textbf{A}\)&lt;/span&gt; of size &lt;span class="math"&gt;\(m\)&lt;/span&gt; by &lt;span class="math"&gt;\(n\)&lt;/span&gt; where &lt;span class="math"&gt;\(m\)&lt;/span&gt;  is the number of rows and  &lt;span class="math"&gt;\(n\)&lt;/span&gt; is the number of columns. If we were building a recommender system, we can think of each row representing a user, each column representing an item, and each element in the matrix indicating whether the user has interacted with the item. In NLP, we can think of each row representing a document, and each column representing a term.&lt;/p&gt;
&lt;p&gt;The goal of singular value decomposition (SVD) is to take this matrix &lt;span class="math"&gt;\(A\)&lt;/span&gt; and represent it as the product of three matricies:&lt;/p&gt;
&lt;div class="math"&gt;$$\textbf{A} = \textbf{U}\mathbb{\Sigma}
\textbf{V}^T$$&lt;/div&gt;
&lt;p&gt;The dimensions of &lt;span class="math"&gt;\(U\)&lt;/span&gt; are &lt;span class="math"&gt;\(m\)&lt;/span&gt; by &lt;span class="math"&gt;\(r\)&lt;/span&gt;. This matrix stores the left  singular vectors. The dimensions of &lt;span class="math"&gt;\(\mathbb{\Sigma}\)&lt;/span&gt; is &lt;span class="math"&gt;\(r\)&lt;/span&gt; by &lt;span class="math"&gt;\(r\)&lt;/span&gt;. This is a diagonal matrix, and contains singular values. The dimensions of &lt;span class="math"&gt;\(V\)&lt;/span&gt; are &lt;span class="math"&gt;\(n\)&lt;/span&gt; by &lt;span class="math"&gt;\(r\)&lt;/span&gt;. Before we get into why this decomposition is useful, let's talk a little bit about the properties of the singular value decomposition of a matrix. Namely:&lt;/p&gt;
&lt;p&gt;It is always possible to decompose a real matrix &lt;span class="math"&gt;\(\textbf{A}\)&lt;/span&gt; (meaning all values are real numbers) into &lt;span class="math"&gt;\(\textbf{A} = \textbf{U}\mathbb{\Sigma}
\textbf{V}^T\)&lt;/span&gt;. In addition, there is only one unique singular value decomposition for each  &lt;/p&gt;
&lt;p&gt;So why do people use SVD? I think the most interesting way  people use SVD is to create embeddings, which we will talk about next (and will help us understand the three matricies of SVD better).&lt;/p&gt;
&lt;h3&gt;Using SVD for embeddings&lt;/h3&gt;
&lt;p&gt;So let's take the example inspired from this &lt;a href="https://www.youtube.com/watch?v=P5mlg91as1c&amp;amp;t=236s"&gt;video&lt;/a&gt;, where we have a matrix &lt;span class="math"&gt;\(\textbf{A}\)&lt;/span&gt; that has the ratings users give to  movies. Let's say we want to figure out which movies are similar and which users are similar.&lt;/p&gt;
&lt;div class="math"&gt;$$\textbf{A} = \begin{bmatrix}
  1 &amp;amp; 1 &amp;amp; 4 \\
   5 &amp;amp; 5 &amp;amp; 1 \\
   2 &amp;amp; 1 &amp;amp; 5
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;The rows of our matrix represent users, the columns of our matrix represent movies, and the values represent the rating a user gives to a movie. Since Disney+ just came out, let's imagine that our three movies are &lt;em&gt;Star Wars&lt;/em&gt;, &lt;em&gt;Avengers&lt;/em&gt;, and &lt;em&gt;Lady and the Tramp&lt;/em&gt;. Let's take a look at the ratings that are users gave our three movies. Right away, you can tell that user 1 and user 3 are similar because they both  disliked &lt;em&gt;Star Wars&lt;/em&gt; and &lt;em&gt;Avengers&lt;/em&gt; and liked &lt;em&gt;Lady and the Tramp&lt;/em&gt;. User 2 is the opposite because she likes both &lt;em&gt;Star Wars&lt;/em&gt; and &lt;em&gt;Avengers&lt;/em&gt; and dislikes &lt;em&gt;Lady and the Tramp&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Let's now take a look at the  SVD  for this matrix. We can use the &lt;code&gt;numpy&lt;/code&gt; function &lt;code&gt;svd&lt;/code&gt; to solve it quickly:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt; 
&lt;span class="n"&gt;U&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;VT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;svd&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;full_matrices&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# convert S to diagonal matrix&lt;/span&gt;
&lt;span class="n"&gt;S&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="kp"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;S&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The results that we get after running this code are as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$\textbf{U} = \begin{bmatrix}
  -0.40865502 &amp;amp; 0.47145889  &amp;amp; -0.78149062 \\
   -0.73017597 &amp;amp; -0.68260113 &amp;amp; -0.02997898 \\
   -0.54758024 &amp;amp; 0.55837461 &amp;amp; 0.62319633
\end{bmatrix}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\mathbb{\Sigma} = \begin{bmatrix}
  8.59266627 &amp;amp; 0 &amp;amp; 0 \\
  0 &amp;amp; 4.99702707 &amp;amp; 0 \\
  0 &amp;amp; 0 &amp;amp; 0.44250069
\end{bmatrix}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\textbf{V}^T = \begin{bmatrix}
  -0.59989475 &amp;amp; -0.53616828 &amp;amp; -0.5938433 \\
   -0.36517664 &amp;amp; -0.476918 &amp;amp; 0.79949687 \\
   0.71187942 &amp;amp; -0.69647167 &amp;amp; -0.09030447
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;We can think of the rows of the matrix &lt;span class="math"&gt;\(U\)&lt;/span&gt; as embeddings for our three users. Similarly, we can think of the three columns as the embeddings for our three movies. You can think of embeddings as summaries of what we know about things (in this case, what we know about users and movies).&lt;/p&gt;
&lt;p&gt;So how can we use these embeddings? The most straightforward thing we can do is use them to find the similarity between users or movies. So recall from above that we posited that user 1 and user 3 have similar preferences.&lt;/p&gt;
&lt;p&gt;The first value of user 1's embedding is -0.4087. Similarly, the first value of user 2's embedding is -0.7302 and the first value of user 3's embedding is -0.5476. &lt;/p&gt;
&lt;p&gt;Notice that the absolute value of the difference between user 1 and user 2's values is 0.1389 whereas the difference between user 1 and user 3's values is 0.3215. So that matches our intuition, that user 1 and user 2 will  have similar embeddings because they have similar preferences. Cool!&lt;/p&gt;
&lt;p&gt;Why did we pick the first value of the embedding vector to compare instead of the second or third? The values are sorted by importance. You can check the values importance by looking at the diagonal values of &lt;span class="math"&gt;\(\Sigma\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$[8.5927, 4.9970, 0.4425]$$&lt;/div&gt;
&lt;p&gt;What this tells you is that the first value of the embeddings is roughly twice as "important" as the second value, and twenty times as "important" as the third value.&lt;/p&gt;
&lt;p&gt;Notice that we could have compared the embeddings for movies by using the matrix &lt;span class="math"&gt;\(V\)&lt;/span&gt; instead of &lt;span class="math"&gt;\(U\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;So what people usually do to create embeddings for users, items, words, documents, etc. is to take the top &lt;span class="math"&gt;\(k\)&lt;/span&gt; values from the vectors created using SVD. Some people refer to this strategy as Latent Semantic Analysis, or LSA.&lt;/p&gt;
&lt;p&gt;How do you know what the value of &lt;span class="math"&gt;\(k\)&lt;/span&gt; is? A lot of people use trial and error, or by checking the &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html"&gt;silhouette score&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Once you create the embeddings, how do you tell how similar embeddings are? People use cosine similarity, manhattan distance (&lt;span class="math"&gt;\(L1\)&lt;/span&gt;), or euclidian distance (&lt;span class="math"&gt;\(L2\)&lt;/span&gt;). In the example above, I used manhattan distance. Again, there's no default best metric to use and in practice people use trial and error.&lt;/p&gt;
&lt;p&gt;Some of the drawbacks of this method (as mentioned &lt;a href="https://github.com/stanfordnlp/cs224n-winter17-notes/blob/master/notes1.pdf"&gt;here&lt;/a&gt;): &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dimensions of the matrix can change often (adding new users or movies), and each time the dimensions change, SVD must be performed again. &lt;/li&gt;
&lt;li&gt;The matrix can be very high dimensional&lt;/li&gt;
&lt;li&gt;Computational cost to perform SVD for a &lt;span class="math"&gt;\(m\)&lt;/span&gt; by &lt;span class="math"&gt;\(n\)&lt;/span&gt; matrix is &lt;span class="math"&gt;\(O(mn\)&lt;/span&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For these reasons, iterative methods tend to be better, the simplest of these being &lt;a href="/word2vec"&gt;word2vec&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Resources&lt;/h3&gt;
&lt;p&gt;1)  &lt;a href="https://www.youtube.com/watch?v=P5mlg91as1c&amp;amp;t=236s"&gt;Great video on SVD that explains the math&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;2) &lt;a href="http://andrew.gibiansky.com/blog/mathematics/cool-linear-algebra-singular-value-decomposition/"&gt;Using SVD to coompress an image example&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;3) &lt;a href="https://github.com/stanfordnlp/cs224n-winter17-notes/blob/master/notes1.pdf"&gt;Notes from Stanford NLP course&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="posts"></category><category term="machine learning"></category><category term="linear algebra"></category><category term="natural language processing"></category></entry><entry><title>Convolutional Networks - VGG16</title><link href="http://www.jasonosajima.com/convnets_vgg.html" rel="alternate"></link><published>2018-08-18T00:00:00-07:00</published><updated>2018-08-18T00:00:00-07:00</updated><author><name>Jason Osajima</name></author><id>tag:www.jasonosajima.com,2018-08-18:/convnets_vgg.html</id><summary type="html">
The Imagenet Large Scale Visual Recognition Challenge (&lt;a href="http://www.image-net.org/challenges/LSVRC/"&gt;ILSVRC&lt;/a&gt;) is an annual computer vision competition. Each year, teams compete on two tasks. The first is to detect objects within an image coming from 200 classes, which is called object localization. The second is to classify images, each labeled with one of 1000 categories, which is called image classification.
&lt;p&gt;In 2012, Alex Krizhevsky, Ilya Sutskever and Geoffrey E. Hinton won the competition by a sizable margin using a convolutional network (ConvNet) named &lt;a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"&gt;AlexNet&lt;/a&gt;. This became a watershed moment for deep learning.&lt;/p&gt;
&lt;p&gt;Two years later, Karen Simonyan and Andrew Zisserman won 1st and 2nd place in the two tasks described above. Their model was also a ConvNet named VGG-19. VGG is the acronym for their lab at Oxford (Visual Geometry Group) and 19 is the number of layers in the model with trainable parameters.&lt;/p&gt;
&lt;p&gt;What attracted me to this model was its simplicity - the model shares most of the same basic architecture and algorithms as &lt;a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf"&gt;LeNet5&lt;/a&gt;, one of the first ConvNets from the 90s. The main difference is the addition of several more layers (from 5 to 19), which seems to validate the idea that deeper networks are able to learn better representations (this trend continues with the introduction of Residual Networks, which won IVCLR the following year with a whopping 152 layers).&lt;/p&gt;
</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;
The Imagenet Large Scale Visual Recognition Challenge (&lt;a href="http://www.image-net.org/challenges/LSVRC/"&gt;ILSVRC&lt;/a&gt;) is an annual computer vision competition. Each year, teams compete on two tasks. The first is to detect objects within an image coming from 200 classes, which is called object localization. The second is to classify images, each labeled with one of 1000 categories, which is called image classification.&lt;/p&gt;
&lt;p&gt;In 2012, Alex Krizhevsky, Ilya Sutskever and Geoffrey E. Hinton won the competition by a sizable margin using a convolutional network (ConvNet) named &lt;a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"&gt;AlexNet&lt;/a&gt;. This became a watershed moment for deep learning.&lt;/p&gt;
&lt;p&gt;Two years later, Karen Simonyan and Andrew Zisserman won 1st and 2nd place in the two tasks described above. Their model was also a ConvNet named VGG-19. VGG is the acronym for their lab at Oxford (Visual Geometry Group) and 19 is the number of layers in the model with trainable parameters.&lt;/p&gt;
&lt;p&gt;What attracted me to this model was its simplicity - the model shares most of the same basic architecture and algorithms as &lt;a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf"&gt;LeNet5&lt;/a&gt;, one of the first ConvNets from the 90s. The main difference is the addition of several more layers (from 5 to 19), which seems to validate the idea that deeper networks are able to learn better representations (this trend continues with the introduction of Residual Networks, which won IVCLR the following year with a whopping 152 layers).&lt;/p&gt;


&lt;p&gt;Similar to my first post on &lt;a href="/forwardprop"&gt;forward propagation&lt;/a&gt; and &lt;a href="/backprop"&gt;backpropagation&lt;/a&gt; for a vanilla neural network, I will walk through forward propagation and backpropagation for VGG-16 and discuss some of the advantages of using a ConvNet over a fully-connected neural network for computer vision tasks. VGG-16 is comparable in performance to VGG-19 but is simpler (it has three fewer layers) so we will roll with that.&lt;/p&gt;
&lt;h2&gt;Objective&lt;/h2&gt;
&lt;p&gt;The objective for this task is to predict 5 (out of 1000) classes for each one of 100,000 test set images. The actual (ground truth) image label has to be one of the five predicted classes.&lt;/p&gt;
&lt;p&gt;So our input will be the pixel values of an image. The images in the ILSVRC dataset are fixed-size 224 x 224 RGB. What this means is that for each color channel (red, green and blue) each image has 224 x 224 pixel values. We can represent each input as a tensor with dimensions &lt;span class="math"&gt;\((224, 224, 3)\)&lt;/span&gt; and label it as &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;Each pixel value is a scalar and can take on values between 0 and 225. We can represent a pixel value as &lt;span class="math"&gt;\(x_{(i,j,k)}\)&lt;/span&gt;, where i is the value for the first dimension, j for the second, and k for the third (which recall is the channel).&lt;/p&gt;
&lt;p&gt;The first two dimensions represent the location of the pixel value within the image, and the third dimension is the channel that pixel value belongs to (so for example, if the third dimension of a pixel value is equal to 1, it belongs to the red channel).&lt;/p&gt;
&lt;p&gt;Representing the image as a tensor looks something like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/input_x.png" width="400" title="[input_x]" alt="[input_x]"&gt;&lt;/p&gt;
&lt;p&gt;Our output &lt;span class="math"&gt;\(\hat{\textbf{y}}\)&lt;/span&gt; will be a vector of probabilities for each of the 1000 classes for the given image. &lt;/p&gt;
&lt;div class="math"&gt;$$
\hat{\textbf{y}} =
\begin{bmatrix}
\hat{y}_{1} \\\\
\hat{y}_{2} \\\\ 
\vdots \\\\
\hat{y}_{1000}
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Let's say we input an image &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; and the model believes the image belongs to class 1 with probability 5%, class 2 with probability 7%, class 4 with probability 9%, class 999 with probability 2%, class 1000 with probability 77%, and all other classes with probability 0%. In this situation, our output &lt;span class="math"&gt;\(\hat{\textbf{y}}\)&lt;/span&gt; would look like this:&lt;/p&gt;
&lt;div class="math"&gt;$$
\hat{\textbf{y}} =
\begin{bmatrix}
0.05 \\\\ 
0.07 \\\\
0 \\\\
0.09 \\\\ 
\vdots \\\\
0.02 \\\\
0.77
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Notice that the sum of the elements in &lt;span class="math"&gt;\(\hat{\textbf{y}}\)&lt;/span&gt; are equal to &lt;span class="math"&gt;\(1\)&lt;/span&gt;. We use a softmax function at the end of our ConvNet to ensure this property. We'll discuss the softmax function later.&lt;/p&gt;
&lt;p&gt;In order to compute accuracy, we use &lt;span class="math"&gt;\(\hat{\textbf{y}}\)&lt;/span&gt; to create a vector of the top 5 classes in decreasing order of probability. &lt;/p&gt;
&lt;div class="math"&gt;$$
\textbf{c} =
\begin{bmatrix}
c_1 \\\\ 
c_2 \\\\
c_3 \\\\
c_4 \\\\ 
c_5
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Using our example, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$
\textbf{c} =
\begin{bmatrix}
1000 \\\\ 
2 \\\\
1 \\\\
4 \\\\ 
999
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Let's say the image was an image of a dog on a street. The picture of the dog is the 4th class. The picture of the street is the 1000th class. Since the image has two ground truth classes, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$
\textbf{C} =
\begin{bmatrix}
C_1 \\\\ 
C_2
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Using our example, we get for our ground truth labels:&lt;/p&gt;
&lt;div class="math"&gt;$$
\textbf{C} =
\begin{bmatrix}
4 \\\\ 
1000
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Let &lt;span class="math"&gt;\(d(c_i, C_k) = 0\)&lt;/span&gt; if &lt;span class="math"&gt;\(c_i = C_k\)&lt;/span&gt; and 1 otherwise. To calculate the error of the algorithm, we use:&lt;/p&gt;
&lt;div class="math"&gt;$$e = \dfrac{1}{n} \sum_k \min_i d(c_i, C_k)$$&lt;/div&gt;
&lt;p&gt;For our example, the error is:&lt;/p&gt;
&lt;div class="math"&gt;$$e = \dfrac{1}{2}\big( \min_i d(c_i, C_1) + \min_i d(c_i, C_2) \big)$$&lt;/div&gt;
&lt;div class="math"&gt;$$e = \dfrac{1}{2}\big( 0 + 0 \big)$$&lt;/div&gt;
&lt;div class="math"&gt;$$e = 0$$&lt;/div&gt;
&lt;p&gt;Since both of the ground truth labels were in our top 5, we get an error of &lt;span class="math"&gt;\(0\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Considering Batches of &lt;span class="math"&gt;\(m\)&lt;/span&gt; training examples&lt;/h3&gt;
&lt;p&gt;We just deconstructed the input and output for our model. Hopefully it makes sense. So far, we've only considered the input and output for one example. What if we had &lt;span class="math"&gt;\(m\)&lt;/span&gt; training examples that we wanted to input into the model as a batch? It's actually not too bad, we are just going to add another dimension.&lt;/p&gt;
&lt;div class="math"&gt;$$
\textbf{X} = [\textbf{x}^{(1)}, \textbf{x}^{(2)}, ... \textbf{x}^{(m)}]
$$&lt;/div&gt;
&lt;p&gt;So we can think of &lt;span class="math"&gt;\(X\)&lt;/span&gt; as a tensor with dimensions &lt;span class="math"&gt;\((m, 224, 224, 3)\)&lt;/span&gt;, where &lt;span class="math"&gt;\(m\)&lt;/span&gt; is the number of examples in our batch. The superscript denotes which training example it is in the batch. So &lt;span class="math"&gt;\(\textbf{x}^{(1)}\)&lt;/span&gt; would be the 1st training example from the batch. Similarly for output:&lt;/p&gt;
&lt;div class="math"&gt;$$
\textbf{Y} = [\textbf{y}^{(1)}, \textbf{y}^{(2)}, ... \textbf{y}^{(m)}]
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(Y\)&lt;/span&gt; is a tensor with dimensions &lt;span class="math"&gt;\((1000, m)\)&lt;/span&gt;, and we use the same superscript notation as above.&lt;/p&gt;
&lt;h2&gt;Defining the Architecture&lt;/h2&gt;
&lt;p&gt;Let's see if we can represent all 16 layers of this model visually:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/VGG_1.png" title="[VGG_1]" alt="[VGG_1]"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Diagram of the architecture of VGG-16&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;If you notice, layers are represented as either 3D rectangular prisms (the layers on the left) or 2D rectangles (the layers on the right). This was done on purpose to represent the dimensions of the layer. For example, recall that the input &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; is a 3D tensor &lt;span class="math"&gt;\((224, 224, 3)\)&lt;/span&gt; and is represented on the far left as a prism. Our output &lt;span class="math"&gt;\(\hat{\textbf{y}}\)&lt;/span&gt; is a matrix &lt;span class="math"&gt;\((1000, 1)\)&lt;/span&gt; and is represented on the far right as a rectangle.&lt;/p&gt;
&lt;p&gt;Alright. If you are crazy you might have counted the layers and noticed that there are 24 layers in this diagram. But this model is called VGG-16. So the 16 refers to the number of layers that have trainable parameters. I'll highlight and label them - &lt;/p&gt;
&lt;p&gt;&lt;img src="/images/VGG_2.png" title="[VGG_2]" alt="[VGG_2]"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Diagram of the architecture of VGG-16 with trainable parameters highlighted in red.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;There are two types of layers with trainable parameters that are highlighted. The 3D ones on the left are Conv Layers, and the ones on the right are Fully-connected Layers.&lt;/p&gt;
&lt;p&gt;What are the other layers that don't have trainable parameters? Recall that the layers on the far left and far right are the input &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; and output &lt;span class="math"&gt;\(\hat{\textbf{y}}\)&lt;/span&gt; layers. In order to get the output &lt;span class="math"&gt;\(\hat{\textbf{y}}\)&lt;/span&gt; layer we apply the Softmax function, so we will call this layer the Softmax Layer.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/VGG_3.png" title="[VGG_3]" alt="[VGG_3]"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Diagram of the architecture of VGG-16 with input and output highlighted in blue&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The layers that follow the string of Conv Layers are called Pooling Layers.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/VGG_4.png" title="[VGG_4]" alt="[VGG_4]"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Diagram of the architecture of VGG-16 with Pooling Layers highlighted in green&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Finally, there's a layer where we flatten the 3D tensor into a column vector. I don't think this layer has an official name, so we'll call it a Flat Layer.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/VGG_5.png" title="[VGG_5]" alt="[VGG_5]"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Diagram of the architecture of VGG-16 with Flat Layer highlighted in purple.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;So within this architecture, there are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Conv Layers&lt;/li&gt;
&lt;li&gt;Pooling Layers&lt;/li&gt;
&lt;li&gt;Flat Layer&lt;/li&gt;
&lt;li&gt;Softmax Layer&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the next few sections, we will take an example from each and describe how the math works. We will ask the question, "what operations are we applying to the previous layer to get to the current layer?" We will start with Conv Layers, which are the integral part of Conv Nets.&lt;/p&gt;
&lt;h2&gt;Conv Layers&lt;/h2&gt;
&lt;p&gt;In &lt;a href="https://www.deeplearningbook.org/contents/convnets.html"&gt;The Deep Learning Book&lt;/a&gt;, the authors describe the difference between Conv Nets and Neural Networks:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Recall in our previous &lt;a href="/forwardprop"&gt;neural network example&lt;/a&gt;, the transition from the first layer &lt;span class="math"&gt;\(\textbf{a}^{[0]}\)&lt;/span&gt; to the second layer &lt;span class="math"&gt;\(\textbf{a}^{[1]}\)&lt;/span&gt; was simply a weight matrix multiplication, the addition of a bias, and the element-wise application of the ReLU function.&lt;/p&gt;
&lt;div class="math"&gt;$$\textbf{a}^{[1]} = g(\textbf{W}^{[1]}\textbf{a}^{[0]} + \textbf{b}^{[1]})$$&lt;/div&gt;
&lt;p&gt;For VGG-16, the only difference that we will be making is replacing the general matrix multiplication with a convolution, and instead of a 2-D weight matrix, we will be using a 3-D filter tensor.&lt;/p&gt;
&lt;p&gt;Let's deconstruct the first Conv Layer in the diagram, the one that succeeds the input layer (&lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt;). &lt;/p&gt;
&lt;p&gt;&lt;img src="/images/VGG_6.png" title="[VGG_6]" alt="[VGG_6]"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Diagram of the architecture of VGG-16 with example Conv Layer and preceding input layer highlighted.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let's first cover the dimensions of each. We discussed earlier that our input is a picture, with dimensions 224 by 224 by 3. We labeled this tensor &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/input_x.png" width="400" title="[input_x]" alt="[input_x]"&gt;&lt;/p&gt;
&lt;p&gt;As in our previous post on forward prop, we can think of this layer as &lt;span class="math"&gt;\(\textbf{a}^{[0]}\)&lt;/span&gt;. Let's call the first Conv Layer &lt;span class="math"&gt;\(\textbf{a}^{[1]}\)&lt;/span&gt;. This layer has dimensions 224 by 224 by 64:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/VGG_7.png" width="400" title="[VGG_7]" alt="[VGG_7]"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Diagram of first Conv Layer &lt;span class="math"&gt;\(\textbf{a}^{[1]}\)&lt;/span&gt; with dimensions &lt;span class="math"&gt;\((224, 224, 64)\)&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;So how do we go from &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; to &lt;span class="math"&gt;\(\textbf{a}^{[1]}\)&lt;/span&gt;? We start by applying the convolution function to &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; 6 times using 6 different filters and then add a bias &lt;span class="math"&gt;\(\textbf{b}^{[1]}\)&lt;/span&gt; to get our intermediate product, &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt;. We'll call the collection of filters &lt;span class="math"&gt;\(\textbf{W}^{[1]}_c\)&lt;/span&gt;, with dimensions &lt;span class="math"&gt;\((16, 16, 3, 6)\)&lt;/span&gt;. The first three dimensions represent the height, width, and number of channels, and the last dimension is the filter. We can represent each filter with &lt;span class="math"&gt;\(\textbf{W}^{[1]}_c(i)\)&lt;/span&gt;, where &lt;span class="math"&gt;\(i\)&lt;/span&gt; is the filter number in &lt;span class="math"&gt;\(\textbf{W}^{[1]}_c\)&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;We add the same bias for each filter, so &lt;span class="math"&gt;\(\textbf{b}^{[1]}\)&lt;/span&gt; has dimensions &lt;span class="math"&gt;\((6,1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let's see if we can represent this visually:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/VGG_8.png" width="600" title="[VGG_8]" alt="[VGG_8]"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Diagram of transition from &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; to &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt;. A convolution is applied to &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; 6 times using 6 different filters &lt;span class="math"&gt;\(\textbf{W}^{[1]}_{c(1)}\)&lt;/span&gt;, &lt;span class="math"&gt;\(\textbf{W}^{[1]}_{c(2)}\)&lt;/span&gt;, ... &lt;span class="math"&gt;\(\textbf{W}^{[1]}_{c(6)}\)&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let's simplify the dimensions to make it easier to visualize. &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; is a tensor with dimensions &lt;span class="math"&gt;\((224, 224, 3)\)&lt;/span&gt;. Let's instead make it &lt;span class="math"&gt;\((5, 5, 3)\)&lt;/span&gt;. &lt;span class="math"&gt;\(\textbf{W}^{[1]}_{c}\)&lt;/span&gt; has dimensions &lt;span class="math"&gt;\((16, 16, 3, 64)\)&lt;/span&gt;. Let's make it &lt;span class="math"&gt;\((3,3,3,6)\)&lt;/span&gt;. We won't change the dimensions for &lt;span class="math"&gt;\(\textbf{b}^{[1]}\)&lt;/span&gt;. Finally, for &lt;span class="math"&gt;\(\textbf{z}^{[1]}_{(1)}\)&lt;/span&gt; we'll change it's dimensions from &lt;span class="math"&gt;\((224, 224, 64)\)&lt;/span&gt; to &lt;span class="math"&gt;\((3,3,6)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/VGG_9.png" title="[VGG_9]" alt="[VGG_9]"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Diagram of convolution between the simplified &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; and the 6 filters of &lt;span class="math"&gt;\(\textbf{W}^{[1]}_{c}\)&lt;/span&gt;, resulting in the 6 channels of &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h3&gt;Casting the Bias (&lt;span class="math"&gt;\(\textbf{b}^{[1]}\)&lt;/span&gt;)&lt;/h3&gt;
&lt;p&gt;If you noticed in our diagram, for each of the six elements in &lt;span class="math"&gt;\(\textbf{b}^{[1]}\)&lt;/span&gt;, we repeated it several times to create a tensor that matched the shape of &lt;span class="math"&gt;\(\textbf{z}^{[1]}_(i)\)&lt;/span&gt; so for example, the scalar &lt;span class="math"&gt;\(b^{[1]}_{(1,1)}\)&lt;/span&gt; was converted into a tensor of shape &lt;span class="math"&gt;\((3,3,3)\)&lt;/span&gt;. We do this because the addition of the bias is elementwise, meaning we add the bias to each element of the product of our convlution.&lt;/p&gt;
&lt;p&gt;So how do we get a value for &lt;span class="math"&gt;\(z^{[1]}_{(i,j,k)}\)&lt;/span&gt; for a given row &lt;span class="math"&gt;\(i\)&lt;/span&gt;, column &lt;span class="math"&gt;\(j\)&lt;/span&gt;, and channel &lt;span class="math"&gt;\(k\)&lt;/span&gt; in &lt;span class="math"&gt;\(z^{[1]}\)&lt;/span&gt;? Unsurprisingly, we use the convolution function:&lt;/p&gt;
&lt;div class="math"&gt;$$z^{[1]}_{(i,j,k)} = (\textbf{x} * \textbf{W}^{[1]}_{c})(i,j,k) + b^{[1]}_{(k,1)}$$&lt;/div&gt;
&lt;p&gt;Which becomes:&lt;/p&gt;
&lt;div class="math"&gt;$$z^{[1]}_{(i,j,k)} = \sum^{3}_{l = 1}\sum^{3}_{m = 1}\sum^{3}_{n = 1}x_{(i + l - 1, j + m - 1, n)}W^{[1]}_{c(l,m,n,k)} + b^{[1]}_{(k,1)}$$&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;Technically, this is not a convolution but a related function called the cross-correlation. But most deep learning libraries and papers use cross-correlation as the convolution function.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So we have defined &lt;span class="math"&gt;\(i\)&lt;/span&gt;, &lt;span class="math"&gt;\(j\)&lt;/span&gt;, &lt;span class="math"&gt;\(k\)&lt;/span&gt; as the coordinates of our end product, &lt;span class="math"&gt;\(z^{[1]}\)&lt;/span&gt;, but what does &lt;span class="math"&gt;\(l\)&lt;/span&gt;, &lt;span class="math"&gt;\(m\)&lt;/span&gt;, and &lt;span class="math"&gt;\(n\)&lt;/span&gt; represent? If you look at the equation above, &lt;span class="math"&gt;\(l\)&lt;/span&gt; is the row number of the filter, &lt;span class="math"&gt;\(m\)&lt;/span&gt; is the column number of the filter, &lt;span class="math"&gt;\(n\)&lt;/span&gt; is the channel number and &lt;span class="math"&gt;\(k\)&lt;/span&gt; tells you which filter we are using. Notice that the convolution is similar to the matrix multiplication we did with a fully-connect neural network, since we are just multiplying elements of &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{W}^{[1]}\)&lt;/span&gt;. We'll talk about the differences a little later, but for now just relish in the fact that the actual math is just multiplication.&lt;/p&gt;
&lt;p&gt;Let's figure out how we get the value for &lt;span class="math"&gt;\(z^{[1]}_{(1,1,1)}\)&lt;/span&gt;. Plugging in 1 for &lt;span class="math"&gt;\(i\)&lt;/span&gt;, &lt;span class="math"&gt;\(j\)&lt;/span&gt;, and &lt;span class="math"&gt;\(k\)&lt;/span&gt;, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$z^{[1]}_{(1,1,1)} = (\textbf{x} * \textbf{W}^{[1]}_{c})(1,1,1)+ b^{[1]}_{(1,1)}$$&lt;/div&gt;
&lt;div class="math"&gt;$$z^{[1]}_{(1,1,1)} = \sum^{3}_{n = 1}\sum^{3}_{l = 1}\sum^{3}_{m = 1}x_{(1 + l, 1 + m, n)}W^{[1]}_{c(l,m,n,1)} + b^{[1]}_{(1,1)}$$&lt;/div&gt;
&lt;p&gt;If you think about it, we are summing over the three dimensions (row, column, channel) of the 1st filter. We know to use the first filter because &lt;span class="math"&gt;\(z^{[1]}_{(1,1,1)}\)&lt;/span&gt; has 1 as it's final dimension. Let's sum over the row and column first and see what we come up with:&lt;/p&gt;
&lt;div class="math"&gt;$$z^{[1]}_{(1,1,1)} =
\sum^{3}_{n = 1} \bigg(
x_{(1, 1, n)}W^{[1]}_{c(1,1,n,1)} + 
x_{(2, 1, n)}W^{[1]}_{c(2,1,n,1)} + 
x_{(3, 1, n)}W^{[1]}_{c(3,1,n,1)} +
$$&lt;/div&gt;
&lt;div class="math"&gt;$$ 
x_{(1, 2, n)}W^{[1]}_{c(1,2,n,1)} + 
x_{(2, 2, n)}W^{[1]}_{c(2,2,n,1)} + 
x_{(3, 2, n)}W^{[1]}_{c(3,2,n,1)} +
$$&lt;/div&gt;
&lt;div class="math"&gt;$$ 
x_{(1, 3, n)}W^{[1]}_{c(1,3,n,1)} + 
x_{(2, 3, n)}W^{[1]}_{c(2,3,n,1)} + 
x_{(3, 3, n)}W^{[1]}_{c(3,3,n,1)} 
\bigg) + b^{[1]}_{(1,1)}$$&lt;/div&gt;
&lt;p&gt;So recall that each channel of our first filter has dimensions &lt;span class="math"&gt;\((3,3)\)&lt;/span&gt; which means that it has 9 values total. So it would make sense that we would have 9 terms in the above equation. Visually, it looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/VGG_10.png" width="400" title="[VGG_10]" alt="[VGG_10]"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Diagram of operations needed to calculate &lt;span class="math"&gt;\(z^{[1]}_{(1,1,1)}\)&lt;/span&gt;. Notice that for each channel of &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt;, there is a corresponding channel in the first filter of &lt;span class="math"&gt;\(\textbf{W}^{[1]}_{c}\)&lt;/span&gt;. We multiply each value of &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; that lines up with &lt;span class="math"&gt;\(\textbf{W}^{[1]}_{c}\)&lt;/span&gt;, and then add them all together. So we have a total of 18 values that we are adding together.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;If we finally sum over the channels, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$z^{[1]}_{(1,1,1)} =
x_{(1, 1, 1)}W^{[1]}_{c(1,1,1,1)} + 
x_{(2, 1, 1)}W^{[1]}_{c(2,1,1,1)} + 
x_{(3, 1, 1)}W^{[1]}_{c(3,1,1,1)} +
$$&lt;/div&gt;
&lt;div class="math"&gt;$$ 
x_{(1, 2, 1)}W^{[1]}_{c(1,2,1,1)} + 
x_{(2, 2, 1)}W^{[1]}_{c(2,2,1,1)} + 
x_{(3, 2, 1)}W^{[1]}_{c(3,2,1,1)} +
$$&lt;/div&gt;
&lt;div class="math"&gt;$$ 
x_{(1, 3, 1)}W^{[1]}_{c(1,3,1,1)} + 
x_{(2, 3, 1)}W^{[1]}_{c(2,3,1,1)} + 
x_{(3, 3, 1)}W^{[1]}_{c(3,3,1,1)} +
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
x_{(1, 1, 2)}W^{[1]}_{c(1,1,2,1)} + 
x_{(2, 1, 2)}W^{[1]}_{c(2,1,2,1)} + 
x_{(3, 1, 2)}W^{[1]}_{c(3,1,2,1)} +
$$&lt;/div&gt;
&lt;div class="math"&gt;$$ 
x_{(1, 2, 2)}W^{[1]}_{c(1,2,2,1)} + 
x_{(2, 2, 2)}W^{[1]}_{c(2,2,2,1)} + 
x_{(3, 2, 2)}W^{[1]}_{c(3,2,2,1)} +
$$&lt;/div&gt;
&lt;div class="math"&gt;$$ 
x_{(1, 3, 2)}W^{[1]}_{c(1,3,2,1)} + 
x_{(2, 3, 2)}W^{[1]}_{c(2,3,2,1)} + 
x_{(3, 3, 2)}W^{[1]}_{c(3,3,2,1)} +
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
x_{(1, 1, 3)}W^{[1]}_{c(1,1,3,1)} + 
x_{(2, 1, 3)}W^{[1]}_{c(2,1,3,1)} + 
x_{(3, 1, 3)}W^{[1]}_{c(3,1,3,1)} +
$$&lt;/div&gt;
&lt;div class="math"&gt;$$ 
x_{(1, 2, 3)}W^{[1]}_{c(1,2,3,1)} + 
x_{(2, 2, 3)}W^{[1]}_{c(2,2,3,1)} + 
x_{(3, 2, 3)}W^{[1]}_{c(3,2,3,1)} +
$$&lt;/div&gt;
&lt;div class="math"&gt;$$ 
x_{(1, 3, 3)}W^{[1]}_{c(1,3,3,1)} + 
x_{(2, 3, 3)}W^{[1]}_{c(2,3,3,1)} + 
x_{(3, 3, 3)}W^{[1]}_{c(3,3,3,1)} + b^{[1]}_{(1,1)}
$$&lt;/div&gt;
&lt;p&gt;Ok great! So we figured out how to calculate &lt;span class="math"&gt;\(z^{[1]}_{(1,1,1)}\)&lt;/span&gt;. Only &lt;span class="math"&gt;\(4 * 4 * 6 - 1 = 95\)&lt;/span&gt; more calculations to go. Just kidding, we aren't going to go through each calculation. I do want to talk about how we decide which values of &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; we choose to multiply with the values in &lt;span class="math"&gt;\(W^{[1]}_{c}\)&lt;/span&gt;. If we follow the formula that we outlined earlier, we end up 'sliding' over the values of &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt;. I think a gif will help demonstrate what I mean:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/conv.gif" width="400" title="[conv_gif]" alt="[conv_gif]"&gt;&lt;/p&gt;
&lt;p&gt;So in this gif, we calculated &lt;span class="math"&gt;\(z^{[1]}_{(1,1,1)}\)&lt;/span&gt; through &lt;span class="math"&gt;\(z^{[1]}_{(3,3,1)}\)&lt;/span&gt;, which are all the outputs that depend on the first filter of &lt;span class="math"&gt;\(W^{[1]}_c\)&lt;/span&gt;. We would then continue this process 5 more times for the other 5 filters of &lt;span class="math"&gt;\(W^{[1]}_c\)&lt;/span&gt;. The final dimensions of our output, &lt;span class="math"&gt;\(z^{[1]}\)&lt;/span&gt; would therefore be &lt;span class="math"&gt;\((3,3,6)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Notice that each channel in our output shrunk in height and width by &lt;span class="math"&gt;\(2\)&lt;/span&gt;, from &lt;span class="math"&gt;\((5,5)\)&lt;/span&gt; to &lt;span class="math"&gt;\((3,3)\)&lt;/span&gt;. Is this a problem? Yes, because it has been demonstrated that deeper architectures (more layers) perform better. If we are shrinking our dimensions from layer to layer, we are losing a lot of information. We could reduce the size of conv filters (&lt;span class="math"&gt;\(W_{c}\)&lt;/span&gt;), but if we do that we are limiting the ability of the filter to learn representations within the data.&lt;/p&gt;
&lt;p&gt;The creators of VGG-16 recognized this problem, and used something which allowed them to maintain the height and width when they transitioned between the input and output of a conv layer. That something is called same padding.&lt;/p&gt;
&lt;h3&gt;Same Padding&lt;/h3&gt;
&lt;p&gt;In order to prevent the problem of the height and width of layers shrinking, many people use something called same padding. Same padding allows us to maintain the height and width between the input and output of a convolution. Let's go back to our simplified example. Currently, we transition from &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt;, which is &lt;span class="math"&gt;\((5,5,3)\)&lt;/span&gt; to &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt;, which is &lt;span class="math"&gt;\((3,3,3)\)&lt;/span&gt;. If we wanted &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt; to have &lt;span class="math"&gt;\((5,5,3)\)&lt;/span&gt; we could 'pad' &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; with 0's. Let's define this transformation as &lt;span class="math"&gt;\(\textbf{s}^{[0]} = h_p(\textbf{x})\)&lt;/span&gt;, where &lt;span class="math"&gt;\(p\)&lt;/span&gt; is equal to the number of borders of &lt;span class="math"&gt;\(0\)&lt;/span&gt; we place around &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;&lt;img src="/images/VGG_11.png" width="300" title="[VGG_11]" alt="[VGG_11]"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; padded with one border of &lt;span class="math"&gt;\(0\)&lt;/span&gt;'s. We define this new tensor as &lt;span class="math"&gt;\(\textbf{s}^{[0]}\)&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In the image above, &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; is padded with one border of &lt;span class="math"&gt;\(0\)&lt;/span&gt;'s. How do we know how many &lt;span class="math"&gt;\(0\)&lt;/span&gt;'s to add to make sure that our input &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; shares the same height and width as &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt;? There's a pretty easy formula to figure that out! So given a filter with height &lt;span class="math"&gt;\(f\)&lt;/span&gt; and width &lt;span class="math"&gt;\(f\)&lt;/span&gt; the padding (&lt;span class="math"&gt;\(p\)&lt;/span&gt;) is equal to:&lt;/p&gt;
&lt;div class="math"&gt;$$p = \dfrac{f-1}{2}$$&lt;/div&gt;
&lt;p&gt;By convention in computer vision, &lt;span class="math"&gt;\(f\)&lt;/span&gt; is almost always odd so we don't need to worry about a non-whole number padding.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A quick note: All of the convolutional layers in VGGNet use the same stride size of 1. So we won't go into how this calculation changes when we increase the stride size.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For our example filter, &lt;span class="math"&gt;\(f = 3\)&lt;/span&gt; and therefore we need &lt;span class="math"&gt;\(p = 1\)&lt;/span&gt; to use same padding.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/pad.gif" width="400" title="[pad_gif]" alt="[pad_gif]"&gt;&lt;/p&gt;
&lt;p&gt;Our gif now ticks up to &lt;span class="math"&gt;\(\textbf{z}^{[1]}_{(5,5,1)}\)&lt;/span&gt;, which shares the height and width of &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; and is what we wanted. &lt;/p&gt;
&lt;p&gt;Alright, so now we've learned how to calculate &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt;, and it shares the same dimensions as &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; thanks to same padding. We need to do one more operation in order to get to our first hidden layer, &lt;span class="math"&gt;\(\textbf{a}^{[1]}\)&lt;/span&gt; which is the ReLU.&lt;/p&gt;
&lt;h3&gt;ReLU Operation&lt;/h3&gt;
&lt;p&gt;In my previous post describing &lt;a href="/forwardprop"&gt;forward propagation&lt;/a&gt; in a fully connected network, I talked about how a ReLU function works. ReLU is implemented for convolutional networks the same way it's implemented for fully connected networks. If our ReLU function is &lt;span class="math"&gt;\(g\)&lt;/span&gt;, then:&lt;/p&gt;
&lt;div class="math"&gt;$$\textbf{a}^{[1]} = g(\textbf{z}^{[1]})$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(g\)&lt;/span&gt; is applied elementwise to every element in &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt;. As we would expect, &lt;span class="math"&gt;\(\textbf{a}^{[1]}\)&lt;/span&gt; is &lt;span class="math"&gt;\((3,3,6)\)&lt;/span&gt; and shares the same dimensions as &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;With that, we have successfully transitions from the input layer &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; to the first hidden layer, &lt;span class="math"&gt;\(\textbf{a}^{[1]}\)&lt;/span&gt;. Before moving on I think it would be useful to talk about why we use convolutional layers instead of fully connected layers. Most of the following conversation comes from the chapter on Convolutional Networks from the &lt;a href="http://www.deeplearningbook.org/contents/convnets.html"&gt;Deep Learning Book&lt;/a&gt; and I highly recommend you check that out.&lt;/p&gt;
&lt;h2&gt;Why ConvNets?&lt;/h2&gt;
&lt;p&gt;There are two main reasons why we use convolutional layers over fully connected layers: &lt;strong&gt;sparse interactions&lt;/strong&gt; and &lt;strong&gt;parameter sharing&lt;/strong&gt;.&lt;/p&gt;
&lt;h3&gt;Sparse Interactions&lt;/h3&gt;
&lt;p&gt;What if we had used a fully connected layer instead of a convolutional layer in the preceding example? Recall that our input &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; had &lt;span class="math"&gt;\(224 * 224 * 3 = 150,528\)&lt;/span&gt; elements. The first hidden layer &lt;span class="math"&gt;\(\textbf{a}^{[1]}\)&lt;/span&gt; had &lt;span class="math"&gt;\(224 * 224 * 6 = 301,056\)&lt;/span&gt; elements. If we connected these two layers fully, we would need a weight for each combination. Which means we would need &lt;span class="math"&gt;\(150,528 * 301,056= 45,317,357,568\)&lt;/span&gt; weights, plus &lt;span class="math"&gt;\(301,056 * 1 = 301,056\)&lt;/span&gt; biases. Good lord, and that's just for the first layer.&lt;/p&gt;
&lt;p&gt;In contrast, our first convolutional layer has &lt;span class="math"&gt;\(16 * 16 *  3 *  6 = 4608\)&lt;/span&gt; weight parameters and &lt;span class="math"&gt;\(6 * 1 = 6\)&lt;/span&gt; bias parameters. While we share those parameters between inputs (which we will discuss next), those parameters are connected to vastly less inputs than the parameters of a fully connected layer would, and therefore conv layers require orders of magnitude less memory (fewer parameters) and runtime (fewer operations).&lt;/p&gt;
&lt;h3&gt;Parameter Sharing&lt;/h3&gt;
&lt;p&gt;In a fully connected layer, we use each weight parameter one time, since each weight parameter connects one input element to one output element. In contrast, in a convolutional layer, we reuse each weight parameter multiple times. In the example above, the weight parameter  &lt;span class="math"&gt;\(W^{[1]}_{c(1,1,1,1)}\)&lt;/span&gt; is used a total of (&lt;span class="math"&gt;\(5 * 5 = 25\)&lt;/span&gt;) times, multiplying it by the input &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; after padding it with one border of 0's (&lt;span class="math"&gt;\(p = 0\)&lt;/span&gt;) to get &lt;span class="math"&gt;\(\textbf{s}^{[0]}\)&lt;/span&gt;. The elements of &lt;span class="math"&gt;\(\textbf{s}^{[0]}\)&lt;/span&gt; that we use are displayed below: &lt;/p&gt;
&lt;div class="math"&gt;$$
s^{[0]}_{(1,1,1)},
s^{[0]}_{(1,2,1)},
s^{[0]}_{(1,3,1)},
s^{[0]}_{(1,4,1)},
s^{[0]}_{(1,5,1)},
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
s^{[0]}_{(2,1,1)},
s^{[0]}_{(2,2,1)},
s^{[0]}_{(2,3,1)},
s^{[0]}_{(2,4,1)},
s^{[0]}_{(2,5,1)},
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
s^{[0]}_{(3,1,1)},
s^{[0]}_{(3,2,1)},
s^{[0]}_{(3,3,1)},
s^{[0]}_{(3,4,1)},
s^{[0]}_{(3,5,1)},
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
s^{[0]}_{(4,1,1)},
s^{[0]}_{(4,2,1)},
s^{[0]}_{(4,3,1)},
s^{[0]}_{(4,4,1)},
s^{[0]}_{(4,5,1)},
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
s^{[0]}_{(5,1,1)},
s^{[0]}_{(5,2,1)},
s^{[0]}_{(5,3,1)},
s^{[0]}_{(5,4,1)},
s^{[0]}_{(5,5,1)},
$$&lt;/div&gt;
&lt;p&gt;So how does parameter sharing help? Well, let's say that one filter detects the edges in a picture. We only need one set of weights to do this job across the entire image for each channel, since the operation won't change as we move across the image. In this case, parameter sharing is more efficient than using one weight parameter per pixel to connect it to the next layer. &lt;/p&gt;
&lt;p&gt;A special case of parameter sharing is equivariance. CNN's are equivariant in the sense that if we translate an image of a corgi dog across an image, for example, the output will be the same, but just translated to where the corgi is in the image. This is not true for rotations or scale however, and these need to be handled separately.&lt;/p&gt;
&lt;p&gt;In this section, we've discussed the transition from our input &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; to our first conv layer, &lt;span class="math"&gt;\(\textbf{a}^{[1]}\)&lt;/span&gt;. In order to make this transition, we transformed &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; to &lt;span class="math"&gt;\(\textbf{s}^{[0]}\)&lt;/span&gt; using same padding. Next, we applied a convolution using filter weights &lt;span class="math"&gt;\(\textbf{W}^{[1]}_c\)&lt;/span&gt; and added a bias &lt;span class="math"&gt;\(\textbf{b}^{[1]}\)&lt;/span&gt; to get &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt;. Next, we used the ReLU activation function to introduce nonlinearity elementwise to get &lt;span class="math"&gt;\(\textbf{a}^{[1]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$\textbf{x} \rightarrow \textbf{s}^{[0]} \rightarrow \textbf{z}^{[1]} \rightarrow \textbf{a}^{[1]}$$&lt;/div&gt;
&lt;p&gt;Within VGG16, we use this same procedure 13 times, until we flatten our output and use 3 fully connected layers. Between conv layers, the creators of VGG16 interspersed pooling layers, which are used for downsampling. We will discuss them next.&lt;/p&gt;
&lt;h1&gt;Pooling Layer&lt;/h1&gt;
&lt;p&gt;We will focus our attention next on the pooling layer. In particular, we will focus on the transition between a conv layer &lt;span class="math"&gt;\(\textbf{a}^{[2]})\)&lt;/span&gt; and the first pooling layer &lt;span class="math"&gt;\(\textbf{m}^{[2]}\)&lt;/span&gt; in the architecture.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/VGG_12.png" title="[VGG_12]" alt="[VGG_12]"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Diagram of the architecture of VGG-16 with example Pooling Layer &lt;span class="math"&gt;\(\textbf{m}^{[2]}\)&lt;/span&gt; and preceding Conv Layer &lt;span class="math"&gt;\(\textbf{a}^{[2]}\)&lt;/span&gt; highlighted.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We'll first deconstruct what happens in the pooling layer of the VGG16 architecture and then discuss the motivation behind pooling layers. Let's start! VGG16 uses a particular type of pooling operation called max pooling. The dimensions of the input &lt;span class="math"&gt;\(\textbf{a}^{[2]}\)&lt;/span&gt; are &lt;span class="math"&gt;\((224, 224, 64)\)&lt;/span&gt; and the dimensions of the output &lt;span class="math"&gt;\(\textbf{m}^{[2]}\)&lt;/span&gt; are &lt;span class="math"&gt;\((112, 112, 64)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Similar to our last example, let's simplify the dimensions to make it easier to work through the example. Let's make &lt;span class="math"&gt;\(\textbf{a}^{[2]}\)&lt;/span&gt; have dimensions &lt;span class="math"&gt;\((6, 6, 6)\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{m}^{[2]}\)&lt;/span&gt; have dimensions &lt;span class="math"&gt;\((3, 3, 6)\)&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;Pooling has some similarities with convolutions. Like the convolutional operation, we are sliding over our input and performing a pooling operation. VGG16 uses max pooling, which takes the max value within the window. If we were to compute the &lt;span class="math"&gt;\(m^{[2]}_{(i,j,k)}\)&lt;/span&gt;, it would look like this:&lt;/p&gt;
&lt;div class="math"&gt;$$m^{[2]}_{(i,j,k)} = \max_{i * s &amp;lt;= l &amp;lt; i * s + f, j * s &amp;lt;= l &amp;lt; j * s + f }a^{[2]}_{(l,m,k)}$$&lt;/div&gt;
&lt;p&gt;Similar to our previous example, &lt;span class="math"&gt;\(i\)&lt;/span&gt; is the height, &lt;span class="math"&gt;\(j\)&lt;/span&gt; is the width, and &lt;span class="math"&gt;\(k\)&lt;/span&gt; is the channel of the output &lt;span class="math"&gt;\(\textbf{m}^{[2]}\)&lt;/span&gt;. &lt;span class="math"&gt;\(l\)&lt;/span&gt; and &lt;span class="math"&gt;\(m\)&lt;/span&gt; are the height and width of our input. The max pooling filter has height and width equal to &lt;span class="math"&gt;\(f\)&lt;/span&gt; and &lt;span class="math"&gt;\(s\)&lt;/span&gt; is the stride size. Stride size indicates how many elements we pass over for our operation. In our previous example, we used a stride size of 1. In this case, we use a stride size of 2. &lt;/p&gt;
&lt;p&gt;How do we figure out how big of a filter to use to get an output that has dimensions &lt;span class="math"&gt;\((3,3,6)\)&lt;/span&gt; when our input has dimensions &lt;span class="math"&gt;\((6,6,6)\)&lt;/span&gt;? We can use a pretty useful formula. Assuming our input's height and width are equal and our output's height and width are equal, let's let &lt;span class="math"&gt;\(n_L\)&lt;/span&gt; represent the height and width of our input (in this case, &lt;span class="math"&gt;\(\textbf{a}^{[2]}\)&lt;/span&gt;) and &lt;span class="math"&gt;\(n_{L+1}\)&lt;/span&gt; represent the height and width of our output (in this case, &lt;span class="math"&gt;\(\textbf{m}^{[2]}\)&lt;/span&gt;). To find the height and width (&lt;span class="math"&gt;\(f\)&lt;/span&gt;) of the window we need to use for max pooling, we use:&lt;/p&gt;
&lt;div class="math"&gt;$$n_{L+1} = \bigg \lfloor \dfrac{n_L + 2p - f}{s} + 1 \bigg \rfloor$$&lt;/div&gt;
&lt;p&gt;Solving for &lt;span class="math"&gt;\(f\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$f = n_L + 2p - s(n_{L+1} - 1)$$&lt;/div&gt;
&lt;p&gt;Where &lt;span class="math"&gt;\(p\)&lt;/span&gt; is equal to the padding. Since we aren't using padding, we set &lt;span class="math"&gt;\(p = 0\)&lt;/span&gt;. And plugging in &lt;span class="math"&gt;\(n_L = 6\)&lt;/span&gt;, &lt;span class="math"&gt;\(s = 2\)&lt;/span&gt;, and &lt;span class="math"&gt;\(n_{L+1} = 3\)&lt;/span&gt;, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$f = 6 + 2*0 - 2(3 - 1)$$&lt;/div&gt;
&lt;div class="math"&gt;$$f = 2$$&lt;/div&gt;
&lt;p&gt;So the height and width of our window should be equal to &lt;span class="math"&gt;\(2\)&lt;/span&gt;. And just to be clear, this is a pooling window, which means that it's not a filter of trainable parameters like in a convolutional layer. But, we can use this same formula to calculate dimensions of a filter in a convolutional layer.&lt;/p&gt;
&lt;p&gt;Plugging in &lt;span class="math"&gt;\(s = 2\)&lt;/span&gt;, &lt;span class="math"&gt;\(f = 2\)&lt;/span&gt; into our equation, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$m^{[2]}_{(i,j,k)} = \max_{i * 2 &amp;lt;= l &amp;lt; i * 2 + 2, j * 2 &amp;lt;= l &amp;lt; j * 2 + 2 }a^{[2]}_{(l,m,k)}$$&lt;/div&gt;
&lt;p&gt;So essentially, to get an element in &lt;span class="math"&gt;\(\textbf{m}^{[2]}\)&lt;/span&gt;, we take a &lt;span class="math"&gt;\(2x2\)&lt;/span&gt; window of &lt;span class="math"&gt;\(\textbf{a}^{[2]}\)&lt;/span&gt; and return the maximum value in that window. We then slide over by &lt;span class="math"&gt;\(2\)&lt;/span&gt; and do it again.&lt;/p&gt;
&lt;p&gt;Great, so now that we know how the max pooling operation works, why do we use it?&lt;/p&gt;
&lt;h3&gt;Why Max Pooling?&lt;/h3&gt;
&lt;p&gt;There are a few reasons why we use max pooling. The first is that with each set of convolutional layers in VGG16, you may notice that we are increasing the depth, or the amount of channels. In the layer before we flatten our tensor to use it in a fully-connected layer, we have a depth of &lt;span class="math"&gt;\(512\)&lt;/span&gt;. Depth is important because it signfies the structured information that the network has learned about the input. However, it would not be memory efficient to maintain our original height and width of &lt;span class="math"&gt;\(224\)&lt;/span&gt; and end up with a depth of &lt;span class="math"&gt;\(512\)&lt;/span&gt;. Pooling allows us to take a summary statistic (in this case, the max) of a window within a convulational layer and send it to the next level. In essence, we are roughly taking the most important 'activations' from the previous layer and sending it to the next layer, thereby reducing the height and width and decreasing the memory requirements. As an example of this, in the final pooling layer, we end up with a tensor with dimensions &lt;span class="math"&gt;\((7,7,512)\)&lt;/span&gt;. This is sometimes called downsampling.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You can see that this trend happens in many convolutional network architectures. As we go deeper, we increase the depth or number of channels of our layer (as a result of a convolution) and decrease the height and width of our layer (as a result of pooling)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The second is it makes the network invariant to small translations in the input. What this means is that we change the input slightly, the max pooling outputs will stay the same since it will still report the maximum value in the window. This is important in image classification, because the location of, say a nose, won't always be in the same location at all times.&lt;/p&gt;
&lt;p&gt;Alright, so we've discussed an example of a pooling layer. Next, we will briefly talk about the flat layer and softmax layers.&lt;/p&gt;
&lt;h2&gt;Flat Layer&lt;/h2&gt;
&lt;p&gt;In a Flat Layer, we take as input the final max pooling layer (&lt;span class="math"&gt;\(\textbf{m}^{[13]}\)&lt;/span&gt;) and flatten it, to get as output a flat layer &lt;span class="math"&gt;\(\textbf{f}^{[13]}\)&lt;/span&gt; with dimensions &lt;span class="math"&gt;\((25,088, 1)\)&lt;/span&gt;. The &lt;span class="math"&gt;\(25,088\)&lt;/span&gt; comes from multiplying all the dimensions of the input layer (&lt;span class="math"&gt;\(7 * 7 * 512 = 25,088\)&lt;/span&gt;). The reason that we do this is because fully connected layers take as input a row (or column, depending on the math notation) vector as opposed to a tensor. So, in this layer nothing too crazy happens, we are just changing the dimension to prepare to use the fully connected layers.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/VGG_13.png" title="[VGG_13]" alt="[VGG_13]"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Diagram of the architecture of VGG-16 with example Flat Layer &lt;span class="math"&gt;\(\textbf{f}^{[13]}\)&lt;/span&gt; and preceding Pooling Layer &lt;span class="math"&gt;\(\textbf{m}^{[13]}\)&lt;/span&gt; highlighted.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Fully Connected Layer&lt;/h2&gt;
&lt;p&gt;After our 13 convolutional layers, we connect our flat layer to 3 fully connected layers. In a &lt;a href="/forwardprop"&gt;previous post&lt;/a&gt; I talk about how fully connected layers work so I won't go into too much detail about them here. What I do want to discuss is why we use fully connected layers at all. The reason why we wouldn't want to use them is the huge amount of weight parameters in the first fully connected layer:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class="math"&gt;\(512 * 7 * 7 * 4096 = 102,760,448\)&lt;/span&gt; weight parameters connected the flat layer to the first fully connected layer!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We can think of the convolutional and pooling layers as creating useful representations of the data. Remember that both operations are local in the sense that they are taking into consideration windows of the data. Fully connected layers, in contrast, are global and connect every value in the previous max pooling layer (&lt;span class="math"&gt;\(\textbf{m}^{[13]}\)&lt;/span&gt;) together.&lt;/p&gt;
&lt;p&gt;The final step is to connect our last fully connected layer (&lt;span class="math"&gt;\(\textbf{a}^{[16]}\)&lt;/span&gt;) to our output layer (&lt;span class="math"&gt;\(\hat{\textbf{y}}\)&lt;/span&gt;). In order to make this transition, we have to use the softmax function, which is what we will discuss next.&lt;/p&gt;
&lt;h2&gt;Softmax Layer&lt;/h2&gt;
&lt;p&gt;In order to make the final transition from fully connected to softmax layer, we use the softmax function. Let's discuss how the softmax function works next.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/VGG_14.png" title="[VGG_14]" alt="[VGG_14]"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;It's difficult to see, but this is a diagram of the architecture of VGG-16 with example Softmax Layer &lt;span class="math"&gt;\(\textbf{a}^{[16]}\)&lt;/span&gt; and preceding Fully Connected Layer &lt;span class="math"&gt;\(\textbf{a}^{[15]}\)&lt;/span&gt; highlighted. Squint and look to the right.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The transition from the fully connected layer &lt;span class="math"&gt;\(\textbf{a}^{[15]}\)&lt;/span&gt; to the softmax layer &lt;span class="math"&gt;\(\textbf{a}^{[16]}\)&lt;/span&gt; starts off as any fully connected layer usually does. We apply a matrix multiplication using &lt;span class="math"&gt;\(\textbf{W}^{[16]}\)&lt;/span&gt; and add a bias &lt;span class="math"&gt;\(\textbf{b}^{[16]}\)&lt;/span&gt; to attain &lt;span class="math"&gt;\(\textbf{z}^{[16]}\)&lt;/span&gt;. &lt;span class="math"&gt;\(\textbf{W}^{[16]}\)&lt;/span&gt; has dimensions &lt;span class="math"&gt;\((1000, 4096)\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{b}^{[16]}\)&lt;/span&gt; has dimensions &lt;span class="math"&gt;\((100, 1)\)&lt;/span&gt;, which makes sense, since &lt;span class="math"&gt;\(\textbf{a}^{[15]}\)&lt;/span&gt; is a row vector with dimensions &lt;span class="math"&gt;\((4096, 1)\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{z}^{[15]}\)&lt;/span&gt; is a row vector with dimensions &lt;span class="math"&gt;\((1000, 1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$\textbf{z}^{[16]} = \textbf{W}^{[16]}\textbf{a}^{[15]} + \textbf{b}^{[16]}$$&lt;/div&gt;
&lt;p&gt;And this point, we would normally use a ReLU function to introduce nonlinearity. Instead, we are going to use the softmax function. This is similar to when we used the sigmoid function to produce the last fully connected layer in the previous post on &lt;a href="/forwardprop"&gt;forward propagation&lt;/a&gt; in a fully connected neural network. We'll denote the softmax function with (&lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;). How do we compute the &lt;span class="math"&gt;\(ith\)&lt;/span&gt; element in &lt;span class="math"&gt;\(\textbf{a}^{[16]}\)&lt;/span&gt;? We do the following:&lt;/p&gt;
&lt;div class="math"&gt;$$a^{[16]}_{i,1} = \sigma_{(i, 1)}(\textbf{z}^{[16]})$$&lt;/div&gt;
&lt;div class="math"&gt;$$a^{[16]}_{i,1} = \dfrac{e^{z^{[16]}_{i,1}}}{\sum_{j = 1}^{1000}e^{z^{[16]}_{j,1}}}$$&lt;/div&gt;
&lt;p&gt;So in order to compute the &lt;span class="math"&gt;\(ith\)&lt;/span&gt; element of &lt;span class="math"&gt;\(a^{[16]}_{i,1}\)&lt;/span&gt;, we take &lt;span class="math"&gt;\(e\)&lt;/span&gt; to the power of &lt;span class="math"&gt;\(z^{[16]}_{i,1}\)&lt;/span&gt; and divide it by the sum of &lt;span class="math"&gt;\(e\)&lt;/span&gt; to the power of all the elements in &lt;span class="math"&gt;\(\textbf{z}\)&lt;/span&gt;. And after applying this activation function, we get a nice vector &lt;span class="math"&gt;\(\textbf{a}^{[16]}\)&lt;/span&gt; who's elements sum to &lt;span class="math"&gt;\(1\)&lt;/span&gt;. Note that &lt;span class="math"&gt;\(\textbf{a}^{[16]}\)&lt;/span&gt; is equal to &lt;span class="math"&gt;\(\hat{\textbf{y}}\)&lt;/span&gt;. Previously, we discussed how we wanted our output (&lt;span class="math"&gt;\(\hat{\textbf{y}}\)&lt;/span&gt;) to be the probability that the training example image comes from one of &lt;span class="math"&gt;\(1,000\)&lt;/span&gt; classes. &lt;/p&gt;
&lt;div class="math"&gt;$$
\hat{\textbf{y}} =
\begin{bmatrix}
\hat{y}_{1} \\\\
\hat{y}_{2} \\\\ 
\vdots \\\\
\hat{y}_{1000}
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Where each &lt;span class="math"&gt;\(\hat{y}_{i}\)&lt;/span&gt; is equal to the probability that &lt;span class="math"&gt;\(y\)&lt;/span&gt; is equal to class &lt;span class="math"&gt;\(i\)&lt;/span&gt; given the input image &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt;, or:&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{y}_{i} = P(y = i | \textbf{x})$$&lt;/div&gt;
&lt;p&gt;After applying the softmax activation function, we now have a vector of probabilities who sum to 1.&lt;/p&gt;
&lt;p&gt;So in the first part of this blog post, we broke down the different types of layers within VGG-16. We talked about conv layers, max pooling layers, flat layers, fully connected layers and finally the softmax layer that outputs the class probabilities. Since there is a lot of repetition within the model (which makes it appealing) we didn't go through each layer's dimensions and operations that are used to produce it. I want to wrap this section up by taking the architecture picture that I've used throughout this post, flip it, and label it with all the different types of layers we've discussed.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/VGG_15.png" title="[VGG_15]" alt="[VGG_15]"&gt;&lt;/p&gt;
&lt;h1&gt;Backpropagation for VGG16&lt;/h1&gt;
&lt;p&gt;Next up, I want to go into some detail about how backpropagation works for VGG-16. For each layer, our objective is to calculate two things: the partial derivative of the cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to that layer's activations and the partial derivative of the cost function with respect to the trainable parameters associated with that layer.&lt;/p&gt;
&lt;p&gt;Before we start caluclating the partial derivatives for each example layer, let's talk about the cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Understanding the Cost Function&lt;/h2&gt;
&lt;p&gt;In our previous post on &lt;a href="/backprop"&gt;backprop&lt;/a&gt; our objective was to predict &lt;span class="math"&gt;\(y\)&lt;/span&gt;, which could be either &lt;span class="math"&gt;\(0\)&lt;/span&gt; or &lt;span class="math"&gt;\(1\)&lt;/span&gt;. Our prediction was a scalar &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For the Imagenet task however, our prediction &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt; is a vector with dimensions &lt;span class="math"&gt;\((1000, 1)\)&lt;/span&gt;. Since we are using a softmax activation in our final layer, each value corresponds with the probability we think the training example belongs to the class label.&lt;/p&gt;
&lt;div class="math"&gt;$$
\hat{\textbf{y}} =
\begin{bmatrix}
\hat{y}_{1} \\\\
\hat{y}_{2} \\\\ 
\vdots \\\\
\hat{y}_{i} \\\\ 
\vdots \\\\
\hat{y}_{1000}
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Where &lt;span class="math"&gt;\(\hat{y}_{i} = P(y = i | \textbf{x})\)&lt;/span&gt;. &lt;span class="math"&gt;\(\hat{\textbf{y}}\)&lt;/span&gt; also has the attribute that the elements in it sum to &lt;span class="math"&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The loss function that we used for a single training example in our fully connected network was:&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{L}(\hat{y}, y) = -ylog(\hat{y}) - (1-y)log(1 -\hat{y})$$&lt;/div&gt;
&lt;p&gt;And the loss function that we use for a single training example for VGG16 is very similar:&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{L}(\hat{\textbf{y}}, \textbf{y}) = -\sum_{i=1}^{1000} y_i log(\hat{y}_i)$$&lt;/div&gt;
&lt;p&gt;Let's deconstruct what's happening in this loss function. Basically, for every possible image label (there are 1000 possible labels) we are calculating what is sometimes called the 'cross entropy' &lt;span class="math"&gt;\(y_i log(\hat{y}_i)\)&lt;/span&gt; between our prediction for that label &lt;span class="math"&gt;\(\hat{y}_i\)&lt;/span&gt; and the actual value &lt;span class="math"&gt;\(y_i\)&lt;/span&gt;. If you look at the loss function right above it, it's essentially the same as the one directly preceding but for &lt;span class="math"&gt;\(2\)&lt;/span&gt; instead of &lt;span class="math"&gt;\(1000\)&lt;/span&gt; classes. We just choose to define the second class as being &lt;span class="math"&gt;\(1-y\)&lt;/span&gt;, and our prediction as &lt;span class="math"&gt;\(1-\hat{y}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We want to minimize the loss function, which means we want to maximize the cross entropy &lt;span class="math"&gt;\(y_i log(\hat{y}_i)\)&lt;/span&gt;. Recall that each image belongs to only &lt;span class="math"&gt;\(1\)&lt;/span&gt; of &lt;span class="math"&gt;\(1000\)&lt;/span&gt; classes. If our training example image was a dog and belonged to class &lt;span class="math"&gt;\(5\)&lt;/span&gt;, &lt;span class="math"&gt;\(y_5 = 1\)&lt;/span&gt; and all other values (&lt;span class="math"&gt;\(y_1, ... y_4, y_6,...y_{1000}\)&lt;/span&gt;) would be equal to &lt;span class="math"&gt;\(0\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
\textbf{y} =
\begin{bmatrix}
0 \\\\
0 \\\\
0 \\\\
0 \\\\ 
1 \\\\ 
\vdots \\\\
0
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;So the cross entropy for the value &lt;span class="math"&gt;\(y_5\)&lt;/span&gt; becomes &lt;span class="math"&gt;\(1 * log(\hat{y}_5)\)&lt;/span&gt; and the cross entropy for all other values become &lt;span class="math"&gt;\(0 * log(\hat{y}_i)\)&lt;/span&gt;. And so, in order to maximize the cross entropy, we just need to maximize &lt;span class="math"&gt;\(\hat{y}_5\)&lt;/span&gt;, which is the probability that given the data (pixels) of our training example image &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; it belongs to class &lt;span class="math"&gt;\(5\)&lt;/span&gt;. So it makes sense that this would be our loss function, since we want to maximize the probability that our training example comes from the correct class.&lt;/p&gt;
&lt;p&gt;Where did this loss function come from? In our &lt;a href="/backprop"&gt;fully connected network example&lt;/a&gt; we showed how we could derive our loss function for binary classification using the probability mass function for a Bernoulli distribution. We will take a similar approach and look at the probability mass function for a categorical (or multinoulli) distribution:&lt;/p&gt;
&lt;div class="math"&gt;$$p(\textbf{y} | \hat{\textbf{y}}, \textbf{x}) = \prod_{i=1}^{1000} \hat{y}_i^{y_i}$$&lt;/div&gt;
&lt;p&gt;This basically reads as the probability that given our example &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; and prediction &lt;span class="math"&gt;\(\hat{\textbf{y}}\)&lt;/span&gt; we actually have an example with labels &lt;span class="math"&gt;\(\textbf{y}\)&lt;/span&gt; is equal to the product of &lt;span class="math"&gt;\(\hat{y}_i^{y_i}\)&lt;/span&gt; for each class label &lt;span class="math"&gt;\(i\)&lt;/span&gt;. And if we take the log of both sides, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$log \ p(\textbf{y} | \hat{\textbf{y}}, \textbf{x}) = log \ \prod_{i=1}^{1000} \hat{y}_i^{y_i}$$&lt;/div&gt;
&lt;div class="math"&gt;$$log \ p(\textbf{y} | \hat{\textbf{y}}, \textbf{x}) = \sum_{i=1}^{1000} log \ \hat{y}_i^{y_i}$$&lt;/div&gt;
&lt;p&gt;And notice the right side is equal &lt;span class="math"&gt;\(-\mathcal{L}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$log \ p(\textbf{y} | \hat{\textbf{y}}, \textbf{x}) = -\mathcal{L}(\hat{\textbf{y}}, \textbf{y}) $$&lt;/div&gt;
&lt;div class="math"&gt;$$\mathcal{L}(\hat{\textbf{y}}, \textbf{y}) = -log \ p(\textbf{y} | \hat{\textbf{y}}, \textbf{x})$$&lt;/div&gt;
&lt;p&gt;So when we say we want to minimize &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt;, we really mean we want to maximize the probability that &lt;span class="math"&gt;\(\textbf{y}\)&lt;/span&gt; is equal to it's value given our prediction &lt;span class="math"&gt;\(\hat{\textbf{y}}\)&lt;/span&gt; and feature vector &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt;, given that we believe &lt;span class="math"&gt;\(\textbf{y}\)&lt;/span&gt; belongs to a categorical (or multillouni) distribution.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Since log increases monotonically, maximizing &lt;span class="math"&gt;\(p(\textbf{y} | \hat{\textbf{y}}, \textbf{x})\)&lt;/span&gt; and maximizing log \ &lt;span class="math"&gt;\(p(\textbf{y} | \hat{\textbf{y}}, \textbf{x})\)&lt;/span&gt; is the same. &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So this is our loss function &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; which we use to determine how well our model is predicting the class label for a single training example &lt;span class="math"&gt;\(i\)&lt;/span&gt; given the pixels of the image &lt;span class="math"&gt;\(\textbf{x}^{(i)}\)&lt;/span&gt;. But what if we were interested in how well our model was performing for a batch of &lt;span class="math"&gt;\(m\)&lt;/span&gt; training examples? We could take the average of our losses &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; over the &lt;span class="math"&gt;\(m\)&lt;/span&gt; training examples and call this our cost function, &lt;span class="math"&gt;\(J\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$J = \dfrac{1}{m}\sum^m_{i = 1} \mathcal{L(\hat{\textbf{y}}^{(i)},\textbf{y}^{(i)})}$$&lt;/div&gt;
&lt;p&gt;We calculate the cost &lt;span class="math"&gt;\(J\)&lt;/span&gt; for our &lt;span class="math"&gt;\(m\)&lt;/span&gt; training examples, and then calculate two things: (1) the partial derivative of the cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to that layer's activations and (2) the partial derivative of the cost function with respect to the trainable parameters associated with that layer. We reshape (2) into gradients and then update the trainable parameters (which is essentially batch gradient descent) and use (1) to calculate the partial derivative of the cost function with respect to the trainable parameters in the previous layer (which is backpropagation).&lt;/p&gt;
&lt;p&gt;For VGG16, we use a batch size of &lt;span class="math"&gt;\(m = 256\)&lt;/span&gt;, so our cost function becomes:&lt;/p&gt;
&lt;div class="math"&gt;$$J = \dfrac{1}{256}\sum^{256}_{i = 1} \mathcal{L(\hat{\textbf{y}}^{(i)},\textbf{y}^{(i)})}$$&lt;/div&gt;
&lt;p&gt;Which simplifies to become:&lt;/p&gt;
&lt;div class="math"&gt;$$J = -\dfrac{1}{256}\sum^{256}_{i = 1} \sum_{i=1}^{1000} y_i log(\hat{y}_i)$$&lt;/div&gt;
&lt;p&gt;Great, so now we've defined our cost function. We now move to calculating the partial derivatives for each type of layer. Before moving forward I wanted to point something out. Let's say we are calculating the partial derivative of &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to the weights in layer &lt;span class="math"&gt;\(j\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial J}{\partial \textbf{W}^{[j]}} =  \dfrac{\partial}{\partial \textbf{W}^{[j]}} \dfrac{1}{256}\sum^{256}_{i = 1} \mathcal{L(\hat{\textbf{y}}^{(i)},\textbf{y}^{(i)})}$$&lt;/div&gt;
&lt;p&gt;Notice that the differentiation on the right side can be placed inside the summation:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial J}{\partial \textbf{W}^{[j]}} =  \dfrac{1}{256}\sum^{256}_{i = 1} \dfrac{\partial\mathcal{L}}{\partial \textbf{W}^{[j]}}$$&lt;/div&gt;
&lt;p&gt;So for every batch of 256 training examples &lt;span class="math"&gt;\(\textbf{x}^{[i]}\)&lt;/span&gt;, we calculate the partial derivative with respect to the loss &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; for each trainable parameter, and then take the average of those 256 partial derivatives of the losses &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; to get the partial derivative of our cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to the trainable parameters.&lt;/p&gt;
&lt;p&gt;Notice that we can calculate these 256 sets of partial derivatives in parallel, since they don't depend on each other. This is one of the ways we can parallelize backpropagation efficiently using GPUs.&lt;/p&gt;
&lt;p&gt;In any case, instead of calculating the partial derivatives of &lt;span class="math"&gt;\(J\)&lt;/span&gt;, we will just calculate the partial derivative of &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; for a single training example &lt;span class="math"&gt;\(\textbf{x}^{[i]}\)&lt;/span&gt; with the knowledge that we can just take the average of all the partial derivatives across the 256 examples in our batch to get the partial derivative of &lt;span class="math"&gt;\(J\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Before moving forward, if you haven't checked out my blog post on &lt;a href="/backprop"&gt;backprop&lt;/a&gt; it might be useful, since I'm using a lot of the same concepts (e.g. jacobian matrix, distinction between partial derivative and gradients)&lt;/p&gt;
&lt;h2&gt;Backprop for the Softmax Layer&lt;/h2&gt;
&lt;p&gt;&lt;img src="/images/VGG_16.png" title="[VGG_16]" alt="[VGG_16]"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Softmax layer is highlighted on the far right.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The last layer of VGG-16 is a fully connected layer with a softmax activation. Since it is a fully connected layer, it has trainable parameters &lt;span class="math"&gt;\(\textbf{W}^{[16]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{b}^{[16]}\)&lt;/span&gt; and we therefore need to calculate the partial derivative of &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to both &lt;span class="math"&gt;\(\textbf{W}^{[16]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{b}^{[16]}\)&lt;/span&gt; which we can calculate the gradients as well as the partial derivative of &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{z}^{[16]}\)&lt;/span&gt;, which can be used to backpropagate the error to the preceding layer:&lt;/p&gt;
&lt;div class="math"&gt;$$\bigg ( d\textbf{W}^{[16]}, d\textbf{b}^{[16]}, \dfrac{\partial \mathcal{L}}{\partial \textbf{z}^{[16]}} \bigg )$$&lt;/div&gt;
&lt;p&gt;Let's focus our attention on calculating the partial derivative of &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{z}^{[16]}\)&lt;/span&gt;. We can use the chain rule to rewrite this partial derivative as:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial \mathcal{L}}{\partial \textbf{z}^{[16]}} = \dfrac{\partial \mathcal{L}}{\partial \textbf{a}^{[16]}}\dfrac{\partial \textbf{a}^{[16]}}{\partial \textbf{z}^{[16]}}$$&lt;/div&gt;
&lt;p&gt;So step &lt;span class="math"&gt;\(1\)&lt;/span&gt; is to figure out the partial derivative of &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{a}^{[16]}\)&lt;/span&gt;. Recall that the softmax layer &lt;span class="math"&gt;\(\textbf{a}^{[16]}\)&lt;/span&gt; is equal to our prediction for the class labels, &lt;span class="math"&gt;\(\hat{\textbf{y}}\)&lt;/span&gt;
so we can write our loss function as:&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{L}(\textbf{a}^{[16]}, \textbf{y}) = -\sum_{i=1}^{1000} y_i log(a^{[16]}_{(i,1)})$$&lt;/div&gt;
&lt;p&gt;Recall that &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; is a scalar value while &lt;span class="math"&gt;\(\textbf{a}^{[16]}\)&lt;/span&gt; is a column vector with dimensions &lt;span class="math"&gt;\((1000, 1)\)&lt;/span&gt;. The partial derivative of &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{a}^{[16]}\)&lt;/span&gt; as represented as a Jacobian matrix is therefore &lt;span class="math"&gt;\((1, 1000)\)&lt;/span&gt;. &lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{a}^{[16]}} = 
\begin{bmatrix}
\dfrac{\partial \mathcal{L}}{\partial a^{[16]}_{(1,1)}} &amp;amp;
\dfrac{\partial \mathcal{L}}{\partial a^{[16]}_{(2,1)}} &amp;amp;
\dots &amp;amp;
\dfrac{\partial \mathcal{L}}{\partial a^{[16]}_{(1000,1)}} 
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Notice that the first dimension of the Jacobian matrix is equal to the number of values in our output &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; which is &lt;span class="math"&gt;\(1\)&lt;/span&gt;, and the second dimension is equal to the number of values in our input &lt;span class="math"&gt;\(\textbf{a}^{[16]}\)&lt;/span&gt;, which is &lt;span class="math"&gt;\(1000\)&lt;/span&gt;. We'll continue to use this formulation in the rest of the blog post.&lt;/p&gt;
&lt;p&gt;Let's take the first value of the Jacobian, the partial derivative of &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(a^{[16]}_{(1,1)}\)&lt;/span&gt;. &lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{L}(\textbf{a}^{[16]}, \textbf{y}) = -\sum_{i=1}^{1000} y_i log(a^{[16]}_{(1,1)})$$&lt;/div&gt;
&lt;div class="math"&gt;$$\mathcal{L}(\textbf{a}^{[16]}, \textbf{y}) = -y_1 log(a^{[16]}_{(1,1)})-y_2 log(a^{[16]}_{(2,1)}) \dots -y_1000 log(a^{[16]}_{(1000,1)})$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial \mathcal{L}}{\partial a^{[16]}_{(1,1)}} = -\dfrac{y_1}{a^{[16]}_{(1,1)}} + 0 + \dots + 0$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial \mathcal{L}}{\partial a^{[16]}_{(1,1)}} = -\dfrac{y_1}{a^{[16]}_{(1,1)}}$$&lt;/div&gt;
&lt;p&gt;So applying this to every partial derivative in the Jacobian, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{a}^{[16]}} = 
\begin{bmatrix}
-\dfrac{y_1}{a^{[16]}_{(1,1)}} &amp;amp;
-\dfrac{y_2}{a^{[16]}_{(2,1)}} &amp;amp;
\dots &amp;amp;
-\dfrac{y_{1000}}{a^{[16]}_{(1000,1)}}
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Notice for any given training example &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt;, its label &lt;span class="math"&gt;\(y\)&lt;/span&gt; will have a &lt;span class="math"&gt;\(1\)&lt;/span&gt; for its class and &lt;span class="math"&gt;\(0\)&lt;/span&gt; for all the others. So let's say for a random training example &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; it has the label &lt;span class="math"&gt;\(3\)&lt;/span&gt;, meaning &lt;span class="math"&gt;\(y_3 = 1\)&lt;/span&gt; and all the others are &lt;span class="math"&gt;\(0\)&lt;/span&gt;. So the partial derivative will look like this:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{a}^{[16]}} = 
\begin{bmatrix}
0 &amp;amp;
0 &amp;amp;
-\dfrac{y_3}{a^{[16]}_{(3,1)}} &amp;amp;
\dots &amp;amp;
0
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;What this means is that we will only update the weights that relate to the third activation of the softmax layer, which makes sense, since we would only want to update the activation that corresponds with the true class label. We just calculated the partial derivative of &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{a}^{[16]}\)&lt;/span&gt;. Let's now work on the partial derivative of &lt;span class="math"&gt;\(\textbf{a}^{[16]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{z}^{[16]}\)&lt;/span&gt;. We'll start by analyzing it's Jacobian matrix:&lt;/p&gt;
&lt;div class="math"&gt;$$ \dfrac{\partial \textbf{a}^{[16]}}{\partial \textbf{z}^{[16]}} = 
\begin{bmatrix}
\\
\dfrac{\partial a^{[16]}_{(1,1)}}{\partial z^{[16]}_{(1, 1)}} &amp;amp;
\dotsc &amp;amp;
\dfrac{\partial a^{[16]}_{(1,1)}}{\partial z^{[16]}_{(1000, 1)}} \\\\
\vdots &amp;amp; \ddots &amp;amp; \vdots \\\\
\dfrac{\partial a^{[16]}_{(1000,1)}}{\partial z^{[16]}_{(1, 1)}} &amp;amp;
\dotsc &amp;amp;
\dfrac{\partial a^{[16]}_{(1000,1)}}{\partial z^{[16]}_{(1000, 1)}} \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;The partial derivative of &lt;span class="math"&gt;\(\textbf{a}^{[16]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{z}^{[16]}\)&lt;/span&gt; has dimensions &lt;span class="math"&gt;\((1000, 1000)\)&lt;/span&gt;. Similar to before let's see if we can calculate the first value in the Jacobian Matrix, the partial derivative of &lt;span class="math"&gt;\(a^{[16]}_{(1,1)}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(z^{[16]}_{(1,1)}\)&lt;/span&gt;. We start with the formula of the softmax activation, that we defined in the previous post on &lt;a href="/vgg_forwardprop"&gt;forward propagation&lt;/a&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$a^{[16]}_{(1,1)} = \sigma_{(1,1)}(\textbf{z}^{[16]})$$&lt;/div&gt;
&lt;div class="math"&gt;$$a^{[16]}_{(1,1)} = \dfrac{e^{z^{[16]}_{(1,1)}}}{\sum_{j = 1}^{1000}e^{z^{[16]}_{(j,1)}}}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial a^{[16]}_{(1,1)}}{\partial z^{[16]}_{(1,1)}} = \dfrac{\partial}{\partial z^{[16]}_{(1,1)}}\dfrac{e^{z^{[16]}_{(1,1)}}}{\sum_{j = 1}^{1000}e^{z^{[16]}_{(j,1)}}}$$&lt;/div&gt;
&lt;p&gt;To calculate this partial derivative, we use the quotient rule for derivatives and get:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial a^{[16]}_{(1,1)}}{\partial z^{[16]}_{(1,1)}} = \dfrac{e^{z^{[16]}_{(1,1)}} * \sum_{j = 1}^{1000}e^{z^{[16]}_{(j,1)}} - e^{2z^{[16]}_{(1,1)}}}{\big( \sum_{j = 1}^{1000}e^{z^{[16]}_{(j,1)}} \big) ^ 2}$$&lt;/div&gt;
&lt;p&gt;We can separate the term on the RHS into two terms:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial a^{[16]}_{(1,1)}}{\partial z^{[16]}_{(1,1)}} = \dfrac{e^{z^{[16]}_{(1,1)}}}{\sum_{j = 1}^{1000}e^{z^{[16]}_{(j,1)}}} - \dfrac{e^{2z^{[16]}_{(1,1)}}}{\big( \sum_{j = 1}^{1000}e^{z^{[16]}_{(j,1)}} \big) ^ 2}$$&lt;/div&gt;
&lt;p&gt;Substituting the softmax function &lt;span class="math"&gt;\(\sigma_{(1,1)}\)&lt;/span&gt; back in we get:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial a^{[16]}_{(1,1)}}{\partial z^{[16]}_{(1,1)}} = \sigma_{(1,1)}(\textbf{z}^{[16]}) - \sigma_{(1,1)}(\textbf{z}^{[16]})^2$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial a^{[16]}_{(1,1)}}{\partial z^{[16]}_{(1,1)}} = \sigma_{(1,1)}(\textbf{z}^{[16]})(1 - \sigma_{(1,1)}(\textbf{z}^{[16]}))$$&lt;/div&gt;
&lt;p&gt;And using the fact that &lt;span class="math"&gt;\(a^{[16]}_{(i,1)} = \sigma_{(i,1)}(\textbf{z}^{[16]})\)&lt;/span&gt;, we can simplify the partial derivative to become:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial a^{[16]}_{(1,1)}}{\partial z^{[16]}_{(1,1)}} = a^{[16]}_{(1,1)}(1 - a^{[16]}_{(1,1)})$$&lt;/div&gt;
&lt;p&gt;The partial derivative of &lt;span class="math"&gt;\(a^{[16]}_{(i,1)}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(z^{[16]}_{(j,1)}\)&lt;/span&gt; looks like the calculation above when &lt;span class="math"&gt;\(i=j\)&lt;/span&gt;. What happens to the partial derivative when &lt;span class="math"&gt;\(i \neq j\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;Let's take the example of the partial derivative of &lt;span class="math"&gt;\(a^{[16]}_{(1,1)}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(z^{[16]}_{(2,1)}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$a^{[16]}_{(1,1)} = \sigma_{(2,1)}(\textbf{z}^{[16]})$$&lt;/div&gt;
&lt;div class="math"&gt;$$a^{[16]}_{(1,1)} = \dfrac{e^{z^{[16]}_{(2,1)}}}{\sum_{j = 1}^{1000}e^{z^{[16]}_{(j,1)}}}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial a^{[16]}_{(1,1)}}{\partial z^{[16]}_{(2,1)}} = \dfrac{\partial}{\partial z^{[16]}_{(2,1)}}\dfrac{e^{z^{[16]}_{(2,1)}}}{\sum_{j = 1}^{1000}e^{z^{[16]}_{(j,1)}}}$$&lt;/div&gt;
&lt;p&gt;To calculate this partial derivative, we use the quotient rule for derivatives and get:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial a^{[16]}_{(1,1)}}{\partial z^{[16]}_{(2,1)}} = 
\dfrac{0 * \sum_{j = 1}^{1000}e^{z^{[16]}_{(j,1)}} - e^{z^{[16]}_{(1,1)}}e^{z^{[16]}_{(2,1)}}}{\big( \sum_{j = 1}^{1000}e^{z^{[16]}_{(j,1)}} \big) ^ 2}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial a^{[16]}_{(1,1)}}{\partial z^{[16]}_{(2,1)}} = 
\dfrac{- e^{z^{[16]}_{(1,1)}}e^{z^{[16]}_{(2,1)}}}{\big( \sum_{j = 1}^{1000}e^{z^{[16]}_{(j,1)}} \big) ^ 2}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial a^{[16]}_{(1,1)}}{\partial z^{[16]}_{(2,1)}} = 
- a^{[16]}_{(1,1)}a^{[16]}_{(2,1)}$$&lt;/div&gt;
&lt;p&gt;Generalizing this, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial a^{[16]}_{(i,1)}}{\partial z^{[16]}_{(j,1)}} = 
\begin{cases}
   a^{[16]}_{(i,1)}(1 - a^{[16]}_{(i,1)})  &amp;amp; i = j \\\\
   - a^{[16]}_{(i,1)}a^{[16]}_{(j,1)} &amp;amp; i \neq j
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;And the Jacobian looks like this:&lt;/p&gt;
&lt;div class="math"&gt;$$ \dfrac{\partial \textbf{a}^{[16]}}{\partial \textbf{z}^{[16]}} = 
\begin{bmatrix}
\\
a^{[16]}_{(1,1)}(1 - a^{[16]}_{(1,1)}) &amp;amp;
- a^{[16]}_{(1,1)}a^{[16]}_{(2,1)} &amp;amp; 
\dotsc &amp;amp;
- a^{[16]}_{(1,1)}a^{[16]}_{(999,1)} &amp;amp;
- a^{[16]}_{(1,1)}a^{[16]}_{(1000,1)} \\\\
\vdots &amp;amp; 
\vdots &amp;amp;
\ddots &amp;amp;
\vdots &amp;amp;
\vdots \\\\
- a^{[16]}_{(1000,1)}a^{[16]}_{(1,1)} &amp;amp; 
- a^{[16]}_{(1000,1)}a^{[16]}_{(2,1)} &amp;amp; 
\dotsc &amp;amp;
- a^{[16]}_{(1000,1)}a^{[16]}_{(999,1)} &amp;amp;
a^{[16]}_{(1000,1)}(1 - a^{[16]}_{(1000,1)}) \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;The Jacobian has dimensions &lt;span class="math"&gt;\((1000, 1000)\)&lt;/span&gt;. Notice that the diagonal elements are equal to &lt;span class="math"&gt;\(a^{[16]}_{(i,1)}(1 - a^{[16]}_{(i,1)})\)&lt;/span&gt;, whereas every other element is equal to &lt;span class="math"&gt;\(- a^{[16]}_{(i,1)}a^{[16]}_{(j,1)}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Substituting in our two Jacobian matricies, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial \mathcal{L}}{\partial \textbf{z}^{[16]}} = \dfrac{\partial \mathcal{L}}{\partial \textbf{a}^{[16]}}\dfrac{\partial \textbf{a}^{[16]}}{\partial \textbf{z}^{[16]}}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial \mathcal{L}}{\partial \textbf{z}^{[16]}} = 
\begin{bmatrix}
-\dfrac{y_1}{a^{[16]}_{(1,1)}} &amp;amp;
-\dfrac{y_2}{a^{[16]}_{(2,1)}} &amp;amp;
\dots &amp;amp;
-\dfrac{y_{1000}}{a^{[16]}_{(1000,1)}}
\end{bmatrix}
\begin{bmatrix}
\\
a^{[16]}_{(1,1)}(1 - a^{[16]}_{(1,1)}) &amp;amp;
- a^{[16]}_{(1,1)}a^{[16]}_{(2,1)} &amp;amp; 
\dotsc &amp;amp;
- a^{[16]}_{(1,1)}a^{[16]}_{(999,1)} &amp;amp;
- a^{[16]}_{(1,1)}a^{[16]}_{(1000,1)} \\\\
\vdots &amp;amp; 
\vdots &amp;amp;
\ddots &amp;amp;
\vdots &amp;amp;
\vdots \\\\
- a^{[16]}_{(1000,1)}a^{[16]}_{(1,1)} &amp;amp; 
- a^{[16]}_{(1000,1)}a^{[16]}_{(2,1)} &amp;amp; 
\dotsc &amp;amp;
- a^{[16]}_{(1000,1)}a^{[16]}_{(999,1)} &amp;amp; 
a^{[16]}_{(1000,1)}(1 - a^{[16]}_{(1000,1)}) \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;This matrix multiplication yields a Jacobian matrix with dimensions &lt;span class="math"&gt;\((1, 1000)\)&lt;/span&gt;. Let's look at the calculations for the first element of this matrix, the partial derivative of &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(z^{[16]}_{(1,1)}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial \mathcal{L}}{\partial z^{[16]}_{(1,1)}} = 
-\dfrac{y_1}{a^{[16]}_{(1,1)}}a^{[16]}_{(1,1)}(1 - a^{[16]}_{(1,1)}) + 
\dfrac{y_2}{a^{[16]}_{(2,1)}}a^{[16]}_{(2,1)}a^{[16]}_{(1,1)} +
\dfrac{y_3}{a^{[16]}_{(3,1)}}a^{[16]}_{(3,1)}a^{[16]}_{(1,1)} +  
\dots + 
\dfrac{y_{1000}}{a^{[16]}_{(1000,1)}}a^{[16]}_{(1000,1)}a^{[16]}_{(1,1)}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial \mathcal{L}}{\partial z^{[16]}_{(1,1)}} = 
-y_1(1 - a^{[16]}_{(1,1)}) + 
y_2a^{[16]}_{(1,1)} +
y_3a^{[16]}_{(1,1)} +  
\dots + 
y_{1000}a^{[16]}_{(1,1)}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial \mathcal{L}}{\partial z^{[16]}_{(1,1)}} = 
-y_1 + 
y_1a^{[16]}_{(1,1)}) + 
y_2a^{[16]}_{(1,1)} +
y_3a^{[16]}_{(1,1)} +  
\dots + 
y_{1000}a^{[16]}_{(1,1)}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial \mathcal{L}}{\partial z^{[16]}_{(1,1)}} = 
-y_1 + 
a^{[16]}_{(1,1)}\sum_i^{1000}{y_i}
$$&lt;/div&gt;
&lt;p&gt;And recall because &lt;span class="math"&gt;\(\textbf{y}\)&lt;/span&gt; has only one class, one element is equal to &lt;span class="math"&gt;\(1\)&lt;/span&gt; whereas the others are equal to &lt;span class="math"&gt;\(0\)&lt;/span&gt;. Therefore, the sum is equal to &lt;span class="math"&gt;\(1\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial \mathcal{L}}{\partial z^{[16]}_{(1,1)}} = 
-y_1 +
a^{[16]}_{(1,1)}*1
$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial \mathcal{L}}{\partial z^{[16]}_{(1,1)}} = 
a^{[16]}_{(1,1)} - y_1
$$&lt;/div&gt;
&lt;p&gt;Notice that the partial derivative is the same with just one class that we calculated in the previous &lt;a href="/backprop"&gt;backprop&lt;/a&gt; blog post! All that fun work to get the same answer. &lt;a href="https://www.ics.uci.edu/~pjsadows/notes.pdf"&gt;This is a great reference&lt;/a&gt; if you want more softmax backprop fun.&lt;/p&gt;
&lt;p&gt;Generalizing to all elements in the Jacobian matrix, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{z}^{[16]}} = 
\begin{bmatrix}
a^{[16]}_{(1,1)} - y_1 &amp;amp;
a^{[16]}_{(2,1)} - y_2 &amp;amp;
a^{[16]}_{(3,1)} - y_3 &amp;amp;
\dots &amp;amp;
a^{[16]}_{(1000,1)} - y_{1000} &amp;amp;
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;And we end up with the partial derivative of &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{z}^{[16]}\)&lt;/span&gt; with dimensions &lt;span class="math"&gt;\((1000, 1)\)&lt;/span&gt;. We sometimes label the transpose of this partial derivative &lt;span class="math"&gt;\(\boldsymbol{\delta}^{[16]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{\delta}^{[2]} = 
\bigg( \dfrac{\mathcal{L}}{\partial{\mathbf{z}^{[16]}}}\bigg)^T
$$&lt;/div&gt;
&lt;p&gt;The dimensions of &lt;span class="math"&gt;\(\boldsymbol{\delta}^{[16]}\)&lt;/span&gt; are &lt;span class="math"&gt;\((1000, 1)\)&lt;/span&gt;, which match the dimensions of &lt;span class="math"&gt;\(\textbf{z}^{[16]}\)&lt;/span&gt;. So we can think of &lt;span class="math"&gt;\(\boldsymbol{\delta}^{[16]}\)&lt;/span&gt; as the gradient for &lt;span class="math"&gt;\(\textbf{z}^{[16]}\)&lt;/span&gt;, although we don't use this explicitly in gradient descent since &lt;span class="math"&gt;\(\textbf{z}^{[16]}\)&lt;/span&gt; has no updatable parameters.&lt;/p&gt;
&lt;p&gt;Next up, we need to calculate the partial derivative of &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to both &lt;span class="math"&gt;\(\textbf{W}^{[16]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{b}^{[16]}\)&lt;/span&gt; in order to get the gradients &lt;span class="math"&gt;\(d\textbf{W}^{[16]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(d\textbf{b}^{[16]}\)&lt;/span&gt; with gradient descent. Luckily, we've already calculated this in the previous post on &lt;a href="\backprop"&gt;backprop&lt;/a&gt; so we can just use the results from that:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial \textbf{z}^{[16]}}{\partial \textbf{W}^{[16]}} =
\begin{bmatrix}
a^{[15]}_{(1,1)} &amp;amp;
a^{[15]}_{(2,1)} &amp;amp;
a^{[15]}_{(3,1)} &amp;amp;
\dots &amp;amp;
a^{[15]}_{(999,1)} &amp;amp;
a^{[15]}_{(1000,1)} &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
\dots &amp;amp;
0 &amp;amp;
0 &amp;amp;
\dots &amp;amp;
\dots &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
\dots &amp;amp;
0 &amp;amp;
0 \\\\
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
\dots &amp;amp;
0 &amp;amp;
0 &amp;amp;
a^{[15]}_{(1,1)} &amp;amp;
a^{[15]}_{(2,1)} &amp;amp;
a^{[15]}_{(3,1)} &amp;amp;
\dots &amp;amp;
a^{[15]}_{(999,1)} &amp;amp;
a^{[15]}_{(1000,1)} &amp;amp;
\dots &amp;amp;
\dots &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
\dots &amp;amp;
0 &amp;amp;
0 &amp;amp; \\\\
\vdots &amp;amp;
\vdots &amp;amp;
\vdots &amp;amp;
\dots &amp;amp;
\vdots &amp;amp;
\vdots &amp;amp;
\vdots &amp;amp;
\vdots &amp;amp;
\vdots &amp;amp;
\dots &amp;amp;
\vdots &amp;amp;
\vdots &amp;amp;
\dots &amp;amp;
\dots &amp;amp;
\vdots &amp;amp;
\vdots &amp;amp;
\vdots &amp;amp;
\dots &amp;amp;
\vdots &amp;amp;
\vdots &amp;amp; \\\\
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
\dots &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
\dots &amp;amp;
0 &amp;amp;
0 &amp;amp;
\dots &amp;amp;
\dots &amp;amp;
a^{[15]}_{(1,1)} &amp;amp;
a^{[15]}_{(2,1)} &amp;amp;
a^{[15]}_{(3,1)} &amp;amp;
\dots &amp;amp;
a^{[15]}_{(999,1)} &amp;amp;
a^{[15]}_{(1000,1)} \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Notice that the Jacobian Matrix for the partial derivative of &lt;span class="math"&gt;\(\textbf{z}^{[16]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{W}^{[16]}\)&lt;/span&gt; is a matrix with dimensions &lt;span class="math"&gt;\((1000, 1000000)\)&lt;/span&gt;. Since &lt;span class="math"&gt;\(\textbf{W}^{[16]}\)&lt;/span&gt; has dimensions &lt;span class="math"&gt;\((1000, 1000)\)&lt;/span&gt;, it has a total of &lt;span class="math"&gt;\(1000 * 1000 = 1,000,000\)&lt;/span&gt; weights which is represented in the second dimension of the Jacobian Matrix. &lt;/p&gt;
&lt;p&gt;Using the chain rule, the partial derivative of &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{W}^{[16]}\)&lt;/span&gt; is equal to:
&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{W}^{[16]}} = \dfrac{\partial \mathcal{L}}{\partial \textbf{z}^{[16]}}\dfrac{\partial \mathcal{\textbf{z}^{[16]}}}{\partial \textbf{W}^{[16]}}
\dfrac{\partial \mathcal{L}}{\partial \textbf{W}^{[16]}} = \boldsymbol{\delta}^{[16]T}\dfrac{\partial \mathcal{\textbf{z}^{[16]}}}{\partial \textbf{W}^{[16]}}
$$&lt;/div&gt;
&lt;p&gt;Plugging in our two results, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{W}^{[16]}} = 
\begin{bmatrix}
\delta^{[16]}_{(1,1)} &amp;amp;
\delta^{[16]}_{(2,1)} &amp;amp;
\delta^{[16]}_{(3,1)} &amp;amp;
\dots &amp;amp;
\delta^{[16]}_{(1000,1)}
\end{bmatrix}
\begin{bmatrix}
a^{[15]}_{(1,1)} &amp;amp;
a^{[15]}_{(2,1)} &amp;amp;
a^{[15]}_{(3,1)} &amp;amp;
\dots &amp;amp;
a^{[15]}_{(999,1)} &amp;amp;
a^{[15]}_{(1000,1)} &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
\dots &amp;amp;
0 &amp;amp;
0 &amp;amp;
\dots &amp;amp;
\dots &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
\dots &amp;amp;
0 &amp;amp;
0 \\\\
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
\dots &amp;amp;
0 &amp;amp;
0 &amp;amp;
a^{[15]}_{(1,1)} &amp;amp;
a^{[15]}_{(2,1)} &amp;amp;
a^{[15]}_{(3,1)} &amp;amp;
\dots &amp;amp;
a^{[15]}_{(999,1)} &amp;amp;
a^{[15]}_{(1000,1)} &amp;amp;
\dots &amp;amp;
\dots &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
\dots &amp;amp;
0 &amp;amp;
0 &amp;amp; \\\\
\vdots &amp;amp;
\vdots &amp;amp;
\vdots &amp;amp;
\dots &amp;amp;
\vdots &amp;amp;
\vdots &amp;amp;
\vdots &amp;amp;
\vdots &amp;amp;
\vdots &amp;amp;
\dots &amp;amp;
\vdots &amp;amp;
\vdots &amp;amp;
\dots &amp;amp;
\dots &amp;amp;
\vdots &amp;amp;
\vdots &amp;amp;
\vdots &amp;amp;
\dots &amp;amp;
\vdots &amp;amp;
\vdots &amp;amp; \\\\
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
\dots &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
\dots &amp;amp;
0 &amp;amp;
0 &amp;amp;
\dots &amp;amp;
\dots &amp;amp;
a^{[15]}_{(1,1)} &amp;amp;
a^{[15]}_{(2,1)} &amp;amp;
a^{[15]}_{(3,1)} &amp;amp;
\dots &amp;amp;
a^{[15]}_{(999,1)} &amp;amp;
a^{[15]}_{(1000,1)} \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{W}^{[16]}} = 
\begin{bmatrix}
\delta^{[16]}_{(1,1)}a^{[15]}_{(1,1)} &amp;amp;
\delta^{[16]}_{(2,1)}a^{[15]}_{(1,1)} &amp;amp;
\delta^{[16]}_{(3,1)}a^{[15]}_{(1,1)} &amp;amp;
\dots &amp;amp;
\delta^{[16]}_{(1000,1)}a^{[15]}_{(1,1)} &amp;amp;
\delta^{[16]}_{(1,1)}a^{[15]}_{(2,1)} &amp;amp;
\delta^{[16]}_{(2,1)}a^{[15]}_{(2,1)} &amp;amp;
\delta^{[16]}_{(3,1)}a^{[15]}_{(2,1)} &amp;amp;
\dots &amp;amp;
\delta^{[16]}_{(1000,1)}a^{[15]}_{(2,1)} &amp;amp;
\dots &amp;amp;
\dots &amp;amp;
\delta^{[16]}_{(1,1)}a^{[15]}_{(1000,1)} &amp;amp;
\delta^{[16]}_{(2,1)}a^{[15]}_{(1000,1)} &amp;amp;
\delta^{[16]}_{(3,1)}a^{[15]}_{(1000,1)} &amp;amp;
\dots &amp;amp;
\delta^{[16]}_{(1000,1)}a^{[15]}_{(1000,1)} &amp;amp;
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;So this is our Jacobian, with dimensions &lt;span class="math"&gt;\((1,1000000)\)&lt;/span&gt;. But we need our gradient matrix &lt;span class="math"&gt;\(d\textbf{W}^{[16]}\)&lt;/span&gt; to have dimensions that match &lt;span class="math"&gt;\(\textbf{W}^{[16]}\)&lt;/span&gt;. So we will reshape the Jacobian into a &lt;span class="math"&gt;\((1000, 1000)\)&lt;/span&gt; matrix.&lt;/p&gt;
&lt;div class="math"&gt;$$
d\textbf{W}^{[16]} = 
\begin{bmatrix}
\delta^{[16]}_{(1,1)}a^{[15]}_{(1,1)} &amp;amp;
\delta^{[16]}_{(2,1)}a^{[15]}_{(1,1)} &amp;amp;
\delta^{[16]}_{(3,1)}a^{[15]}_{(1,1)} &amp;amp;
\dots &amp;amp;
\delta^{[16]}_{(1000,1)}a^{[15]}_{(1,1)} \\\\
\delta^{[16]}_{(1,1)}a^{[15]}_{(2,1)} &amp;amp;
\delta^{[16]}_{(2,1)}a^{[15]}_{(2,1)} &amp;amp;
\delta^{[16]}_{(3,1)}a^{[15]}_{(2,1)} &amp;amp;
\dots &amp;amp;
\delta^{[16]}_{(1000,1)}a^{[15]}_{(2,1)} \\\\
\vdots &amp;amp;
\vdots &amp;amp;
\vdots &amp;amp;
\dots &amp;amp;
\vdots \\\\
\delta^{[16]}_{(1,1)}a^{[15]}_{(1000,1)} &amp;amp;
\delta^{[16]}_{(2,1)}a^{[15]}_{(1000,1)} &amp;amp;
\delta^{[16]}_{(3,1)}a^{[15]}_{(1000,1)} &amp;amp;
\dots &amp;amp;
\delta^{[16]}_{(1000,1)}a^{[15]}_{(1000,1)} \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
d\textbf{W}^{[16]} = 
\begin{bmatrix}
\delta^{[16]}_{(2,1)} \\\\
\delta^{[16]}_{(2,1)} \\\\
\vdots \\\\
\delta^{[16]}_{(1000,1)}
\end{bmatrix}
\begin{bmatrix}
a^{[15]}_{(1,1)} &amp;amp;
a^{[15]}_{(2,1)} &amp;amp;
\vdots &amp;amp;
a^{[15]}_{(1000,1)}
\end{bmatrix}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
d\textbf{W}^{[16]} = 
\boldsymbol{\delta}^{[16]}\textbf{a}^{[15]T}
$$&lt;/div&gt;
&lt;p&gt;Ok great, now that we have &lt;span class="math"&gt;\(d\textbf{W}^{[16]}\)&lt;/span&gt; taken care of, let's move on to looking at the partial derivative of &lt;span class="math"&gt;\(\textbf{z}^{[16]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{b}^{[16]}\)&lt;/span&gt;. Again, we will use calculations we did in a simpler example in the &lt;a href="\backprop"&gt;backprop&lt;/a&gt; post.&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial \textbf{z}^{[16]}}{\partial \textbf{b}^{[16]}} =
\begin{bmatrix}
1 &amp;amp;
0 &amp;amp;
\dots &amp;amp;
0 &amp;amp;
0 \\\\
0 &amp;amp;
1 &amp;amp;
\dots &amp;amp;
0 &amp;amp;
0 \\\\
0 &amp;amp;
0 &amp;amp;
\dots &amp;amp;
1 &amp;amp;
0 \\\\
0 &amp;amp;
0 &amp;amp;
\dots &amp;amp;
0 &amp;amp;
1 \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Using the chain rule, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{b}^{[16]}} = \dfrac{\partial \mathcal{L}}{\partial \textbf{z}^{[16]}}\dfrac{\partial \mathcal{\textbf{z}^{[16]}}}{\partial \textbf{b}^{[16]}}
\dfrac{\partial \mathcal{L}}{\partial \textbf{b}^{[16]}} = \boldsymbol{\delta}^{[16]T}\dfrac{\partial \mathcal{\textbf{z}^{[16]}}}{\partial \textbf{b}^{[16]}}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{b}^{[16]}} = 
\begin{bmatrix}
\delta^{[16]}_{(1,1)} &amp;amp;
\delta^{[16]}_{(2,1)} &amp;amp;
\delta^{[16]}_{(3,1)} &amp;amp;
\dots &amp;amp;
\delta^{[16]}_{(1000,1)}
\end{bmatrix}
\begin{bmatrix}
1 &amp;amp;
0 &amp;amp;
\dots &amp;amp;
0 &amp;amp;
0 \\\\
0 &amp;amp;
1 &amp;amp;
\dots &amp;amp;
0 &amp;amp;
0 \\\\
0 &amp;amp;
0 &amp;amp;
\dots &amp;amp;
1 &amp;amp;
0 \\\\
0 &amp;amp;
0 &amp;amp;
\dots &amp;amp;
0 &amp;amp;
1 \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{b}^{[16]}} = 
\begin{bmatrix}
\delta^{[16]}_{(1,1)} &amp;amp;
\delta^{[16]}_{(2,1)} &amp;amp;
\delta^{[16]}_{(3,1)} &amp;amp;
\dots &amp;amp;
\delta^{[16]}_{(1000,1)}
\end{bmatrix}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{b}^{[16]}} = \boldsymbol{\delta}^{[16]T}
$$&lt;/div&gt;
&lt;p&gt;So the partial derivative of &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{b}^{[16]}\)&lt;/span&gt; is just the transpose of &lt;span class="math"&gt;\(\boldsymbol{\delta}^{[16]}\)&lt;/span&gt;, meaning that the gradient &lt;span class="math"&gt;\(d\textbf{b}^{[16]}\)&lt;/span&gt; will just be equal to:&lt;/p&gt;
&lt;div class="math"&gt;$$d\textbf{b}^{[16]} = \boldsymbol{\delta}^{[16]T}$$&lt;/div&gt;
&lt;p&gt;Great, so in this section we've talked about how to calculate the gradients &lt;span class="math"&gt;\(\boldsymbol{\delta}^{[16]}\)&lt;/span&gt;, &lt;span class="math"&gt;\(d\textbf{W}^{[16]}\)&lt;/span&gt;, and &lt;span class="math"&gt;\(d\textbf{b}^{[16]}\)&lt;/span&gt; and therefore know how to calculate the gradients for the softmax layer for hopefully any architecture we will encounter in the future.&lt;/p&gt;
&lt;p&gt;After this softmax layer, we have two more fully-connected layers. The only difference between these two fully connected layers and the softmax layer we calculated above is that they use a ReLU as opposed to softmax activation function. I show how to calculate the gradients for these layers in my &lt;a href="/backprop"&gt;backprop post&lt;/a&gt; that I'm sure you are sick of hearing about.&lt;/p&gt;
&lt;p&gt;Working backwards, after the two fully connected layers we reshape our output from a column vector &lt;span class="math"&gt;\(f^{[13]}\)&lt;/span&gt; which has dimensions &lt;span class="math"&gt;\((25088, 1)\)&lt;/span&gt; to a 3D tensor &lt;span class="math"&gt;\(m^{[13]}\)&lt;/span&gt; of shape &lt;span class="math"&gt;\((7, 7, 512)\)&lt;/span&gt;. What is the partial derivative for this transition? Since we don't change the dimensions and only reshape the elements, the partial derivative of &lt;span class="math"&gt;\(\textbf{f}^{[13]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{m}^{[13]}\)&lt;/span&gt; is just the identity matrix with dimensions &lt;span class="math"&gt;\((25088, 25088)\)&lt;/span&gt;. So this partial derivative doesn't change the calculations for the calculating the partial derivatives for the preceding layer.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h2&gt;Backprop for the Max Pooling Layer&lt;/h2&gt;
&lt;p&gt;There are a total of &lt;span class="math"&gt;\(5\)&lt;/span&gt; max pooling layers in the VGG-16 architecture. Since they don't use trainable parameters, we only need to calculate their gradient &lt;span class="math"&gt;\(\boldsymbol{\delta}^{[13]}\)&lt;/span&gt;, which is the input into the max pooling layer and we can use to calculate the gradients for the trainable parameters of that conv layer, &lt;span class="math"&gt;\(\textbf{W}_c^{[13]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{b}_c^{[13]}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In order to calculate its gradient, we need to find the partial derivative of &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{a}^{[13]}\)&lt;/span&gt;. Using the chain rule, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{a}^{[13]}} = \dfrac{\partial \mathcal{L}}{\partial \textbf{m}^{[13]}}\dfrac{\partial \textbf{m}^{[13]}}{\partial \textbf{a}^{[13]}}
$$&lt;/div&gt;
&lt;p&gt;Let's focus on the partial of &lt;span class="math"&gt;\(\textbf{m}^{[13]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{a}^{[13]}\)&lt;/span&gt;. &lt;span class="math"&gt;\(\textbf{m}^{[13]}\)&lt;/span&gt; has &lt;span class="math"&gt;\(7 * 7 * 512 = 25088\)&lt;/span&gt; values, and will therefore be the first dimension of the Jacobian. &lt;span class="math"&gt;\(\textbf{a}^{[13]}\)&lt;/span&gt; has &lt;span class="math"&gt;\(14 * 14 * 512 = 100352\)&lt;/span&gt;, and is the second dimension of the Jacobian. Good lord, this Jacobian has dimensions &lt;span class="math"&gt;\((25088, 100352)\)&lt;/span&gt; and has a total of &lt;span class="math"&gt;\(25088 * 100352 = 2517630976\)&lt;/span&gt; values. As you'll soon see, this Jacobian matrix is very sparse. There are computational shortcuts that libraries like tensorflow and pytorch use to handle these crazy matricies. So no worries.&lt;/p&gt;
&lt;p&gt;Let's spend some time understanding how the elements of this matrix are aligned. We can start by looking at the first row of this matrix:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{bmatrix}
\dfrac{\partial m^{[13]}_{(1,1,1)}}{\partial a^{[13]}_{(1, 1, 1)}}, &amp;amp; 
\dots, &amp;amp; 
\dfrac{\partial m^{[13]}_{(1,1,1)}}{\partial a^{[13]}_{(14, 14, 512)}}
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;So notice that the value in the numerator stays the same and the value of the denominator starts with &lt;span class="math"&gt;\(a^{[13]}_{(1, 1, 1)}\)&lt;/span&gt; and finishes at &lt;span class="math"&gt;\(a^{[13]}_{(14, 14, 512)}\)&lt;/span&gt;. Since they are the ones chaning, let's focus on the values in the denominator:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{bmatrix}
a^{[13]}_{(1, 1, 1)} &amp;amp; 
\dots &amp;amp;
a^{[13]}_{(14, 14, 512)}
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;So basically what we are doing is taking the 3D tensor array &lt;span class="math"&gt;\(\textbf{a}^{[13]}\)&lt;/span&gt; and flattening it into a 1D vector. In math, this operation is sometimes called &lt;span class="math"&gt;\(\textrm{vec}(\textbf{a})\)&lt;/span&gt;. If we expanded this a little out:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{bmatrix}
a^{[13]}_{(1, 1, 1)} &amp;amp;
\dots &amp;amp;
a^{[13]}_{(14, 1, 1)} &amp;amp;
a^{[13]}_{(1, 2, 1)} &amp;amp;
\dots &amp;amp;
a^{[13]}_{(14, 14, 1)} &amp;amp;
a^{[13]}_{(1, 1, 2)} &amp;amp;
\dots &amp;amp;
a^{[13]}_{(14, 14, 512)}
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;So we think of the first dimension as the width, the second as the height, and the third as the channel. So the first &lt;span class="math"&gt;\(14\)&lt;/span&gt; values of the vector are all the values for width for a height of &lt;span class="math"&gt;\(1\)&lt;/span&gt; and channel of &lt;span class="math"&gt;\(1\)&lt;/span&gt;. Then we move to the next height &lt;span class="math"&gt;\(2\)&lt;/span&gt;, keep the channel &lt;span class="math"&gt;\(1\)&lt;/span&gt; the same, and go through all &lt;span class="math"&gt;\(14\)&lt;/span&gt; values of the width. We continue this until we finish all &lt;span class="math"&gt;\(14\)&lt;/span&gt; heights, and we have the first &lt;span class="math"&gt;\(14 * 14 = 196\)&lt;/span&gt; values. Next, we reset the height to &lt;span class="math"&gt;\(1\)&lt;/span&gt; and width to &lt;span class="math"&gt;\(1\)&lt;/span&gt; and repeat the process for channel &lt;span class="math"&gt;\(2\)&lt;/span&gt;. We go through all &lt;span class="math"&gt;\(512\)&lt;/span&gt; channels in this way, until we have a total of &lt;span class="math"&gt;\(14 * 14 * 512 = 100352\)&lt;/span&gt; values.&lt;/p&gt;
&lt;p&gt;So that was the first row of our Jacobian matrix. We can think of the first column of our Jacobian matrix in a similar way:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{bmatrix}
\dfrac{\partial m^{[13]}_{(1,1,1)}}{\partial a^{[13]}_{(1, 1, 1)}}, \\\\ 
\dots \\\\
\dfrac{\partial m^{[13]}_{(7,7,512)}}{\partial a^{[13]}_{(1, 1, 1)}}
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;In this case, the value in the numerator is the one changing, from &lt;span class="math"&gt;\(m^{[13]}_{(1,1,1)}\)&lt;/span&gt; to &lt;span class="math"&gt;\(m^{[13]}_{(7,7,512)}\)&lt;/span&gt;. We can think of this as being similar to above, where we convert &lt;span class="math"&gt;\(\textbf{m}^{[13]}\)&lt;/span&gt; into a flat vector, using the &lt;span class="math"&gt;\(vec\)&lt;/span&gt; operation.&lt;/p&gt;
&lt;p&gt;Now that we have a better intutition about the values in the Jacobian Matrix, let's take the first value in this Jacobian Matrix and see if we can figure it out. Recall that the max pooling operation was defined as being:&lt;/p&gt;
&lt;div class="math"&gt;$$m^{[13]}_{(i,j,k)} = \max_{i * s &amp;lt;= l &amp;lt; i * s + f, j * s &amp;lt;= l &amp;lt; j * s + f }a^{[2]}_{(l,m,k)}$$&lt;/div&gt;
&lt;p&gt;Since all of our max pooling layers in VGG16 use a stride size of &lt;span class="math"&gt;\(2\)&lt;/span&gt; (&lt;span class="math"&gt;\(s = 2\)&lt;/span&gt;) and a &lt;span class="math"&gt;\(2x2\)&lt;/span&gt; filter (&lt;span class="math"&gt;\(f = 2\)&lt;/span&gt;), we get:&lt;/p&gt;
&lt;div class="math"&gt;$$m^{[13]}_{(i,j,k)} = \max_{i * 2 &amp;lt;= l &amp;lt; i * 2 + 2, j * 2 &amp;lt;= l &amp;lt; j * 2 + 2 }a^{[2]}_{(l,m,k)}$$&lt;/div&gt;
&lt;p&gt;Let's say we are interested in calculating &lt;span class="math"&gt;\(m^{[13]}_{(1,1,1)}\)&lt;/span&gt;. We would look for the max value within a &lt;span class="math"&gt;\(2\)&lt;/span&gt; by &lt;span class="math"&gt;\(2\)&lt;/span&gt; window within &lt;span class="math"&gt;\(\textbf{a}^{[2]}\)&lt;/span&gt;. Based on the equation above, the values we will look at are:&lt;/p&gt;
&lt;div class="math"&gt;$$(a^{[13]}_{(1, 1, 1)}, a^{[13]}_{(2, 1, 1)}, a^{[13]}_{(1, 2, 1)}, a^{[13]}_{(2, 2, 1)})$$&lt;/div&gt;
&lt;p&gt;Let's say the actual numerical values are as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$0, 5, 1, -4$$&lt;/div&gt;
&lt;p&gt;Since &lt;span class="math"&gt;\(5\)&lt;/span&gt; is the largest, &lt;span class="math"&gt;\(m^{[13]}_{(1,1,1)} = a^{[13]}_{(2, 1, 1)}\)&lt;/span&gt;, and the partial derivative of &lt;span class="math"&gt;\(m^{[13]}_{(1,1,1)}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(a^{[13]}_{(2, 1, 1)}\)&lt;/span&gt; is equal to:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial m^{[13]}_{(1,1,1)}}{\partial a^{[13]}_{(1, 1, 1)}} = 1$$&lt;/div&gt;
&lt;p&gt;Note that the partial derivatives of &lt;span class="math"&gt;\(m^{[13]}_{(1,1,1)}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(a^{[13]}_{(1, 1, 1)}\)&lt;/span&gt;, &lt;span class="math"&gt;\(a^{[13]}_{(1, 2, 1)}\)&lt;/span&gt;, and &lt;span class="math"&gt;\(a^{[13]}_{(2, 2, 1)}\)&lt;/span&gt; are equal to &lt;span class="math"&gt;\(0\)&lt;/span&gt;. And actually, the partial derivative of &lt;span class="math"&gt;\(m^{[13]}_{(1,1,1)}\)&lt;/span&gt; with respect to all the other values in the conv layer &lt;span class="math"&gt;\(\textbf{a}^{[13]}\)&lt;/span&gt; are also equal to 0 since they aren't in the 2 by 2 max pooling window we used. So the first row of our Jacobian matrix looks like this:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{bmatrix}
\dfrac{\partial m^{[13]}_{(1,1,1)}}{\partial a^{[13]}_{(1, 1, 1)}}, &amp;amp;
\dfrac{\partial m^{[13]}_{(1,1,1)}}{\partial a^{[13]}_{(2, 1, 1)}}, &amp;amp; 
\dots, &amp;amp; 
\dfrac{\partial m^{[13]}_{(1,1,1)}}{\partial a^{[13]}_{(14, 14, 512)}}
\end{bmatrix}
=
\begin{bmatrix}
0 &amp;amp;
1 &amp;amp;
\dots &amp;amp; 
0
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;And we can continue this process for &lt;span class="math"&gt;\(m^{[13]}_{(2,1,1)}\)&lt;/span&gt; through &lt;span class="math"&gt;\(m^{[13]}_{(7,7,512)}\)&lt;/span&gt; and eventually fill up our &lt;span class="math"&gt;\(25088\)&lt;/span&gt; rows of our Jacobian matrix. And that's all to it! Notice that this matrix is very sparse. Each row has exactly one nonzero value and each column has at most one nonzero value. So we don't have to hold this whole matrix in memory. Instead, we can just record the locations of the nonzero values.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Notice that this means that we will only update the weights with errors that correspond with the max values in each window.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We just figured out the partial derivative of &lt;span class="math"&gt;\(\textbf{m}^{[13]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{a}^{[13]}\)&lt;/span&gt;. Since we already calculated the partial derivative of &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{m}^{[13]}\)&lt;/span&gt;, using the chain rule we can use that to calculate the partial derivative of &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{a}^{[13]}\)&lt;/span&gt; and send that result to the preceding layer and for the first time calculate the partial derivatives of the convolutional weights and biases. We'll work on that next.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h2&gt;Backprop for the Conv Layer&lt;/h2&gt;
&lt;p&gt;The final type of layer that we need to calculate the partial derivatives for in order to get the gradients of the trainable parameters are conv layers. Let's focus on the first conv layer that we reach in VGG16, &lt;span class="math"&gt;\(\textbf{a}^{[13]}\)&lt;/span&gt;. Our objective is to calculate the gradients for the trainable parameters in this layer, &lt;span class="math"&gt;\(d\textbf{W}^{[13]}_c\)&lt;/span&gt; and &lt;span class="math"&gt;\(d\textbf{b}^{[13]}_c\)&lt;/span&gt;, as well as the partial derivative of &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{z}^{[13]}\)&lt;/span&gt;, which we use to calculate the gradients for the preceding layers. Let's focus on this first. We can use the chain rule to break apart this partial derivative:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{z}^{[13]}} = \dfrac{\partial \mathcal{L}}{\partial \textbf{a}^{[13]}}\dfrac{\partial \mathcal{\textbf{a}^{[13]}}}{\partial \textbf{z}^{[13]}}
$$&lt;/div&gt;
&lt;p&gt;We've already calculated the partial derivative of &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{a}^{[13]}\)&lt;/span&gt;, so we focus our attention on calculating the partial derivative of &lt;span class="math"&gt;\(\textbf{a}^{[13]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{z}^{[13]}\)&lt;/span&gt;. &lt;span class="math"&gt;\(\textbf{a}^{[13]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{z}^{[13]}\)&lt;/span&gt; have the same dimensions &lt;span class="math"&gt;\((14, 14, 512)\)&lt;/span&gt;. So therefore, the Jacobian matrix is &lt;span class="math"&gt;\(100352\)&lt;/span&gt; by &lt;span class="math"&gt;\(100352\)&lt;/span&gt;. The position of the values from &lt;span class="math"&gt;\(\textbf{a}^{[13]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{z}^{[13]}\)&lt;/span&gt; is very similar to our previous max pooling example, where we can think of the changing values in each row as &lt;span class="math"&gt;\(vec(\textbf{z}^{[13]})\)&lt;/span&gt; and the changing values in each column as &lt;span class="math"&gt;\(vec(\textbf{a}^{[13]})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The transition from &lt;span class="math"&gt;\(\textbf{z}^{[13]}\)&lt;/span&gt; to &lt;span class="math"&gt;\(\textbf{a}^{[13]}\)&lt;/span&gt; just consists of applying the ReLU &lt;span class="math"&gt;\(g(z)\)&lt;/span&gt; nonlinear activation function to each element. What is the ReLU function?&lt;/p&gt;
&lt;div class="math"&gt;$$
g(z) = \begin{cases}
   z &amp;amp;\text{if } z &amp;gt; 0  \\
   0 &amp;amp;\text{if } z =&amp;lt; 0
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;So the ReLU function just returns the value if it's greater than &lt;span class="math"&gt;\(0\)&lt;/span&gt;, and 0 otherwise. What is the derivative of the ReLU function?&lt;/p&gt;
&lt;div class="math"&gt;$$
g'(z) = \begin{cases}
   1 &amp;amp;\text{if } z &amp;gt; 0  \\
   \text{Undefined} &amp;amp;\text{if } z = 0  \\
   0 &amp;amp;\text{if } z &amp;lt; 0
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;Since this function is applied elementwise, the partial derivative of &lt;span class="math"&gt;\(\textbf{a}^{[13]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{z}^{[13]}\)&lt;/span&gt; is just a diagonal matrix, with the derivatives of ReLU that correspond with that element in the diagonals and &lt;span class="math"&gt;\(0\)&lt;/span&gt; for all the other values.&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial \textbf{a}^{[13]}}{\partial \textbf{z}^{[13]}} = 
\begin{bmatrix}
g'(z^{[13]}_{(1,1,1)}) &amp;amp; 0 &amp;amp; \dots &amp;amp; 0 &amp;amp; 0 \\\\
0 &amp;amp; g'(z^{[13]}_{(2,1,1)}) &amp;amp; \dots &amp;amp; 0 &amp;amp; 0 \\\\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots &amp;amp; \vdots \\\\
0 &amp;amp; 0 &amp;amp; \dots &amp;amp; g'(z^{[13]}_{(13,14,512)}) &amp;amp; 0 \\\\
0 &amp;amp; 0 &amp;amp; \dots &amp;amp; 0 &amp;amp; g'(z^{[13]}_{(14,14,512)}) \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Great, so next we focus our attention on calculating the partial derivative of &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{W}_c^{[13]}\)&lt;/span&gt;. Using the chain rule:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{W}_c^{[13]}} = 
\dfrac{\partial \mathcal{L}}{\partial \textbf{z}^{[13]}} 
\dfrac{\partial \mathcal{\textbf{z}^{[13]}}}{\partial \textbf{W}^{[13]}}
$$&lt;/div&gt;
&lt;p&gt;We just calculated the partial derivative of &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{z}^{[13]}\)&lt;/span&gt; and focus on calculating the partial derivative of &lt;span class="math"&gt;\(\textbf{z}^{[13]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{W}_c^{[13]}\)&lt;/span&gt;. This derivative takes it's first dimension from the values in &lt;span class="math"&gt;\(\textbf{z}^{[13]}\)&lt;/span&gt; &lt;span class="math"&gt;\(7 * 7 * 512 = 25088\)&lt;/span&gt; and second dimension from values in &lt;span class="math"&gt;\(\textbf{W}_c^{[13]}\)&lt;/span&gt; &lt;span class="math"&gt;\(3 * 3 * 512 * 512 = 2359296\)&lt;/span&gt;. Its dimensions are therefore &lt;span class="math"&gt;\((25088 , 2359296)\)&lt;/span&gt;. Again, we can think of getting the values for each indexed partial derivatives using the &lt;span class="math"&gt;\(vec()\)&lt;/span&gt; function.&lt;/p&gt;
&lt;p&gt;Let's deconstruct the derivative for &lt;span class="math"&gt;\(z^{[13]}_{(1,1,1)}\)&lt;/span&gt;. Since this is from the first channel of &lt;span class="math"&gt;\(\textbf{z}^{[13]}\)&lt;/span&gt;, we use the first filter channel in &lt;span class="math"&gt;\(\textbf{W}_c^{[13]}\)&lt;/span&gt;. The calculation is as follows:&lt;/p&gt;
&lt;div class="math"&gt;$$z^{[13]}_{(1,1,1)} =
s^{[12]}_{(1, 1, 1)}W^{[13]}_{c(1,1,1,1)} + 
s^{[12]}_{(2, 1, 1)}W^{[13]}_{c(2,1,1,1)} + 
s^{[12]}_{(3, 1, 1)}W^{[13]}_{c(3,1,1,1)} +
$$&lt;/div&gt;
&lt;div class="math"&gt;$$ 
s^{[12]}_{(1, 2, 1)}W^{[13]}_{c(1,2,1,1)} + 
s^{[12]}_{(2, 2, 1)}W^{[13]}_{c(2,2,1,1)} + 
s^{[12]}_{(3, 2, 1)}W^{[13]}_{c(3,2,1,1)} +
$$&lt;/div&gt;
&lt;div class="math"&gt;$$ 
s^{[12]}_{(1, 3, 1)}W^{[13]}_{c(1,3,1,1)} + 
s^{[12]}_{(2, 3, 1)}W^{[13]}_{c(2,3,1,1)} + 
s^{[12]}_{(3, 3, 1)}W^{[13]}_{c(3,3,1,1)} + \dots
$$&lt;/div&gt;
&lt;p&gt;These are the calculations when we multiply the first channel of the first filter with the same padded input &lt;span class="math"&gt;\(\textbf{s^{[12]}}\)&lt;/span&gt;. Next, we just move to the next channel of the first filter, and the third dimension goes from &lt;span class="math"&gt;\(1 \rightarrow 2\)&lt;/span&gt;. We repeat this process for the 512 channels.&lt;/p&gt;
&lt;div class="math"&gt;$$
\dots + 
s^{[12]}_{(1, 1, 2)}W^{[13]}_{c(1,1,2,1)} + 
s^{[12]}_{(2, 1, 2)}W^{[13]}_{c(2,1,2,1)} + 
s^{[12]}_{(3, 1, 2)}W^{[13]}_{c(3,1,2,1)} +
$$&lt;/div&gt;
&lt;div class="math"&gt;$$ 
s^{[12]}_{(1, 2, 2)}W^{[13]}_{c(1,2,2,1)} + 
s^{[12]}_{(2, 2, 2)}W^{[13]}_{c(2,2,2,1)} + 
s^{[12]}_{(3, 2, 2)}W^{[13]}_{c(3,2,2,1)} +
$$&lt;/div&gt;
&lt;div class="math"&gt;$$ 
s^{[12]}_{(1, 3, 2)}W^{[13]}_{c(1,3,2,1)} + 
s^{[12]}_{(2, 3, 2)}W^{[13]}_{c(2,3,2,1)} + 
s^{[12]}_{(3, 3, 2)}W^{[13]}_{c(3,3,2,1)} + \dots
$$&lt;/div&gt;
&lt;p&gt;And then eventually we reach the channel number &lt;span class="math"&gt;\(512\)&lt;/span&gt; in the first filter:&lt;/p&gt;
&lt;div class="math"&gt;$$
\vdots
$$&lt;/div&gt;
&lt;div class="math"&gt;$$ 
s^{[12]}_{(1, 3, 512)}W^{[13]}_{c(1,3,512,1)} + 
s^{[12]}_{(2, 3, 512)}W^{[13]}_{c(2,3,512,1)} + 
s^{[12]}_{(3, 3, 512)}W^{[13]}_{c(3,3,512,1)} + b^{[1]}_{(1,1)}
$$&lt;/div&gt;
&lt;p&gt;Recall from the blog post on &lt;a href="/vgg_forwardprop"&gt;VGG16 forward propagation&lt;/a&gt; that &lt;span class="math"&gt;\(\textbf{s}^{[12]}\)&lt;/span&gt; is the activation from the previous layer padded with one border of zeros &lt;span class="math"&gt;\(p = 1\)&lt;/span&gt; using same padding.&lt;/p&gt;
&lt;p&gt;What happens when we take the partial derivative of &lt;span class="math"&gt;\(z^{[13]}_{(1,1,1)}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(W^{[13]}_{c(1,1,1,1)}\)&lt;/span&gt;? Notice we just get the value for the padding layer &lt;span class="math"&gt;\(s^{[13]}_{(1, 1, 1)}\)&lt;/span&gt; and everything else is equal to 0. &lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial z^{[13]}_{(1,1,1)}}{\partial W^{[13]}_{c(1,1,1,1)}} = s^{[12]}_{(1, 1, 1)} + 0 + 0 + ... + 0$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial z^{[13]}_{(1,1,1)}}{\partial W^{[13]}_{c(1,1,1,1)}} = s^{[12]}_{(1, 1, 1)}$$&lt;/div&gt;
&lt;p&gt;So notice that the first row of the Jacobian Matrix will have &lt;span class="math"&gt;\(3 * 3 * 512 = 4608\)&lt;/span&gt; nonzero elements, which correspond to the values multiplied by the weights in the filter. Notice that this is a very sparse row, since there are a total of &lt;span class="math"&gt;\(2359296\)&lt;/span&gt; elements in the row.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we breakdown the architecture of VGG-16 and used it to explain some of the fundamental building blocks of the convolutional network - pooling layers and conv layers. We discussed some of the benefits of convolutional networks over fully connected layers and talked briefly about how backpropagation works for VGG-16.&lt;/p&gt;
&lt;p&gt;You might have felt a little disatisfied with the math behind backpropagation the way that I explained it. At the end of this post, I also feel disatisfied. There is a transformation called &lt;code&gt;im2col&lt;/code&gt; which flattens the input and filter bank as 2-dimensinoal matrices. Many explanations of backpropagation for convolutional networks use this function to simplify the computation (at the expense of memory) and I think it makes everything a lot simpler. In a future post, I will describe the &lt;code&gt;im2col&lt;/code&gt; operation within the context of backpropagation, but I think for now we ware good.&lt;/p&gt;
&lt;p&gt;As always, thanks so much for reading though my post! Any commens and questions would be greatly appreciated.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="posts"></category><category term="neural networks"></category><category term="machine learning"></category><category term="convolutional networks"></category><category term="VGG"></category></entry><entry><title>The Math behind Neural Networks - Backpropagation</title><link href="http://www.jasonosajima.com/backprop.html" rel="alternate"></link><published>2018-07-18T00:00:00-07:00</published><updated>2018-07-18T00:00:00-07:00</updated><author><name>Jason Osajima</name></author><id>tag:www.jasonosajima.com,2018-07-18:/backprop.html</id><summary type="html">
&lt;p&gt;The hardest part about deep learning for me was backpropagation. Forward propagation made sense; basically you do a bunch of matrix multiplications, add some bias terms, and throw in non-linearities so it doesn't turn into one large matrix multiplication. Gradient descent also intuitively made sense to me as well; we want to use the partial derivatives of our parameters with respect to our cost function (&lt;span class="math"&gt;\(J\)&lt;/span&gt;) to update our parameters in order to minimize &lt;span class="math"&gt;\(J\)&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;The objective of backpropagation is pretty clear: we need to calculate the partial derivatives of our parameters with respect to cost function (&lt;span class="math"&gt;\(J\)&lt;/span&gt;) in order to use it for gradient descent. The difficult part lies in keeping track of the calculations, since each partial derivative of parameters in each layer rely on inputs from the previous layer. Maybe it's also the fact that we are going backwards makes it hard for my brain to wrap my head around it.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;&lt;em&gt;This is part two in a two-part series on the math behind neural networks. Part two is about backpropagation. Part one is about forward propagation and can be found &lt;a href="/forwardprop"&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;


&lt;p&gt;The hardest part about deep learning for me was backpropagation. Forward propagation made sense; basically you do a bunch of matrix multiplications, add some bias terms, and throw in non-linearities so it doesn't turn into one large matrix multiplication. Gradient descent also intuitively made sense to me as well; we want to use the partial derivatives of our parameters with respect to our cost function (&lt;span class="math"&gt;\(J\)&lt;/span&gt;) to update our parameters in order to minimize &lt;span class="math"&gt;\(J\)&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;The objective of backpropagation is pretty clear: we need to calculate the partial derivatives of our parameters with respect to cost function (&lt;span class="math"&gt;\(J\)&lt;/span&gt;) in order to use it for gradient descent. The difficult part lies in keeping track of the calculations, since each partial derivative of parameters in each layer rely on inputs from the previous layer. Maybe it's also the fact that we are going backwards makes it hard for my brain to wrap my head around it.&lt;/p&gt;


&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Why do we need to learn the math behind backpropagation? Today we have great deep learning frameworks like Tensorflow and PyTorch that do backpropagation automatically, so why do we need to know how it works under the hood?&lt;/p&gt;
&lt;p&gt;Josh Waitzkin is an internationally-recognized chess player (and the movie and book "Searching for Bobby Fischer" were both based on his career) and world champion in Tai Chi Chuan. &lt;/p&gt;
&lt;p&gt;Most kids start their chess career by learning opening variations of the game. Kids learn strong opening positions in order to gain a decisive advantage in the beginning of a match. This advantage proves too much for most of their opponents and they end up winning. &lt;/p&gt;
&lt;p&gt;Josh Waitzkin's teacher did not focus on opening variations. Instead, he focused on end game scenarios, such as King v. King or King v. King Pawn. These end game scenarios were simple enough for Josh to develop an intuitive understanding of how pieces interact with one another. &lt;/p&gt;
&lt;p&gt;Kids that learn opening variations couldn't internalize these first principles about the interactions between pieces because there was too much stuff going on. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A critical challenge for all practical martial artists is to make their diverse techniques take on the efficiency of the jab. When I watched William Chen spar, he was incredibly understated and exuded shocking power. While some are content to call such abilities chi and stand in awe, I wanted to to understand what was going on. The next phase of my martial growth would involve turning the large into the small. My understanding of this process, in the spirit of my numbers to leave numbers method of chess study, is to touch the essence (for example, highly refined and deeply internalized body mechanics or feeling) of a technique, and then to incrementally condense the external manifestation of the tecnhique while keeping true to its essence. Over time expanding decreases while potency increases. I call this method 'making smaller circles'.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I think there are several ways that we can interpret this quote. For someone who is a machine learning practitioner, it may be good to take a model that you have a pretty good understanding of and try and simplify it to try to understand everything at its most basic level.&lt;/p&gt;
&lt;p&gt;The best resource for me to learn backpropagation has been Andrew Ng's &lt;a href="https://www.coursera.org/specializations/deep-learning"&gt;Deep Learning Specialization in Coursera&lt;/a&gt;. He is always very clear about understanding the intutition behind the problem, defining the problem and then laying out the mathematical notation. In this blog post I will rely heavily on the notation and concepts used in his course.&lt;/p&gt;
&lt;p&gt;Let's make smaller circles.&lt;/p&gt;
&lt;p&gt;Backpropagation can be used in different ways, but for our purposes we will use it to train a binary classifier. In my &lt;a href="/forwardprop"&gt;previous post&lt;/a&gt; on forward propagation, I layout the architecture for a 3 layer Neural Network, which we will rely on for this example.&lt;/p&gt;
&lt;p&gt;Backpropagation starts with our loss function, so we will introduce this idea first. But before we get into the math, let's define what notation we will use through the course of this blog post.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h2&gt;Some Notation&lt;/h2&gt;
&lt;p&gt;For this blog post, any time we define a vector or matrix, we will bold it. Anytime we define a scalar, we will keep it normal. In the previous blog post on forward propagation, we introduced a 3-layer neural network architecture:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/nn_3.png" title="[nn_3]" alt="[nn_3]"&gt;&lt;/p&gt;
&lt;p&gt;So for example, &lt;span class="math"&gt;\(\textbf{a}^{(i)[1]}\)&lt;/span&gt; is a vector and is therefore bolded. The third entry for &lt;span class="math"&gt;\(\textbf{a}^{(i)[1]}\)&lt;/span&gt; is &lt;span class="math"&gt;\(a^{(i)[1]}_{31}\)&lt;/span&gt;. Since it is a scalar, it is not bolded. &lt;/p&gt;
&lt;p&gt;You may have noticed that &lt;span class="math"&gt;\(a^{(i)[3]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\hat{y}^{(i)}\)&lt;/span&gt; are not bolded. That's because they are scalars &lt;span class="math"&gt;\(a^{(i)[3]} = \hat{y}^{(i)} \in (0, 1)\)&lt;/span&gt;, and represent the probability that we think the &lt;span class="math"&gt;\(ith\)&lt;/span&gt; example belongs to the positive class, &lt;span class="math"&gt;\(y = 1\)&lt;/span&gt;. More on this later.&lt;/p&gt;
&lt;p&gt;The &lt;span class="math"&gt;\(i\)&lt;/span&gt; denotes that &lt;span class="math"&gt;\(\textbf{a}^{(i)[1]}\)&lt;/span&gt; is the activation in the &lt;span class="math"&gt;\(L = 1\)&lt;/span&gt; layer for the &lt;span class="math"&gt;\(ith\)&lt;/span&gt; example. For simplicity, we will get rid of the &lt;span class="math"&gt;\((i)\)&lt;/span&gt; notation and assume that we are working with the &lt;span class="math"&gt;\(ith\)&lt;/span&gt; training example. Our architecture therefore becomes:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/nn_simplified.png" title="[nn_simplified]" alt="[nn_simplified]"&gt;&lt;/p&gt;
&lt;p&gt;Lovely, that looks much simpler. You might be wondering why we decided to define all of our vectors as column vectors instead of row vectors. If a vector has &lt;span class="math"&gt;\(m\)&lt;/span&gt; entries, a column vector is defined to be a &lt;span class="math"&gt;\((m,1)\)&lt;/span&gt; dimensional matrix and a row vector is a &lt;span class="math"&gt;\((1, m)\)&lt;/span&gt; dimensional matrix.&lt;/p&gt;
&lt;p&gt;Some people use row vectors and others use column vectors. Most of the resources I used to write this blog post use column vectors to define the activations for each layer, so that's what we will roll with.&lt;/p&gt;
&lt;p&gt;When we define a column vector for the outputs from different layers, we will bold it and use a lowercase letter to represent it. When we define a weight matrix, we will bold it and use an uppercase letter to define it. So for example, the weight matrix we use to transition from the 2nd layer (1st hidden layer) to the 3rd layer (2nd hidden layer) would be &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt; with dimensions &lt;span class="math"&gt;\((2, 4)\)&lt;/span&gt; that match the 1st dimension of the layer it's transitioning to (2) and the 1st dimension of the layer it comes from (4). Each entry is a scalar, and therefore is not bolded.&lt;/p&gt;
&lt;div class="math"&gt;$$\textbf{W}^{[2]} = 
\begin{bmatrix}
    W^{[2]}_{11} &amp;amp; W^{[2]}_{12} &amp;amp; W^{[2]}_{13} &amp;amp; W^{[2]}_{14} \\\\
    W^{[2]}_{21} &amp;amp; W^{[2]}_{22} &amp;amp; W^{[2]}_{23} &amp;amp; W^{[2]}_{24} \\
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h2&gt;Understanding the Loss Function&lt;/h2&gt;
&lt;p&gt;In the previous post we used forward propagation to go from an input vector for the &lt;span class="math"&gt;\(ith\)&lt;/span&gt; training example &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; to &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt;. Recall that &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt; is our best guess for the class &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; belongs to. In our example, &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; could belong to either happy (&lt;span class="math"&gt;\(0\)&lt;/span&gt;) or sad (&lt;span class="math"&gt;\(1\)&lt;/span&gt;).  &lt;span class="math"&gt;\(y\)&lt;/span&gt; is the class (either 0 or 1) that &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; actually belongs to. So how can we measure how well our model is doing, i.e. how can we measure how close the prediction &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt; is to the actual &lt;span class="math"&gt;\(y\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;In order to measure the error between these two values, we use what's called a loss function. When I was introduced to the concept of a loss function, I immediately thought we should use this one:&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{L}(\hat{y}, y) = \dfrac{1}{2}(\hat{y} - y)^2$$&lt;/div&gt;
&lt;p&gt;This is a pretty simple loss function: just subtract the actual from the predicted, square it so it isn't negative, and then divide it by 2 (so the derivative looks prettier). It turns out that people don't use this loss function in logistic regression because when you try to learn the parameters the optimization problem is non-convex.&lt;/p&gt;
&lt;p&gt;A better loss function to use is this:
&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{L}(\hat{y}, y) = -ylog(\hat{y}) - (1-y)log(1 -\hat{y})$$&lt;/div&gt;
&lt;p&gt;There are several choices for what we can use for our loss function. Let's spend some time to understand how we use this loss function. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;One thing that I didn't get when I first started working on this: In high school math if you had for example &lt;span class="math"&gt;\(log \ 2\)&lt;/span&gt;, this was shorthand for log base 10, or &lt;span class="math"&gt;\(log_{10} \ 2\)&lt;/span&gt;. However, you'll find that most people that work on stats and computer science problems actually use &lt;span class="math"&gt;\(log \ 2\)&lt;/span&gt; to mean &lt;span class="math"&gt;\(ln \ 2\)&lt;/span&gt;. So in this blog post, whenever I use &lt;span class="math"&gt;\(log\)&lt;/span&gt;, I mean &lt;span class="math"&gt;\(ln\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Our objective is to try to get &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; to be as low as possible, since &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; represents the error between our prediction &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt; and the actual &lt;span class="math"&gt;\(y\)&lt;/span&gt;. Notice that when &lt;span class="math"&gt;\(y = 1\)&lt;/span&gt;, our equation turns into this:&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{L}(\hat{y},  y = 1) = -1log(\hat{y}) - (1-1)log(1 - \hat{y})$$&lt;/div&gt;
&lt;div class="math"&gt;$$\mathcal{L}(\hat{y},  y = 1) = -1log(\hat{y})$$&lt;/div&gt;
&lt;p&gt;So to minimize &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; we want &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt; to be as large as possible, which makes sense since in the final layer we put each entry &lt;span class="math"&gt;\(z^{[3]}_{1j}\)&lt;/span&gt; in the activity &lt;span class="math"&gt;\(\mathbf{z}^{[3]}\)&lt;/span&gt; through the sigmoid function, like &lt;span class="math"&gt;\(\sigma(\mathbf{z}^{[3]})\)&lt;/span&gt;. The range of the sigmoid function is &lt;span class="math"&gt;\((0, 1)\)&lt;/span&gt;, so the greatest value that &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt; can take is a number super close to 1. Conversely, when &lt;span class="math"&gt;\(y = 0\)&lt;/span&gt;, our equation turns into this:&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{L}(\hat{y},  y = 0) = -0log(\hat{y}) - (1-0)log(1 - \hat{y})$$&lt;/div&gt;
&lt;div class="math"&gt;$$\mathcal{L}(\hat{y},  y = 0) = -1log(1 - \hat{y})$$&lt;/div&gt;
&lt;p&gt;In this case, in order to minimize &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt;, we want &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt; to be close to 0 as possible.&lt;/p&gt;
&lt;p&gt;This seems like it works, but where does this loss function come from? I'm glad you asked, it's kind of fun to figure out how it works.&lt;/p&gt;
&lt;p&gt;So in the previous post we talked about how &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt; is the probability that the example &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt; is from either the class 1, or &lt;span class="math"&gt;\(y = 1\)&lt;/span&gt;. And also keep in mind that all of these technically should have a superscript &lt;span class="math"&gt;\((i)\)&lt;/span&gt; attached to them. So more formally, &lt;span class="math"&gt;\(\hat{y} = P(y = 1 | \mathbf{x})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We can think of &lt;span class="math"&gt;\(y\)&lt;/span&gt; as a Bernoulli random variable that can take on 1 with probability &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt; and 0 with probability &lt;span class="math"&gt;\(1 - \hat{y}\)&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;The probability mass function for a Bernoulli random variable looks like this:&lt;/p&gt;
&lt;div class="math"&gt;$$p(k | p) = k^p(1-k)^{(1-p)}$$&lt;/div&gt;
&lt;p&gt;Which tells you the probability that &lt;span class="math"&gt;\(k\)&lt;/span&gt; is equal to a particular value. We want to calculate the probability that &lt;span class="math"&gt;\(y\)&lt;/span&gt; takes on a particular value so, thinking about our example, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$p(y | \hat{y}, \mathbf{x}) = y^{\hat{y}}(1-y)^{(1-\hat{y})}$$&lt;/div&gt;
&lt;p&gt;We want to maximize &lt;span class="math"&gt;\(p\)&lt;/span&gt;, or maximize the probability that given our training example feature vector &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt; and prediction outputted from our neural network &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt;, we get the value &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Ok so hopefully that makes sense so far. Why can't we just use this as our loss function, since the function shows how close how our prediction (&lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt;) is to our actual (&lt;span class="math"&gt;\(y\)&lt;/span&gt;)? The reason is in backpropagation we need to take the derivative of our loss function, and it gets a little messy to take the derivative of this function.&lt;/p&gt;
&lt;p&gt;But notice that if we take the log of both sides, our objective stays the same. Instead of maximizing &lt;span class="math"&gt;\(p\)&lt;/span&gt;, we still just need to maximize the &lt;span class="math"&gt;\(log\)&lt;/span&gt; of &lt;span class="math"&gt;\(p\)&lt;/span&gt;, since the &lt;span class="math"&gt;\(log\)&lt;/span&gt; function increases monotonically. If we take the &lt;span class="math"&gt;\(log\)&lt;/span&gt; of both sides, and do a little math, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$log \ p(y | \hat{y}, \mathbf{x}) = log \ (y^{\hat{y}}(1-y)^{(1-\hat{y})})$$&lt;/div&gt;
&lt;div class="math"&gt;$$log \ p(y | \hat{y}, \mathbf{x}) = \hat{y}log \ y + (1-\hat{y})log \ (1-y)$$&lt;/div&gt;
&lt;p&gt;Notice that the right side just becomes &lt;span class="math"&gt;\(-\mathcal{L}(\hat{y}, y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$log \ p(y | \hat{y}, x) = -\mathcal{L}(\hat{y}, y)$$&lt;/div&gt;
&lt;div class="math"&gt;$$\mathcal{L}(\hat{y}, y) = -log \ p(y | \hat{y}, x)$$&lt;/div&gt;
&lt;p&gt;So when we say we want to minimize &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt;, we really mean we want to maximize the probability that &lt;span class="math"&gt;\(y\)&lt;/span&gt; is equal to it's value given our prediction &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt; and feature vector &lt;span class="math"&gt;\(x\)&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;So we figured out what &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; is equal to, which represents our loss for one training example &lt;span class="math"&gt;\(i\)&lt;/span&gt;. We could just use the gradients of &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to each scalar entry in each of our parameters. We then could use those gradients to update the values of our parameters in gradient descent. In that case, our loss function &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; would be the same as our cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt; or:&lt;/p&gt;
&lt;div class="math"&gt;$$ J = \mathcal{L}(\hat{y},y) $$&lt;/div&gt;
&lt;p&gt;In this case, we call our optimization algorithm stochastic gradient descent. We could also take a batch of training examples, say &lt;span class="math"&gt;\(m\)&lt;/span&gt; training examples and define our cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt; to be the average of the loss &lt;span class="math"&gt;\(\mathcal{L}(\hat{y}^{(i)},y^{(i)})\)&lt;/span&gt; for &lt;span class="math"&gt;\(m\)&lt;/span&gt; training examples, or:&lt;/p&gt;
&lt;div class="math"&gt;$$J = \dfrac{1}{m}\sum^m_{i = 1} \mathcal{L(\hat{y}^{(i)},y^{(i)})}$$&lt;/div&gt;
&lt;p&gt;If &lt;span class="math"&gt;\(m\)&lt;/span&gt; is equal to the number of training examples we have access to, we usually call our optimization algorithm batch gradient descent. If &lt;span class="math"&gt;\(m\)&lt;/span&gt; is less than the number of training examples, we call it mini-batch gradient descent.&lt;/p&gt;
&lt;p&gt;For simplicity, in this blog post we will focus on stochastic gradient descent, so our cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt; is:&lt;/p&gt;
&lt;div class="math"&gt;$$ J = \mathcal{L}(\hat{y},y) $$&lt;/div&gt;
&lt;p&gt;Great, so now we have defined &lt;span class="math"&gt;\(J\)&lt;/span&gt; and we want to minimize it. How do we do that? Most people use gradient descent, which requires us to calculate the gradient for &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to the parameters that we can change. Calculating these gradients is the objective of backpropagation.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h2&gt;Introducting Backpropagation&lt;/h2&gt;
&lt;p&gt;In backpropagation, our objective is to calculate the gradients of &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to what we can change in our neural network. In our three layer network, we can change the value of our parameters. Recall that the architecture of our network looked like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/nn_3.png" title="[nn_3]" alt="[nn_3]"&gt;&lt;/p&gt;
&lt;p&gt;We can think of the nodes from the two layers connecting the input to the output layer as the intermediate products of the model.&lt;/p&gt;
&lt;p&gt;Notice that this diagram doesn't include any of the parameters of our model. The parameters are the weights (&lt;span class="math"&gt;\(\textbf{W}^{[j]}\)&lt;/span&gt;) and biases (&lt;span class="math"&gt;\(\textbf{b}^{[j]}\)&lt;/span&gt;) associated with the j-th layer. Because in order to go from one layer to the next, we multiply the nodes from the previous layer by the weights, add a bias, and send it through an activation function.&lt;/p&gt;
&lt;p&gt;We can think of the lines that connect the nodes of each layer to represent these transformations. In the diagram, there are three sets of lines connecting the four layers, and unsurprisingly there are three sets of weights and biases to go along with them:&lt;/p&gt;
&lt;div class="math"&gt;$$(\textbf{W}^{[1]}, \textbf{b}^{[1]}, \textbf{W}^{[2]}, \textbf{b}^{[2]}, \textbf{W}^{[3]}, b^{[3]})$$&lt;/div&gt;
&lt;p&gt;Let's understand the dimensions of each of these parameters. In the previous post, we talked about how the dimensions of the weights that connect layers are equal to the number of entries in those layers, represented by column vectors. So for example, &lt;span class="math"&gt;\(\textbf{W}^{[1]}\)&lt;/span&gt; connects &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; a column vector with 3 entries to &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt;, a column vector with 3 entires. So the dimensions of &lt;span class="math"&gt;\(\textbf{W}^{[1]}\)&lt;/span&gt; will be &lt;span class="math"&gt;\((4, 3)\)&lt;/span&gt;. Similarly, the dimensions of &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt; will be &lt;span class="math"&gt;\((2, 4)\)&lt;/span&gt; and the dimensions of &lt;span class="math"&gt;\(\textbf{W}^{[3]}\)&lt;/span&gt; will be &lt;span class="math"&gt;\((1, 2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Biases are simpler, since they match the dimensions of the layer that we are headed towards. So for example, &lt;span class="math"&gt;\(\textbf{b}^{[1]}\)&lt;/span&gt; is &lt;span class="math"&gt;\((4, 1)\)&lt;/span&gt;, &lt;span class="math"&gt;\(\textbf{b}^{[2]}\)&lt;/span&gt; is &lt;span class="math"&gt;\((2, 1)\)&lt;/span&gt;, and &lt;span class="math"&gt;\(b^{[3]}\)&lt;/span&gt; is a scalar value.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;Gradients for Activation Functions?&lt;/h3&gt;
&lt;p&gt;Do we need to worry about parameters in the activation functions we use? Let's first recall the activation functions that we are using in our example. We use a ReLU function &lt;span class="math"&gt;\(g()\)&lt;/span&gt; to go from the input layer &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; to the first hidden layer &lt;span class="math"&gt;\(\textbf{a}^{[1]}\)&lt;/span&gt;, a ReLU function &lt;span class="math"&gt;\(g()\)&lt;/span&gt; to go from the first hidden layer &lt;span class="math"&gt;\(\textbf{a}^{[1]}\)&lt;/span&gt; to the second hidden layer &lt;span class="math"&gt;\(\textbf{a}^{[2]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
g(z) = \begin{cases}
   x &amp;amp;\text{if } z &amp;gt; 0  \\
   0 &amp;amp;\text{otherwise}
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;Where &lt;span class="math"&gt;\(z\)&lt;/span&gt; is a scalar value.&lt;/p&gt;
&lt;p&gt;Notice that the ReLU function doesn't include any parameters that we would need to optimize in our model.&lt;/p&gt;
&lt;p&gt;We use a sigmoid function &lt;span class="math"&gt;\(\sigma()\)&lt;/span&gt; to go from the second hidden layer &lt;span class="math"&gt;\(\textbf{a}^{[2]}\)&lt;/span&gt; to the final output layer &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt;. &lt;/p&gt;
&lt;div class="math"&gt;$$
\sigma(z) = \dfrac{1}{1+e^{-z}}
$$&lt;/div&gt;
&lt;p&gt;Same as the ReLU function, there are no parameters that we need to optimize in this function.&lt;/p&gt;
&lt;p&gt;So we talked about how in backpropagation we calculate the gradient with respect to each of the parameters that we are interested in optimizing. The gradient is just the partial derivative with respect to each parameter. So for example, the gradient of the cost function (&lt;span class="math"&gt;\(J\)&lt;/span&gt;) with respect to the weight that connects the third node in the input layer to the second node in the first hidden layer will be:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{J}}{\partial{W^{[1]}_{23}}}$$&lt;/div&gt;
&lt;p&gt;Each time we do backpropagation, we need to not only calculate this gradient, but the gradients for all of our parameters. How many gradients do we need to calculate? If we multiply the dimensions for each of our weights &lt;span class="math"&gt;\((12 + 8 + 2 = 22)\)&lt;/span&gt; and biases &lt;span class="math"&gt;\((4+2+1 = 7)\)&lt;/span&gt;, we get 22 + 7 = 29 parameters and therefore 29 gradients (like the one above).&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h4&gt;Vectorizing the Gradients&lt;/h4&gt;
&lt;p&gt;In the same way we combined the feature vectors &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; with dimensions &lt;span class="math"&gt;\((3,1)\)&lt;/span&gt; of &lt;span class="math"&gt;\(m\)&lt;/span&gt; training examples into a matrix &lt;span class="math"&gt;\(X\)&lt;/span&gt; with dimensions &lt;span class="math"&gt;\((3, m)\)&lt;/span&gt;, we can take our 29 gradients and combine them into gradient matricies to make our notation a little easier to follow. For example, we could represent the gradient of the cost function (&lt;span class="math"&gt;\(J\)&lt;/span&gt;) with respect to &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt; as just a matrix of the partial derivatives of &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to each entry &lt;span class="math"&gt;\(W^{[2]}_{ij}\)&lt;/span&gt; like so:&lt;/p&gt;
&lt;div class="math"&gt;$$d\textbf{W}^{[2]} = 
\begin{bmatrix}
    \dfrac{\partial{J}}{\partial{W^{[2]}_{11}}} &amp;amp; 
    \dfrac{\partial{J}}{\partial{W^{[2]}_{12}}} &amp;amp;
    \dfrac{\partial{J}}{\partial{W^{[2]}_{13}}} &amp;amp;
    \dfrac{\partial{J}}{\partial{W^{[2]}_{14}}} \\\\
    \dfrac{\partial{J}}{\partial{W^{[2]}_{21}}} &amp;amp;
    \dfrac{\partial{J}}{\partial{W^{[2]}_{22}}} &amp;amp;
    \dfrac{\partial{J}}{\partial{W^{[2]}_{23}}} &amp;amp;
    \dfrac{\partial{J}}{\partial{W^{[2]}_{24}}} \\
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;Notice that the dimensions of the matrix &lt;span class="math"&gt;\(d\textbf{W}^{[2]}\)&lt;/span&gt; match &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt;, which makes the gradient update a very simple elementwise operation.&lt;/p&gt;
&lt;p&gt;So why isn't that matrix of partial derivatives equal to:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{J}}{\partial{\textbf{W}^{[2]}}}$$&lt;/div&gt;
&lt;p&gt;There are tons of resources online that treat these two things as the same thing. In fact, it wasn't until I took Andrew Ng's Deep Learning Specialization on Coursera that I was introduced to the notation &lt;span class="math"&gt;\(d\textbf{W}^{[2]}\)&lt;/span&gt;. The problem is that for simple examples, they are equal to each other, but for more complex examples they won't be equal to each other. We will think of them as separate, and the partial derivative of &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to  &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt; we can sometimes use as an intermediate calculation to arrive at &lt;span class="math"&gt;\(d\textbf{W}^{[2]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We started with 29 gradients, and we can now collapse those gradients into 6 gradient matricies:&lt;/p&gt;
&lt;div class="math"&gt;$$\bigg( d\textbf{W}^{[1]}, d\textbf{b}^{[1]}, d\textbf{W}^{[2]}, d\textbf{b}^{[2]}, 
d\textbf{W}^{[3]}, db^{[3]} \bigg)$$&lt;/div&gt;
&lt;p&gt;Keep in mind that each of these gradient matricies should match the dimensions of the parameter they correspond to.&lt;/p&gt;
&lt;p&gt;Next, let's talk about how we implement backpropagation to calculate these gradient matricies.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h2&gt;Implementing Backpropagation&lt;/h2&gt;
&lt;p&gt;In forward propagation, given a feature vector &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; for the &lt;span class="math"&gt;\(ith\)&lt;/span&gt; example, our goal was to calculate one output, &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt; which is our best guess for what class the example &lt;span class="math"&gt;\(i\)&lt;/span&gt; belongs to.&lt;/p&gt;
&lt;p&gt;In backpropagation, for our 3 layer neural network example our goal is to calculate the 6 gradient matricies.&lt;/p&gt;
&lt;p&gt;We do this (unsurprisingly) by working backwards. So we will start by calculating:&lt;/p&gt;
&lt;div class="math"&gt;$$d\textbf{W}^{[3]}, db^{[3]}$$&lt;/div&gt;
&lt;p&gt;What are the equations that connect our cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt; to our weights and biases &lt;span class="math"&gt;\(\textbf{W}^{[3]}, b^{[3]}\)&lt;/span&gt; ? We are going to use &lt;span class="math"&gt;\(a^{[3]}\)&lt;/span&gt; instead of &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt;, but if it tickles your fancy you can feel free to use &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$z^{[3]} = \textbf{W}^{[3]}a^{[2]} + b^{[3]}$$&lt;/div&gt;
&lt;div class="math"&gt;$$a^{[3]} = \sigma(z^{[3]})$$&lt;/div&gt;
&lt;div class="math"&gt;$$J(a^{[3]}, y) = -ylog(a^{[3]}) - (1-y)log(1 -a^{[3]})$$&lt;/div&gt;
&lt;p&gt;So using the chain rule from calculus, we can think of &lt;span class="math"&gt;\(J\)&lt;/span&gt; as the composition of two other functions, &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(a^{[3]}\)&lt;/span&gt; and thefore write the two gradient matricies as:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{J}}{\partial{\textbf{W}^{[3]}}} = 
\dfrac{d{J}}{d{a^{[3]}}}
\dfrac{d{a^{[3]}}}{d{z^{[3]}}}
\dfrac{\partial{z^{[3]}}}{\partial{\textbf{W}^{[3]}}}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{J}}{\partial{b^{[3]}}} =
\dfrac{d{J}}{d{a^{[3]}}}
\dfrac{d{a^{[3]}}}{d{z^{[3]}}}
\dfrac{\partial{z^{[3]}}}{\partial{b^{[3]}}}$$&lt;/div&gt;
&lt;p&gt;Before moving on, notice that the first equation calculates the partial derivative of &lt;span class="math"&gt;\(J\)&lt;/span&gt; (a scalar value) with respect to &lt;span class="math"&gt;\(\textbf{W}^{[3]}\)&lt;/span&gt;, which is a row vector with dimensions &lt;span class="math"&gt;\((1,2)\)&lt;/span&gt;. So we are definitely going down the dark path of calculating vector and matrix derivatives. &lt;/p&gt;
&lt;p&gt;If you threw up in a little bit in your mouth at the prospect of taking a derivative with respect to a row vector, I found these &lt;a href="https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf"&gt;two&lt;/a&gt; &lt;a href="http://cs231n.stanford.edu/vecDerivs.pdf"&gt;resources&lt;/a&gt; helpful to understand the math a bit better.&lt;/p&gt;
&lt;p&gt;Ok great, so now let's figure out what these 2 derivatives and 2 partial derivatives are equal to, so we can ultimately calculate the gradient of &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{W}^{[3]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(b^{[3]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;So why do some of the derivatives use &lt;span class="math"&gt;\(d\)&lt;/span&gt; and others use &lt;span class="math"&gt;\(\partial\)&lt;/span&gt;? The simplest answer is that the ones that have a &lt;span class="math"&gt;\(d\)&lt;/span&gt; only depend on one variable, whereas the ones that use &lt;span class="math"&gt;\(\partial\)&lt;/span&gt; rely on more than one variable. So for example, when we think of the derivative of &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(a^{[3]}\)&lt;/span&gt;, we think of this equation:&lt;/p&gt;
&lt;div class="math"&gt;$$J(a^{[3]} | y) = -ylog(a^{[3]}) - (1-y)log(1 -a^{[3]})$$&lt;/div&gt;
&lt;p&gt;Which only uses one variable, &lt;span class="math"&gt;\(a^{[3]}\)&lt;/span&gt;. Recall that &lt;span class="math"&gt;\(y\)&lt;/span&gt; is constant, since that is the label for the class that the training example &lt;span class="math"&gt;\(i\)&lt;/span&gt; belongs to. Since we only consider one variable, we use &lt;span class="math"&gt;\(d\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;But remember that &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; is a composition of two other functions, &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(a^{[3]}\)&lt;/span&gt;. So technically if we wrote everything out we would get:&lt;/p&gt;
&lt;div class="math"&gt;$$J(\textbf{W}^{[3]}, b^{[3]} \ | \ y) = -ylog(\sigma(\textbf{W}^{[3]}a^{[2]} + b^{[3]})) - (1-y)log(1 - \sigma(\textbf{W}^{[3]}a^{[2]} + b^{[3]}))$$&lt;/div&gt;
&lt;p&gt;Because we are now working with two variables, &lt;span class="math"&gt;\(\textbf{W}^{[3]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{W}^{[3]}\)&lt;/span&gt;, we use &lt;span class="math"&gt;\(\partial\)&lt;/span&gt; to represent their derivatives instead of &lt;span class="math"&gt;\(d\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;I'm sure you can appreciate how I and I imagine a lot of other people get lost in the complexity of backpropagation. To calculate the first gradient matricies for a very simple network, we are already having to calculate 4 other gradients. My hope is that by going through this simple, 3-layer example you can scale to more complex models much easier than if you hand-waved your way through backpropagation. &lt;/p&gt;
&lt;p&gt;Let's start by calculating the derivative of &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(a^{[3]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$J(a^{[3]}, y) = -ylog(a^{[3]}) - (1-y)log(1 -a^{[3]})$$&lt;/div&gt;
&lt;p&gt;We can take the derivative of both sides with respect to &lt;span class="math"&gt;\(a^{[3]}\)&lt;/span&gt; and end up with:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{dJ}{d{a^{[3]}}} = \dfrac{-y}{a^{[3]}} - \dfrac{(1-y)}{1-a^{[3]}}(-1) $$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{dJ}{d{a^{[3]}}} = \dfrac{-y}{a^{[3]}} + \dfrac{(1-y)}{1-a^{[3]}} $$&lt;/div&gt;
&lt;p&gt;Let's do the same for the derivative of &lt;span class="math"&gt;\(a^{[3]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$a^{[3]} = \sigma(z^{[3]})$$&lt;/div&gt;
&lt;div class="math"&gt;$$a^{[3]} = \dfrac{1}{1+e^{-z^{[3]}}}$$&lt;/div&gt;
&lt;div class="math"&gt;$$a^{[3]} = (1+e^{-z^{[3]}})^{-1}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{d{a^{[3]}}}{d{z^{[3]}}} = -(1+e^{-z^{[3]}})^{-2}(e^{-z^{[3]}})(-1)$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{d{a^{[3]}}}{d{z^{[3]}}} = \dfrac{e^{-z^{[3]}}}{(1+e^{-z^{[3]}})^2}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{d{a^{[3]}}}{d{z^{[3]}}} = \dfrac{1}{1+e^{-z^{[3]}}}\dfrac{1}{1+e^{-z^{[3]}}}(e^{-z^{[3]}})$$&lt;/div&gt;
&lt;p&gt;Note that:&lt;/p&gt;
&lt;div class="math"&gt;$$a^{[3]} = \dfrac{1}{1+e^{-z^{[3]}}}$$&lt;/div&gt;
&lt;div class="math"&gt;$$1+e^{-z^{[3]}} = \dfrac{1}{a^{[3]}}$$&lt;/div&gt;
&lt;div class="math"&gt;$$e^{-z^{[3]}} = \dfrac{1-a^{[3]}}{a^{[3]}}$$&lt;/div&gt;
&lt;p&gt;So back to the derivative, we can substitue things in and get:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{d{a^{[3]}}}{d{z^{[3]}}} = (a^{[3]})^2\bigg(\dfrac{1-a^{[3]}}{a^{[3]}}\bigg)$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{d{a^{[3]}}}{d{z^{[3]}}} = a^{[3]}(1-a^{[3]})$$&lt;/div&gt;
&lt;p&gt;Finally, we need to calculate the partial derivatives of &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{W}^{[3]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(b^{[3]}\)&lt;/span&gt;. Up to this point, we've been calculating the deriviatives of scalars, and this is the first time we will calculate a gradient matrix. So the first gradient matrix we need to solve for is this guy:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{z^{[3]}}}{\partial{\textbf{W}^{[3]}}} = 
\begin{bmatrix}
    \dfrac{\partial{z^{[3]}}}{\partial{W^{[3]}_{11}}} &amp;amp; \dfrac{\partial{z^{[3]}}}{\partial{W^{[3]}_{12}}}
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;We call this matrix of partial derivatives a Jacobian matrix. What is a Jacobian Matrix? Let's spend a little bit of time deconstructing that.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h4&gt;A simple Jacobian Matrix Explanation&lt;/h4&gt;
&lt;p&gt;Let's say we have a function &lt;span class="math"&gt;\(\mathcal{f}: \mathbb{R}^m \rightarrow \mathbb{R}^n\)&lt;/span&gt; that maps either a column or row vector with &lt;span class="math"&gt;\(m\)&lt;/span&gt; entries to one that has &lt;span class="math"&gt;\(n\)&lt;/span&gt; entries. Let's make the first vector a column vector called &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; with &lt;span class="math"&gt;\(m\)&lt;/span&gt; entries.&lt;/p&gt;
&lt;div class="math"&gt;$$f(\textbf{x}) = 
\begin{bmatrix}
    \\
    f_1(x_{11}, x_{21}, \dotsc, x_{m1}) \\\\
    f_2(x_{11}, x_{21}, \dotsc, x_{m1}) \\\\
    \vdots \\\\
    f_n((x_{11}, x_{21}, \dotsc, x_{m1})) \\\\
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;So &lt;span class="math"&gt;\(f(\textbf{x})\)&lt;/span&gt; is a (n, 1) column vector. It could also be a row vector, depending on how you define it.&lt;/p&gt;
&lt;p&gt;So then, the Jacobian Matrix of &lt;span class="math"&gt;\(f\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; is defined to be: &lt;/p&gt;
&lt;div class="math"&gt;$$ \dfrac{\partial\mathcal{f}}{\partial \mathbf{x}} = 
\begin{bmatrix}
\\
\dfrac{\partial\mathcal{f_1}}{\partial x_{11}} &amp;amp;
\dotsc &amp;amp;
\dfrac{\partial\mathcal{f_1}}{\partial x_{m1}} \\\\
\vdots &amp;amp; \ddots &amp;amp; \vdots \\\\
\dfrac{\partial\mathcal{f_n}}{\partial x_{11}} &amp;amp;
\dotsc &amp;amp;
\dfrac{\partial\mathcal{f_n}}{\partial x_{m1}} \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Notice that the matrix has dimensions &lt;span class="math"&gt;\((n, m)\)&lt;/span&gt;. It gets its first dimension from the number of entries in the output vector &lt;span class="math"&gt;\(\mathcal{f}(\textbf{x})\)&lt;/span&gt; and its second dimensions from the number of entries in the input vector &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt;, regardless of whether those vectors are column or row vectors. So actually, if &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; was a row vector, we would get:&lt;/p&gt;
&lt;div class="math"&gt;$$ \dfrac{\partial\mathcal{f}}{\partial\mathbf{x}} = 
\begin{bmatrix}
\\
\dfrac{\partial\mathcal{f_1}}{\partial x_{11}} &amp;amp;
\dotsc &amp;amp;
\dfrac{\partial\mathcal{f_1}}{\partial x_{1m}} \\\\
\vdots &amp;amp; \ddots &amp;amp; \vdots \\\\
\dfrac{\partial\mathcal{f_n}}{\partial x_{11}} &amp;amp;
\dotsc &amp;amp;
\dfrac{\partial\mathcal{f_n}}{\partial x_{1m}} \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;See the slight difference? This kind of tripped me out when I first started thinking about it.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h4&gt;Back to the Gradients&lt;/h4&gt;
&lt;p&gt;In order to solve for the two partial derivatives in the matrix, let's deconstruct our equation for &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt; so we use &lt;span class="math"&gt;\(W^{[3]}_{11}\)&lt;/span&gt; and &lt;span class="math"&gt;\(W^{[3]}_{12}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$z^{[3]} = \textbf{W}^{[3]}a^{[2]} + b^{[3]}$$&lt;/div&gt;
&lt;div class="math"&gt;$$z^{[3]} = 
\begin{bmatrix}
    W^{[3]}_{11} &amp;amp; W^{[3]}_{12}
\end{bmatrix}
\begin{bmatrix}
    a^{[2]}_{11} \\
    a^{[2]}_{21}
\end{bmatrix}+ b^{[3]}$$&lt;/div&gt;
&lt;div class="math"&gt;$$z^{[3]} = W^{[3]}_{11}a^{[2]}_{11} + W^{[3]}_{12}a^{[2]}_{21}+ b^{[3]}$$&lt;/div&gt;
&lt;p&gt;Now we can calculate the partial derivative of &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(W^{[3]}_{11}\)&lt;/span&gt; -&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{z^{[3]}}}{\partial{W^{[3]}_{11}}} = (1)a^{[2]}_{11} + 0 + 0$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{z^{[3]}}}{\partial{W^{[3]}_{11}}} = a^{[2]}_{11}$$&lt;/div&gt;
&lt;p&gt;And the partial derivative of &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(W^{[3]}_{12}\)&lt;/span&gt; -&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{z^{[3]}}}{\partial{W^{[3]}_{12}}} = (1)a^{[2]}_{21} + 0 + 0$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{z^{[3]}}}{\partial{W^{[3]}_{12}}} = a^{[2]}_{21}$$&lt;/div&gt;
&lt;p&gt;So interestingly, the partial gradients with respect to the elements of the weight matrix &lt;span class="math"&gt;\(\textbf{W}^{[3]}\)&lt;/span&gt; are equal to the elements of the activations vector &lt;span class="math"&gt;\(a^{[2]}\)&lt;/span&gt; that they multiply with. This is a key point going forward when we try to generalize this process computing the larger gradient matricies.&lt;/p&gt;
&lt;p&gt;Ok so now we will update our Jacobian matrix of &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{W}^{[3]}\)&lt;/span&gt; and get the following:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{z^{[3]}}}{\partial{\textbf{W}^{[3]}}} = 
\begin{bmatrix}
    a^{[2]}_{11} &amp;amp; a^{[2]}_{21}
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;And just like a good Jacobian, the partial derivative has dimensions &lt;span class="math"&gt;\((1,2)\)&lt;/span&gt;, which match the number of entries in &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt; (1 since it's a scalar value) and the number of entries in &lt;span class="math"&gt;\(\textbf{W}^{[3]}\)&lt;/span&gt; (2).&lt;/p&gt;
&lt;p&gt;Notice that &lt;span class="math"&gt;\(a^{[2]}\)&lt;/span&gt; has dimensions (2, 1), and the derivative has dimensions (1,2). On closer investigation, it's just the transpose of &lt;span class="math"&gt;\(a^{[2]}\)&lt;/span&gt;. So another way to write it would be - &lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{z^{[3]}}}{\partial{\textbf{W}^{[3]}}} = a^{[2]T}$$&lt;/div&gt;
&lt;p&gt;Great! Let's move on to the calculating the partial derivative of &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(b^{[3]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$z^{[3]} = W^{[3]}_{11}a^{[2]}_{11} + W^{[3]}_{12}a^{[2]}_{21}+ b^{[3]}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{z^{[3]}}}{\partial{b^{[3]}}} = 0 + 0 + 1$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{z^{[3]}}}{\partial{b^{[3]}}} = 1$$&lt;/div&gt;
&lt;p&gt;And it's just 1! That's nice and simple. Now let's summarize all of our results from previous calculations and combine them to solve our original problem. So recall that our original problem was to calculate the partial derivative of the cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to the weights &lt;span class="math"&gt;\(W^{[3]}\)&lt;/span&gt; and biases &lt;span class="math"&gt;\(b^{[3]}\)&lt;/span&gt; connecting the second hidden layer &lt;span class="math"&gt;\(a^{[2]}\)&lt;/span&gt; to the output layer. We were able to deconstruct the derivative using the chain rule, and the results looked like this:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{J}}{\partial{\textbf{W}^{[3]}}} = 
\dfrac{d{J}}{d{a^{[3]}}}
\dfrac{d{a^{[3]}}}{d{z^{[3]}}}
\dfrac{\partial{z^{[3]}}}{\partial{\textbf{W}^{[3]}}}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{\mathcal{L}}}{\partial{b^{[3]}}} =
\dfrac{d{\mathcal{L}}}{d{a^{[3]}}}
\dfrac{d{a^{[3]}}}{d{z^{[3]}}}
\dfrac{\partial{z^{[3]}}}{\partial{b^{[3]}}}$$&lt;/div&gt;
&lt;p&gt;We spent the last few sections calculating the 4 intermediate derivatives needed to calculate the partial derivative of the loss function &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to the weights &lt;span class="math"&gt;\(\textbf{W}^{[3]}\)&lt;/span&gt; and biases &lt;span class="math"&gt;\(b^{[3]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{d{\mathcal{L}}}{d{a^{[3]}}} = \dfrac{-y}{a^{[3]}} + \dfrac{(1-y)}{1-a^{[3]}} $$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{d{a^{[3]}}}{d{z^{[3]}}} = a^{[3]}(1-a^{[3]})$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{z^{[3]}}}{\partial{\textbf{W}^{[3]}}} = a^{[2]T}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{z^{[3]}}}{\partial{b^{[3]}}} = 1$$&lt;/div&gt;
&lt;p&gt;Let's simplify the derivative of the cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt;.
&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{dJ}{d z^{[3]}} = \dfrac{dJ}{d{a^{[3]}}}\dfrac{d{a^{[3]}}}{d{z^{[3]}}}$$&lt;/div&gt;
&lt;div class="math"&gt;$$ = \bigg(\dfrac{-y}{a^{[3]}} + \dfrac{(1-y)}{1-a^{[3]}}\bigg)a^{[3]}(1-a^{[3]})$$&lt;/div&gt;
&lt;div class="math"&gt;$$= -y(1-a^{[3]}) + (1-y)a^{[3]}$$&lt;/div&gt;
&lt;div class="math"&gt;$$= -y+ya^{[3]} + a^{[3]} -ya^{[3]}$$&lt;/div&gt;
&lt;div class="math"&gt;$$=a^{[3]} - y$$&lt;/div&gt;
&lt;p&gt;If we substitute these values in, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{J}}{\partial{\textbf{W}^{[3]}}} = (a^{[3]} - y) a^{[2]T}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{J}}{\partial{b^{[3]}}} = a^{[3]} - y$$&lt;/div&gt;
&lt;p&gt;And that's it! And luckily, the dimensions of our partial derivatives, &lt;span class="math"&gt;\((1,2)\)&lt;/span&gt; and &lt;span class="math"&gt;\(1\)&lt;/span&gt; respectively, match the gradient matricies we wanted to calculate. So we get:&lt;/p&gt;
&lt;div class="math"&gt;$$d\textbf{W}^{[3]} = (a^{[3]} - y) a^{[2]T}$$&lt;/div&gt;
&lt;div class="math"&gt;$$d b^{[3]} = a^{[3]} - y$$&lt;/div&gt;
&lt;div class="math"&gt;$$\bigg( d\textbf{W}^{[1]}, d\textbf{b}^{[1]}, d\textbf{W}^{[2]}, d\textbf{b}^{[2]}, 
d\textbf{W}^{[3]}, db^{[3]} \bigg)$$&lt;/div&gt;
&lt;p&gt;We solved the first two gradient matricies &lt;span class="math"&gt;\((d\textbf{W}^{[3]}, db^{[3]})\)&lt;/span&gt;. Our goal was to solve for 29 gradients, and we just finished 3, since the partial derivative with respect to &lt;span class="math"&gt;\(\textbf{W}^{[3]}\)&lt;/span&gt; is a &lt;span class="math"&gt;\((1, 2)\)&lt;/span&gt; matrix and therefore has two elements and the partial derivative with respect to &lt;span class="math"&gt;\(b^{[3]}\)&lt;/span&gt; has only one element.&lt;/p&gt;
&lt;p&gt;Next, we continue to move backwards and focus on calculating the derivative of the loss function &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{b}^{[2]}\)&lt;/span&gt;. Starting with breaking it up using the chain rule, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial{J}}{\partial{\textbf{W}^{[2]}}} = 
\dfrac{d{J}}{d{z^{[3]}}}
\dfrac{d{z^{[3]}}}{d{\textbf{a}^{[2]}}}
\dfrac{d{\textbf{a}^{[2]}}}{d{\textbf{z}^{[2]}}}
\dfrac{\partial{\textbf{z}^{[2]}}}{\partial{\textbf{W}^{[2]}}}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial{J}}{\partial{\textbf{b}^{[2]}}} = 
\dfrac{d{\mathcal{L}}}{d{z^{[3]}}}
\dfrac{d{z^{[3]}}}{d{\textbf{a}^{[2]}}}
\dfrac{d{\textbf{a}^{[2]}}}{d{\textbf{z}^{[2]}}}
\dfrac{\partial{\textbf{z}^{[2]}}}{\partial{\textbf{b}^{[2]}}}
$$&lt;/div&gt;
&lt;p&gt;Notice that since:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{d{J}}{d{z^{[3]}}} = 
\dfrac{d{J}}{d{a^{[3]}}}
\dfrac{d{a^{[3]}}}{d{z^{[3]}}}
$$&lt;/div&gt;
&lt;p&gt;We already know that it is equal to:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{d{J}}{d{z^{[3]}}} = a^{[3]} - y$$&lt;/div&gt;
&lt;p&gt;And don't need to calculate it again.&lt;/p&gt;
&lt;p&gt;We sometimes refer to the partial derivative of the cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt; as &lt;span class="math"&gt;\(\delta^{[3]}\)&lt;/span&gt;, and will use that notation for the rest of this blog post.&lt;/p&gt;
&lt;p&gt;Let's focus on what we need to calculate. Let's start with the derivative of &lt;span class="math"&gt;\(\textbf{z}^{[3]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt;, which we can break down into two steps:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{d{z^{[3]}}}{d{\textbf{z}^{[2]}}} = 
\dfrac{d{z^{[3]}}}{d{\textbf{a}^{[2]}}}
\dfrac{d{\textbf{a}^{[2]}}}{d{\textbf{z}^{[2]}}}
$$&lt;/div&gt;
&lt;p&gt;We will start with calculating the derivative of &lt;span class="math"&gt;\(\textbf{z}^{[3]}\)&lt;/span&gt; with respect to the activation &lt;span class="math"&gt;\(\textbf{a}^{[2]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Recall that &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt; matches the number of nodes in the output layer and is therefore a scalar value. &lt;span class="math"&gt;\(\textbf{a}^{[2]}\)&lt;/span&gt; is a &lt;span class="math"&gt;\((2,1)\)&lt;/span&gt; column vector and its number of entries match the number of nodes in the 3rd layer or the 2nd hidden layer. So the Jacobian matrix looks like:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{d{z^{[3]}}}{d{\textbf{a}^{[2]}}} = 
\begin{bmatrix}
    \dfrac{\partial{z^{[3]}}}{\partial{a^{[2]}_{11}}} &amp;amp;
    \dfrac{\partial{z^{[3]}}}{\partial{a^{[2]}_{21}}} 
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;And the equation involving &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{a}^{[2]}\)&lt;/span&gt; is:&lt;/p&gt;
&lt;div class="math"&gt;$$z^{[3]} = \textbf{W}^{[3]}\textbf{a}^{[2]} + b^{[3]}$$&lt;/div&gt;
&lt;p&gt;Let's break this equation down:&lt;/p&gt;
&lt;div class="math"&gt;$$z^{[3]} = 
W^{[3]}_{11}a^{[2]}_{11} + 
W^{[3]}_{12}a^{[2]}_{21} +
b^{[3]}
$$&lt;/div&gt;
&lt;p&gt;If we take the partial derivative of &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(a^{[2]}_{21}\)&lt;/span&gt;, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{z^{[3]}}}{\partial{a^{[2]}_{21}}} =
0 + 
W^{[3]}_{12}(1) +
0
$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{z^{[3]}}}{\partial{a^{[2]}_{21}}} =
W^{[3]}_{12}
$$&lt;/div&gt;
&lt;p&gt;Applying this logic to every partial derivative in the vector, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{d{z^{[3]}}}{d{\textbf{a}^{[2]}}} = 
\begin{bmatrix}
\\
W^{[3]}_{11} &amp;amp; 
W^{[3]}_{12}
\\\\
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;Next, we need to calculate the derivative of &lt;span class="math"&gt;\(\textbf{a}^{[2]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;Both &lt;span class="math"&gt;\(\textbf{a}^{[2]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; are column vectors with dimensions &lt;span class="math"&gt;\((2,1)\)&lt;/span&gt; and therefore the Jacobian matrix of &lt;span class="math"&gt;\(\textbf{a}^{[2]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; will be: &lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{d{\textbf{a}^{[2]}}}{d{\textbf{z}^{[2]}}} = 
\begin{bmatrix}
\\
    \dfrac{\partial{a^{[3]}_{11}}}{\partial{z^{[2]}_{11}}} &amp;amp;
    \dfrac{\partial{a^{[3]}_{11}}}{\partial{z^{[2]}_{21}}} \\\\
    \dfrac{\partial{a^{[3]}_{21}}}{\partial{z^{[2]}_{11}}} &amp;amp;
    \dfrac{\partial{a^{[3]}_{21}}}{\partial{z^{[2]}_{21}}} \\\\
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;Notice that in order to go from &lt;span class="math"&gt;\(\textbf{z}^{[2]} \rightarrow \textbf{a}^{[2]}\)&lt;/span&gt;, we apply the ReLU function &lt;span class="math"&gt;\(g(z)\)&lt;/span&gt; to each element in &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt;. So for example, to calculate &lt;span class="math"&gt;\(a^{[2]}_{21}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$a^{[2]}_{21} = g(z^{[2]}_{21})$$&lt;/div&gt;
&lt;p&gt;Applying this logic to every partial derivative in the vector, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{d{\textbf{a}^{[2]}}}{d{\textbf{z}^{[2]}}} = 
\begin{bmatrix}
\\
    g'(z^{[2]}_{11}) &amp;amp; 0 \\\\ 
    0 &amp;amp; g'(z^{[2]}_{21})
\\\\
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;Where &lt;span class="math"&gt;\(g'(z)\)&lt;/span&gt; is the derivative of the ReLU function &lt;span class="math"&gt;\(g(z)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;What is the derivative of ReLU equal to?&lt;/p&gt;
&lt;div class="math"&gt;$$
g'(z) = \begin{cases}
   1 &amp;amp;\text{if } z &amp;gt; 0  \\
   \text{Undefined} &amp;amp;\text{if } z = 0  \\
   0 &amp;amp;\text{if } z &amp;lt; 0
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;Why is the derivative of &lt;span class="math"&gt;\(g\)&lt;/span&gt; undefined when &lt;span class="math"&gt;\(z = 0\)&lt;/span&gt;? For a function  to be differentiable at a point, it has to be continuous at that point. In order for a point to be continuous at a point, the limit of the function as it approaches that point has to defined. In order for the limit to be defined, the left and right hand limits have to equal. In this case, the right hand limit is equal to 1 and the left hand limit is equal to 0. Therefore, ReLU is not differentiable at 0.&lt;/p&gt;
&lt;p&gt;Does it matter that the derivative of the ReLU function is undefined at &lt;span class="math"&gt;\(z=0\)&lt;/span&gt; for backpropagation? In practice, no since &lt;span class="math"&gt;\(z\)&lt;/span&gt; will never be truly equal to &lt;span class="math"&gt;\(0\)&lt;/span&gt; - software implementations will have a rounding error for float points.&lt;/p&gt;
&lt;p&gt;Finally, we will need to calculate the partial derivatives of &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{b}^{[2]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let's first focus on the partial derivative of &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt;. &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; is a one-dimensional array &lt;span class="math"&gt;\((2,1)\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt; is a two-dimensional matrix &lt;span class="math"&gt;\((2,4)\)&lt;/span&gt;. Notice that each entry in &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; can be though of as the result of a function involving all 8 of the weight entries in &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt;. So for example, &lt;span class="math"&gt;\(z^{[2]}_{11}\)&lt;/span&gt; can be though of as:&lt;/p&gt;
&lt;div class="math"&gt;$$z^{[2]}_{11} = f_1(W^{[2]}_{11}, \dotsc, W^{[2]}_{14},W^{[2]}_{21},\dotsc W^{[2]}_{24} | \textbf{a}^{[2]}, \textbf{b}^{[2]})$$&lt;/div&gt;
&lt;div class="math"&gt;$$z^{[2]}_{11} = W^{[2]}_{11}a^{[2]}_{11} + \dotsc + W^{[2]}_{14}a^{[2]}_{41} +  b^{[2]}_{11}$$&lt;/div&gt;
&lt;p&gt;If we represented the entries in &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; in this way, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$\textbf{z}^{[2]} = 
\begin{bmatrix}
f_1(W^{[2]}_{11}, W^{[2]}_{12}, W^{[2]}_{13},W^{[2]}_{14},W^{[2]}_{21},\dotsc W^{[2]}_{24} | \textbf{a}^{[2]}, \textbf{b}^{[2]}) \\\\
f_1(W^{[2]}_{11}, W^{[2]}_{12}, W^{[2]}_{13},W^{[2]}_{14},W^{[2]}_{21},\dotsc W^{[2]}_{24} | \textbf{a}^{[2]}, \textbf{b}^{[2]})
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;And the Jacobian matrix would look like:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial \textbf{z}^{[2]}}{\partial \textbf{W}^{[2]}} = 
\begin{bmatrix}
\dfrac{\partial z^{[2]}_{11}}{\partial W^{[2]}_{11}} &amp;amp;
\dotsc &amp;amp; 
\dfrac{\partial z^{[2]}_{11}}{\partial W^{[2]}_{14}} &amp;amp;
\dfrac{\partial z^{[2]}_{11}}{\partial W^{[2]}_{21}} &amp;amp;
\dotsc &amp;amp;
\dfrac{\partial z^{[2]}_{11}}{\partial W^{[2]}_{24}} \\\\
\dfrac{\partial z^{[2]}_{21}}{\partial W^{[2]}_{11}} &amp;amp;
\dotsc &amp;amp; 
\dfrac{\partial z^{[2]}_{21}}{\partial W^{[2]}_{14}} &amp;amp;
\dfrac{\partial z^{[2]}_{21}}{\partial W^{[2]}_{21}} &amp;amp;
\dotsc &amp;amp;
\dfrac{\partial z^{[2]}_{21}}{\partial W^{[2]}_{24}} \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;So the Jacobian matrix has dimensions &lt;span class="math"&gt;\((2, 8)\)&lt;/span&gt;, which is equal to the number of entries in &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; and the number of entries in &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;But do we need to calculate all 16 derivatives? Luckily, no. The reason is that most of the derivatives will be equal to &lt;span class="math"&gt;\(0\)&lt;/span&gt;, and will stay &lt;span class="math"&gt;\(0\)&lt;/span&gt; regardless if we change the values of any of the variables.&lt;/p&gt;
&lt;p&gt;In order to illustrate this point, let's look at the first element of &lt;span class="math"&gt;\(z^{[2]}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$z^{[2]}_{11} = W^{[2]}_{11}a^{[1]}_{11} + W^{[2]}_{12}a^{[1]}_{21} + W^{[2]}_{13}a^{[1]}_{31} + W^{[2]}_{14}a^{[1]}_{41} $$&lt;/div&gt;
&lt;p&gt;Notice that in order to calculate the partial derivative of &lt;span class="math"&gt;\(z^{[2]}_1\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(W^{[2]}\)&lt;/span&gt;, we only need to worry about &lt;span class="math"&gt;\(W^{[2]}_{11}\)&lt;/span&gt;, &lt;span class="math"&gt;\(W^{[2]}_{12}\)&lt;/span&gt;, &lt;span class="math"&gt;\(W^{[2]}_{13}\)&lt;/span&gt;, and &lt;span class="math"&gt;\(W^{[2]}_{14}\)&lt;/span&gt; and not the other 4 scalar variables in &lt;span class="math"&gt;\(W^{[2]}\)&lt;/span&gt;. As a general rule of thumb, the partial derivative of a scalar element in vector &lt;span class="math"&gt;\(a\)&lt;/span&gt; with respect to a scalar element in matrix &lt;span class="math"&gt;\(W\)&lt;/span&gt; will be nonzero when the x-dimension of &lt;span class="math"&gt;\(W\)&lt;/span&gt; matches the dimension of &lt;span class="math"&gt;\(a\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;And from a previous calculation, we know that the derivative of a scalar component of &lt;span class="math"&gt;\(z\)&lt;/span&gt; with respect to a scalar component of &lt;span class="math"&gt;\(W\)&lt;/span&gt; is just that scalar component of &lt;span class="math"&gt;\(W\)&lt;/span&gt; if it's included in the calculation of the scalar component of &lt;span class="math"&gt;\(z\)&lt;/span&gt;, so we get this fun matrix:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial \textbf{z}^{[2]}}{\partial \textbf{W}^{[2]}} = 
\begin{bmatrix}
a^{[2]}_{11} &amp;amp;
\dotsc &amp;amp; 
a^{[2]}_{41} &amp;amp;
0 &amp;amp;
\dotsc &amp;amp;
0 \\\\
0 &amp;amp;
\dotsc &amp;amp; 
0 &amp;amp;
a^{[2]}_{11} &amp;amp; 
\dotsc &amp;amp;
a^{[2]}_{41} &amp;amp; \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Now, let's calculate the Jacobian matrix of &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{b}^{[2]}\)&lt;/span&gt;. It's pretty simple, just like last time the derivatives that involve the entry of &lt;span class="math"&gt;\(\textbf{b}^{[2]}\)&lt;/span&gt; are equal to 1.&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial \textbf{z}^{[2]}}{\partial \textbf{b}^{[2]}} = 
\begin{bmatrix}
    1 &amp;amp; 0 \\\\
    0 &amp;amp; 1 
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;The dimensions &lt;span class="math"&gt;\((2,2)\)&lt;/span&gt; of the Jacobian matrix are equal to the number of entries in &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{b}^{[2]}\)&lt;/span&gt;, respectively.&lt;/p&gt;
&lt;p&gt;We have all the pieces to finally calculate the derivative of the cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\mathbf{W}^{[2]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\mathbf{b}^{[2]}\)&lt;/span&gt;. &lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial{J}}{\partial{\mathbf{W}^{[2]}}} = 
\delta^{[3]}
\dfrac{d{z^{[3]}}}{d{\mathbf{z}^{[2]}}}
\dfrac{\partial{\mathbf{z}^{[2]}}}{\partial{\mathbf{W}^{[2]}}}
$$&lt;/div&gt;
&lt;p&gt;And this is how we deconstructed the derivative of the loss function &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(b^{[2]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial{J}}{\partial{\mathbf{b}^{[2]}}} = 
\delta^{[3]}
\dfrac{d{z^{[3]}}}{d{\mathbf{z}^{[2]}}}
\dfrac{\partial{\mathbf{z}^{[2]}}}{\partial{\mathbf{b}^{[2]}}}
$$&lt;/div&gt;
&lt;p&gt;To summarize our calculations and the dimensions of each:&lt;/p&gt;
&lt;div class="math"&gt;$$
\delta^{[3]} = 
a^{[3]} - y
$$&lt;/div&gt;
&lt;p&gt;
Is a scalar value.&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{d{z^{[3]}}}{d{\textbf{a}^{[2]}}} = 
\begin{bmatrix}
\\
    W^{[3]}_{11} &amp;amp; 
    W^{[3]}_{12}
\\\\
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;Is a &lt;span class="math"&gt;\((1,2)\)&lt;/span&gt; matrix.&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{d{\textbf{a}^{[2]}}}{d{\textbf{z}^{[2]}}} = 
\begin{bmatrix}
\\
    g'(z^{[2]}_{11}) &amp;amp; 0 \\\\ 
    0 &amp;amp; g'(z^{[2]}_{21})
\\\\
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;Is a &lt;span class="math"&gt;\((2, 2)\)&lt;/span&gt; matrix.&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial \textbf{z}^{[2]}}{\partial \textbf{W}^{[2]}} = 
\begin{bmatrix}
a^{[2]}_{11} &amp;amp;
\dotsc &amp;amp; 
a^{[2]}_{41} &amp;amp;
0 &amp;amp;
\dotsc &amp;amp;
0 \\\\
0 &amp;amp;
\dotsc &amp;amp; 
0 &amp;amp;
a^{[2]}_{11} &amp;amp; 
\dotsc &amp;amp;
a^{[2]}_{41} &amp;amp; \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Is a &lt;span class="math"&gt;\((2,8)\)&lt;/span&gt; matrix.&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial \textbf{z}^{[2]}}{\partial \textbf{b}^{[2]}} = 
\begin{bmatrix}
    1 &amp;amp; 0 \\\\
    0 &amp;amp; 1 
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;Is a &lt;span class="math"&gt;\((2,2)\)&lt;/span&gt; matrix.&lt;/p&gt;
&lt;p&gt;Let's first try and calculate the derivative of &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt;. We can start by looking at:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{J}}{\partial{\mathbf{z}^{[2]}}} = 
\delta^{[3]}
\dfrac{d{z^{[3]}}}{d{\mathbf{a}^{[2]}}}
\dfrac{d{a^{[3]}}}{d{\mathbf{z}^{[2]}}}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{J}}{\partial{\mathbf{z}^{[2]}}} = \delta^{[3]}
\begin{bmatrix}
W^{[3]}_{11} &amp;amp; 
W^{[3]}_{12}
\end{bmatrix}
\begin{bmatrix}
g'(z^{[2]}_{11}) &amp;amp; 0 \\\\ 
0 &amp;amp; g'(z^{[2]}_{21})
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;So this is the Jacobian Matrix of &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt;. But notice that this matrix is &lt;span class="math"&gt;\((1,2)\)&lt;/span&gt; but &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; is &lt;span class="math"&gt;\((2,1)\)&lt;/span&gt;. We can define a gradient matrix similar to what we did for the weights and bias parameters that match the dimensions of &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; called &lt;span class="math"&gt;\(\boldsymbol{\delta}^{[2]}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{\delta}^{[2]} = 
\bigg( \dfrac{\partial{J}}{\partial{\mathbf{z}^{[2]}}}\bigg)^T
$$&lt;/div&gt;
&lt;p&gt;Recall that given three matricies &lt;span class="math"&gt;\(\textbf{A}\)&lt;/span&gt;, &lt;span class="math"&gt;\(\textbf{B}\)&lt;/span&gt;, and &lt;span class="math"&gt;\(\textbf{d}\)&lt;/span&gt; the &lt;span class="math"&gt;\((\textbf{ABC})^T = \textbf{C}^T\textbf{B}^T\textbf{A}^T\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{\delta}^{[2]} = 
\begin{bmatrix}
g'(z^{[2]}_{11}) &amp;amp; 0 \\\\ 
0 &amp;amp; g'(z^{[2]}_{21})
\end{bmatrix}
\begin{bmatrix}
W^{[3]}_{11} \\\\ 
W^{[3]}_{12}
\end{bmatrix}
\delta^{[3]}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\boldsymbol{\delta}^{[2]} = 
\begin{bmatrix}
g'(z^{[2]}_{11})W^{[3]}_{11}\delta^{[3]} \\\\ 
g'(z^{[2]}_{21})W^{[3]}_{12}\delta^{[3]}
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Notice when we rearranged the products in the matrix, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{\delta}^{[2]} = 
\begin{bmatrix}
W^{[3]}_{11}\delta^{[3]}g'(z^{[2]}_{11}) \\\\ 
W^{[3]}_{12}\delta^{[3]}g'(z^{[2]}_{21})
\end{bmatrix}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\boldsymbol{\delta}^{[2]} = 
\begin{bmatrix}
W^{[3]}_{11}\delta^{[3]} \\\\ 
W^{[3]}_{12}\delta^{[3]}
\end{bmatrix}
*
g'(\textbf{z}^{[2]})
$$&lt;/div&gt;
&lt;p&gt;Where &lt;span class="math"&gt;\(*\)&lt;/span&gt; indicates elementwise multiplication between two matricies. &lt;span class="math"&gt;\(g'(\textbf{z}^{[2]})\)&lt;/span&gt; is a columnwise vector of the derivative of ReLU &lt;span class="math"&gt;\(g'(z)\)&lt;/span&gt; applied to each entry of &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Next, we can decompose the result into:&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{\delta}^{[2]}= 
\begin{bmatrix}
W^{[3]}_{11} \\\\ 
W^{[3]}_{12}
\end{bmatrix}
\delta^{[3]}
*
g'(\textbf{z}^{[2]})
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\boldsymbol{\delta}^{[2]}= 
\textbf{W}^{[3]T}
\delta^{[3]}
*
g'(\textbf{z}^{[2]})
$$&lt;/div&gt;
&lt;p&gt;This final result the gradient of the cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt;. It is a &lt;span class="math"&gt;\((2,1)\)&lt;/span&gt; column vector with dimensions that match &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Plugging that result into our equation we get:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial{J}}{\partial{\mathbf{W}^{[2]}}} = 
\dfrac{\partial{J}}{\partial{\mathbf{z}^{[2]}}}
\dfrac{\partial{\mathbf{z}^{[2]}}}{\partial{\mathbf{W}^{[2]}}}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial{J}}{\partial{\mathbf{W}^{[2]}}} = 
\boldsymbol{\delta}^{[2]T}
\begin{bmatrix}
a^{[2]}_{11} &amp;amp;
\dotsc &amp;amp; 
a^{[2]}_{41} &amp;amp;
0 &amp;amp;
\dotsc &amp;amp;
0 \\\\
0 &amp;amp;
\dotsc &amp;amp; 
0 &amp;amp;
a^{[2]}_{11} &amp;amp; 
\dotsc &amp;amp;
a^{[2]}_{41} &amp;amp; \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
 = 
\begin{bmatrix}
\delta^{[2]}_{11} &amp;amp;
\delta^{[2]}_{21}
\end{bmatrix}
\begin{bmatrix}
a^{[2]}_{11} &amp;amp;
\dotsc &amp;amp; 
a^{[2]}_{41} &amp;amp;
0 &amp;amp;
\dotsc &amp;amp;
0 \\\\
0 &amp;amp;
\dotsc &amp;amp; 
0 &amp;amp;
a^{[2]}_{11} &amp;amp; 
\dotsc &amp;amp;
a^{[2]}_{41} &amp;amp; \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;And we get this fun &lt;span class="math"&gt;\((1, 8)\)&lt;/span&gt; Jacobian matrix:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{J}}{\partial{\textbf{W}^{[2]}}} = 
\begin{bmatrix}
\delta^{[2]}_{11}a^{[2]}_{11} &amp;amp; 
\delta^{[2]}_{11}a^{[2]}_{21} &amp;amp;
\delta^{[2]}_{11}a^{[2]}_{31} &amp;amp; 
\delta^{[2]}_{11}a^{[2]}_{41} &amp;amp;
\delta^{[2]}_{21}a^{[2]}_{11} &amp;amp; 
\delta^{[2]}_{21}a^{[2]}_{21} &amp;amp;
\delta^{[2]}_{21}a^{[2]}_{31} &amp;amp; 
\delta^{[2]}_{21}a^{[2]}_{41}
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;So this is our Jacobian. But we need our gradient matrix &lt;span class="math"&gt;\(d\textbf{W}^{[2]}\)&lt;/span&gt; to have dimensions that match &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt;. So we will reshape the Jacobian into a &lt;span class="math"&gt;\((2,4)\)&lt;/span&gt; matrix.&lt;/p&gt;
&lt;div class="math"&gt;$$d\textbf{W}^{[2]} = 
\begin{bmatrix}
\delta^{[2]}_{11}a^{[2]}_{11} &amp;amp; 
\dotsc &amp;amp;
\delta^{[2]}_{11}a^{[2]}_{41} \\\\
\delta^{[2]}_{21}a^{[2]}_{11} &amp;amp; 
\dotsc &amp;amp; 
\delta^{[2]}_{21}a^{[2]}_{41} \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;And, interestingly, we can break this apart into two matricies.&lt;/p&gt;
&lt;div class="math"&gt;$$d\textbf{W}^{[2]} = 
\begin{bmatrix}
\delta^{[2]}_{11} \\\\
\delta^{[2]}_{21} \\\\
\end{bmatrix}
\begin{bmatrix}
a^{[2]}_{11} &amp;amp; a^{[2]}_{21} &amp;amp;  a^{[2]}_{31} &amp;amp; a^{[2]}_{41} \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Which becomes:&lt;/p&gt;
&lt;div class="math"&gt;$$d\textbf{W}^{[2]} = \boldsymbol{\delta}^{[2]}\textbf{a}^{[2]T}$$&lt;/div&gt;
&lt;p&gt;Alright, so you might be wondering, why did we need to go through all that work to arrive at that simple result? The reason is because most tutorials, blog posts, and courses skip the math and arrive at this result. But I think it's important to work through how we arrive there step by step. When we go through it step by step, we begin to understand how each of these operations relates to linear algebra and multivariate calculus. When we are just presented with the final result, we tend to just memorize it.&lt;/p&gt;
&lt;p&gt;Let's now calculate the partial derivative of the loss &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{b}^{[2]}\)&lt;/span&gt;, which luckily is a lot easier.&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial J}{\partial{\textbf{b}^{[2]}}} = \dfrac{dJ}{d{\textbf{z}^{[2]}}}
\dfrac{\partial{\textbf{z}^{[2]}}}{\partial{\textbf{b}^{[2]}}}$$&lt;/div&gt;
&lt;p&gt;We know that:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial \textbf{z}^{[2]}}{\partial \textbf{b}^{[2]}} = 
\begin{bmatrix}
    1 &amp;amp; 0 \\\\
    0 &amp;amp; 1 
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;Which is just the identity matrix. So the partial derivative just simplfies to become:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{J}}{\partial{\textbf{b}^{[2]}}} = \boldsymbol{\delta}^{[2]T}$$&lt;/div&gt;
&lt;p&gt;And the gradient &lt;span class="math"&gt;\(d\textbf{b}^{[2]}\)&lt;/span&gt; is therefore just the transpose of the Jacobian, or:&lt;/p&gt;
&lt;div class="math"&gt;$$
d\textbf{b}^{[2]} = \boldsymbol{\delta}^{[2]}
$$&lt;/div&gt;
&lt;p&gt;Which is a &lt;span class="math"&gt;\((2,1)\)&lt;/span&gt; column vector, and matches the dimensions of &lt;span class="math"&gt;\(\textbf{b}^{[2]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Great! So we calculated two more gradient matricies (four in total), and we still have two more to go.&lt;/p&gt;
&lt;div class="math"&gt;$$\bigg( d\textbf{W}^{[1]}, d\textbf{b}^{[1]}, d\textbf{W}^{[2]}, d\textbf{b}^{[2]}, 
d\textbf{W}^{[3]}, db^{[3]} \bigg)$$&lt;/div&gt;
&lt;p&gt;We also said that by breaking down all the gradient matricies into their respective partial derivatives, we needed to solve for 29. We solved for &lt;span class="math"&gt;\(3\)&lt;/span&gt; in layer 3, and &lt;span class="math"&gt;\(8 + 2 = 10\)&lt;/span&gt; in the second layer. 16 more to go!&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;Calculating &lt;span class="math"&gt;\(d\textbf{W}^{[1]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(d\textbf{b}^{[1]}\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;So by this point you should know the drill. In order to calculate our gradient matricies, we need to calculate the Jacobian Matricies of &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{W}^{[1]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{b}^{[1]}\)&lt;/span&gt; &lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial{J}}{\partial{\textbf{W}^{[1]}}} = 
\dfrac{d{J}}{d{\textbf{z}^{[2]}}}
\dfrac{d{\textbf{z}^{[2]}}}{d{\textbf{a}^{[1]}}}
\dfrac{d\textbf{a}^{[1]}}{d{\textbf{z}^{[1]}}}
\dfrac{\partial{\textbf{z}^{[1]}}}{\partial{\textbf{W}^{[1]}}}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial{J}}{\partial{\textbf{b}^{[1]}}} = 
\dfrac{d{J}}{d{\textbf{z}^{[2]}}}
\dfrac{d{\textbf{z}^{[2]}}}{d{\textbf{a}^{[1]}}}
\dfrac{d{\textbf{a}^{[1]}}}{d{\textbf{a}^{[1]}}}
\dfrac{\partial{\textbf{z}^{[1]}}}{\partial{\textbf{b}^{[1]}}}
$$&lt;/div&gt;
&lt;p&gt;We already know that:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{d{J}}{d{\mathbf{z}^{[2]}}} = \delta^{[2]T}$$&lt;/div&gt;
&lt;p&gt;Which is a &lt;span class="math"&gt;\((1,2)\)&lt;/span&gt; matrix.&lt;/p&gt;
&lt;p&gt;We are going to lump a couple of derivatives together.&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{d{\textbf{z}^{[2]}}}{d{\textbf{z}^{[1]}}} =
\dfrac{d{\textbf{z}^{[2]}}}{d{\textbf{a}^{[1]}}}
\dfrac{d\textbf{a}^{[1]}}{d{\textbf{z}^{[1]}}}
$$&lt;/div&gt;
&lt;p&gt;And just solve for the Jacobian matrix of &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt;. &lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{d{\textbf{z}^{[2]}}}{d{\textbf{z}^{[1]}}} = 
\begin{bmatrix}
\\
    \dfrac{\partial{z^{[2]}_{11}}}{\partial{z^{[1]}_{11}}} &amp;amp; 
    \dfrac{\partial{z^{[2]}_{11}}}{\partial{z^{[1]}_{21}}} &amp;amp;
    \dfrac{\partial{z^{[2]}_{11}}}{\partial{z^{[1]}_{31}}} &amp;amp;
    \dfrac{\partial{z^{[2]}_{11}}}{\partial{z^{[1]}_{41}}} \\\\
    \dfrac{\partial{z^{[2]}_{21}}}{\partial{z^{[1]}_{11}}} &amp;amp; 
    \dfrac{\partial{z^{[2]}_{21}}}{\partial{z^{[1]}_{21}}} &amp;amp;
    \dfrac{\partial{z^{[2]}_{21}}}{\partial{z^{[1]}_{31}}} &amp;amp;
    \dfrac{\partial{z^{[2]}_{21}}}{\partial{z^{[1]}_{41}}} \\\\
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;With dimensions &lt;span class="math"&gt;\((2,4)\)&lt;/span&gt;. Again, the first dimension matches the number of entries in &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; and the second dimension mathces the number of entries in &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt;  Similar to the previous layer, when we plug in values we get this matrix:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{d{\textbf{z}^{[2]}}}{d{\textbf{z}^{[1]}}} = 
\begin{bmatrix}\\
    W^{[2]}_{11} &amp;amp; 
    \dotsc &amp;amp;
    W^{[2]}_{14}\\\\
    W^{[2]}_{21} &amp;amp; 
    \dotsc &amp;amp;
    W^{[2]}_{24} \\\\
\end{bmatrix}
\begin{bmatrix}\\
    g'(z^{[1]}_{11}) &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\\
    0 &amp;amp; g'(z^{[1]}_{21}) &amp;amp; 0 &amp;amp; 0 \\\\
    0 &amp;amp; 0 &amp;amp; g'(z^{[1]}_{31}) &amp;amp; 0 \\\\
    0 &amp;amp; 0 &amp;amp; 0 &amp;amp;g'(z^{[1]}_{41}) \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Alright, now we need to calculate the partial derivative of &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{W}^{[1]}\)&lt;/span&gt;. Like last time, we can construct the Jacobian matrix:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial \textbf{z}^{[1]}}{\partial \textbf{W}^{[1]}} = 
\begin{bmatrix}
\dfrac{\partial z^{[1]}_{11}}{\partial W^{[1]}_{11}} &amp;amp;
\dotsc &amp;amp; 
\dfrac{\partial z^{[1]}_{11}}{\partial W^{[1]}_{43}} \\\\
\dfrac{\partial z^{[1]}_{21}}{\partial W^{[1]}_{11}} &amp;amp;
\dotsc &amp;amp; 
\dfrac{\partial z^{[1]}_{21}}{\partial W^{[1]}_{43}} \\\\
\dfrac{\partial z^{[1]}_{31}}{\partial W^{[1]}_{11}} &amp;amp;
\dotsc &amp;amp; 
\dfrac{\partial z^{[1]}_{31}}{\partial W^{[1]}_{43}} \\\\
\dfrac{\partial z^{[1]}_{41}}{\partial W^{[1]}_{11}} &amp;amp;
\dotsc &amp;amp; 
\dfrac{\partial z^{[1]}_{41}}{\partial W^{[1]}_{43}} \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;So the Jacobian matrix has dimensions &lt;span class="math"&gt;\((4, 12)\)&lt;/span&gt;, which is equal to the number of entries in &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt; and the number of entries in &lt;span class="math"&gt;\(\textbf{W}^{[1]}\)&lt;/span&gt; &lt;span class="math"&gt;\((4*3 = 12)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;And similar to our previous calculation, it simplifies to become:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial \textbf{z}^{[1]}}{\partial \textbf{W}^{[1]}} = 
\begin{bmatrix}
x_{11} &amp;amp;
x_{21} &amp;amp; 
x_{31} &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 \\\\
0 &amp;amp;
0 &amp;amp; 
0 &amp;amp;
x_{11} &amp;amp;
x_{21} &amp;amp; 
x_{31} &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 \\\\
0 &amp;amp;
0 &amp;amp; 
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
x_{11} &amp;amp;
x_{21} &amp;amp; 
x_{31} &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 \\\\
0 &amp;amp;
0 &amp;amp; 
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
x_{11} &amp;amp;
x_{21} &amp;amp; 
x_{31} &amp;amp; \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Like last time, the Jacobian matrix of &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{b}^{[1]}\)&lt;/span&gt; is equal to the identity matrix:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial \textbf{z}^{[1]}}{\partial \textbf{b}^{[1]}} = 
\begin{bmatrix}
\\
    1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\\
    0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \\\\
    0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 \\\\
    0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 \\\\
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;The dimensions &lt;span class="math"&gt;\((4,4)\)&lt;/span&gt; of the Jacobian matrix are equal to the number of entries in &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{b}^{[1]}\)&lt;/span&gt;, respectively.&lt;/p&gt;
&lt;p&gt;Now we have the intermediate pieces to calculate the partial derivative of &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{W}^{[1]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{b}^{[1]}\)&lt;/span&gt;, which will eventually allow us to calculate the gradient matrix.&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial{J}}{\partial{\textbf{W}^{[1]}}} = 
\dfrac{d{J}}{d{\textbf{z}^{[2]}}}
\dfrac{d{\textbf{z}^{[2]}}}{d{\textbf{z}^{[1]}}}
\dfrac{\partial{\textbf{z}^{[1]}}}{\partial{\textbf{W}^{[1]}}}
$$&lt;/div&gt;
&lt;h3&gt;&lt;/h3&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial{J}}{\partial{\textbf{b}^{[1]}}} = 
\dfrac{d{J}}{d{\textbf{z}^{[2]}}}
\dfrac{d{\textbf{z}^{[2]}}}{d{\textbf{z}^{[1]}}}
\dfrac{\partial{\textbf{z}^{[1]}}}{\partial{\textbf{b}^{[1]}}}
$$&lt;/div&gt;
&lt;p&gt;Let's start by calculating the derivative of &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{d{J}}{d{\textbf{z}^{[1]}}} = 
\dfrac{d{J}}{d{\textbf{z}^{[2]}}}
\dfrac{d{\textbf{z}^{[2]}}}{d{\textbf{z}^{[1]}}}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\dfrac{d{J}}{d{\mathbf{z}^{[2]}}} = 
\boldsymbol{\delta}^{[2]T}
\dfrac{d{\textbf{z}^{[2]}}}{d{\textbf{z}^{[1]}}}$$&lt;/div&gt;
&lt;p&gt;So this is the Jacobian Matrix of &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt;. Let's figure out the dimensions for this matrix based on the dimensions of its components. &lt;span class="math"&gt;\(\boldsymbol{\delta}^{[2]T}\)&lt;/span&gt; has dimensions (1, 2), and the derivative of &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt; has dimensions &lt;span class="math"&gt;\((2,4)\)&lt;/span&gt;. Doing the matrix multiply, we get a matrix of dimensions &lt;span class="math"&gt;\((1,4)\)&lt;/span&gt;, which matches what we would expect. &lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{dJ}{d{\textbf{z}^{[1]}}} = 
\boldsymbol{\delta}^{[2]T}
\begin{bmatrix}\\
    W^{[2]}_{11} &amp;amp; 
    \dotsc &amp;amp;
    W^{[2]}_{14} \\\\
    W^{[2]}_{21} &amp;amp; 
    \dotsc &amp;amp;
    W^{[2]}_{24} \\\\
\end{bmatrix}
\begin{bmatrix}\\
    g'(z^{[1]}_{11}) &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\\
    0 &amp;amp; g'(z^{[1]}_{21}) &amp;amp; 0 &amp;amp; 0 \\\\
    0 &amp;amp; 0 &amp;amp; g'(z^{[1]}_{31}) &amp;amp; 0 \\\\
    0 &amp;amp; 0 &amp;amp; 0 &amp;amp;g'(z^{[1]}_{41}) \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;To calculate the gradient of the cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt;, we need to take the transpose of the Jacobian matrix.&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{\delta}^{[1]} = 
\begin{bmatrix}\\
    g'(z^{[1]}_{11}) &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\\
    0 &amp;amp; g'(z^{[1]}_{21}) &amp;amp; 0 &amp;amp; 0 \\\\
    0 &amp;amp; 0 &amp;amp; g'(z^{[1]}_{31}) &amp;amp; 0 \\\\
    0 &amp;amp; 0 &amp;amp; 0 &amp;amp;g'(z^{[1]}_{41}) \\\\
\end{bmatrix}
\begin{bmatrix}\\
    W^{[2]}_{11} &amp;amp; 
    W^{[2]}_{21} \\\\
    \vdots &amp;amp; \vdots \\\\
    W^{[2]}_{14} &amp;amp; 
    W^{[2]}_{24} \\\\
\end{bmatrix}
\begin{bmatrix}\\
    \delta^{[1]}_{11} \\\\
    \delta^{[1]}_{21} \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
 = 
\begin{bmatrix}\\
    g'(z^{[1]}_{11})W^{[2]}_{11} &amp;amp; 
    g'(z^{[1]}_{11})W^{[2]}_{21} \\\\
    \vdots &amp;amp; \vdots \\\\
    g'(z^{[1]}_{41})W^{[2]}_{14} &amp;amp; 
    g'(z^{[1]}_{41})W^{[2]}_{24} \\\\
\end{bmatrix}
\begin{bmatrix}\\
    \delta^{[1]}_{11} \\\\
    \delta^{[1]}_{21} \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
 = 
\begin{bmatrix}\\
    g'(z^{[1]}_{11})W^{[2]}_{11}\delta^{[1]}_{11} + 
    g'(z^{[1]}_{11})W^{[2]}_{21}\delta^{[1]}_{21} \\\\
    \vdots &amp;amp; \\\\
    g'(z^{[1]}_{41})W^{[2]}_{14}\delta^{[1]}_{11} + 
    g'(z^{[1]}_{41})W^{[2]}_{24}\delta^{[1]}_{21} \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
 = 
\begin{bmatrix}\\
    g'(z^{[1]}_{11})(W^{[2]}_{11}\delta^{[1]}_{11} + 
    W^{[2]}_{21}\delta^{[1]}_{21}) \\\\
    \vdots &amp;amp; \\\\
    g'(z^{[1]}_{41})(W^{[2]}_{14}\delta^{[1]}_{11} + 
    W^{[2]}_{24}\delta^{[1]}_{21}) \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
 = 
\begin{bmatrix}\\
    W^{[2]}_{11}\delta^{[1]}_{11} + 
    W^{[2]}_{21}\delta^{[1]}_{12} \\\\
    \vdots &amp;amp; \\\\
    W^{[2]}_{14}\delta^{[1]}_{11} + 
    W^{[2]}_{24}\delta^{[1]}_{12} \\\\
\end{bmatrix}
\begin{bmatrix}\\
    g'(z^{[1]}_{11}) \\\\
    \vdots &amp;amp; \\\\
    g'(z^{[1]}_{41}) \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
 = 
\begin{bmatrix}\\
    W^{[2]}_{11}\delta^{[1]}_{11} + 
    W^{[2]}_{21}\delta^{[1]}_{12} \\\\
    \vdots &amp;amp; \\\\
    W^{[2]}_{14}\delta^{[1]}_{11} + 
    W^{[2]}_{24}\delta^{[1]}_{12} \\\\
\end{bmatrix}
* g'(\textbf{z}^{[1]})
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
 = 
\begin{bmatrix}\\
    W^{[2]}_{11} &amp;amp; 
    W^{[2]}_{21} \\\\
    \vdots &amp;amp; \vdots \\\\
    W^{[2]}_{14} &amp;amp; 
    W^{[2]}_{24} \\\\
\end{bmatrix}
\begin{bmatrix}\\
    \delta^{[1]}_{11} \\\\
    \delta^{[1]}_{12} \\\\
\end{bmatrix}
* g'(\textbf{z}^{[1]})
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\boldsymbol{\delta}^{[1]} = 
\textbf{W}^{[2]T}\boldsymbol{\delta}^{[1]}
* g'(\textbf{z}^{[1]})
$$&lt;/div&gt;
&lt;p&gt;Great, now we can use &lt;span class="math"&gt;\(\boldsymbol{\delta}^{[1]}\)&lt;/span&gt; to calculate the derivative of &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{W}^{[1]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial{J}}{\partial{\textbf{W}^{[1]}}} = 
\boldsymbol{\delta}^{[1]T}
\dfrac{\partial{\textbf{z}^{[1]}}}{\partial{\textbf{W}^{[1]}}}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial{J}}{\partial{\textbf{W}^{[1]}}} = 
\begin{bmatrix}
\delta^{[1]}_{11} &amp;amp; \delta^{[1]}_{21} &amp;amp; \delta^{[1]}_{31} &amp;amp; \delta^{[1]}_{41}
\end{bmatrix}
\begin{bmatrix}
x_{11} &amp;amp;
x_{21} &amp;amp; 
x_{31} &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 \\\\
0 &amp;amp;
0 &amp;amp; 
0 &amp;amp;
x_{11} &amp;amp;
x_{21} &amp;amp; 
x_{31} &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 \\\\
0 &amp;amp;
0 &amp;amp; 
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
x_{11} &amp;amp;
x_{21} &amp;amp; 
x_{31} &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 \\\\
0 &amp;amp;
0 &amp;amp; 
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
x_{11} &amp;amp;
x_{21} &amp;amp; 
x_{31} &amp;amp; \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial{J}}{\partial{\textbf{W}^{[1]}}} =
\begin{bmatrix}
\delta^{[1]}_{11}x_{11} &amp;amp; 
\delta^{[1]}_{11}x_{21} &amp;amp;
\delta^{[1]}_{11}x_{31} &amp;amp;
\dotsc
\delta^{[1]}_{41}x_{11} &amp;amp; 
\delta^{[1]}_{41}x_{21} &amp;amp;
\delta^{[1]}_{41}x_{31} 
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Which yields a &lt;span class="math"&gt;\((1, 12)\)&lt;/span&gt; Jacobian matrix. Like before, we need our gradient matrix &lt;span class="math"&gt;\(d\textbf{W}^{[1]}\)&lt;/span&gt; to have dimensions that match &lt;span class="math"&gt;\(\textbf{W}^{[1]}\)&lt;/span&gt;. So we will reshape the Jacobian into a &lt;span class="math"&gt;\((4,3)\)&lt;/span&gt; matrix.&lt;/p&gt;
&lt;div class="math"&gt;$$d\textbf{W}^{[1]} = 
\begin{bmatrix}
\\
\delta^{[1]}_{11}x_{11} &amp;amp; 
\dotsc &amp;amp;
\delta^{[1]}_{11}x_{31} \\\\
\vdots &amp;amp; \vdots \\\\
\delta^{[1]}_{41}x_{11} &amp;amp; 
\dotsc &amp;amp; 
\delta^{[1]}_{41}x_{31} \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;And like last time, this breaks apart into two matricies and we get:&lt;/p&gt;
&lt;div class="math"&gt;$$d\textbf{W}^{[1]} = 
\begin{bmatrix}
\\
\delta^{[1]}_{11} \\\\
\delta^{[1]}_{21} \\\\
\delta^{[1]}_{31} \\\\
\delta^{[1]}_{41} \\\\
\end{bmatrix}
\begin{bmatrix}
\\
x_{11} &amp;amp; x_{21} &amp;amp;  x_{31}\\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Which becomes:&lt;/p&gt;
&lt;div class="math"&gt;$$d\textbf{W}^{[1]} = \boldsymbol{\delta}^{[1]}\textbf{x}^{T}$$&lt;/div&gt;
&lt;p&gt;Let's now calculate the partial derivative of the loss &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{b}^{[1]}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial J}{\partial{\textbf{b}^{[1]}}} = \dfrac{dJ}{d{\textbf{z}^{[1]}}}
\dfrac{\partial{\textbf{z}^{[1]}}}{\partial{\textbf{b}^{[1]}}}$$&lt;/div&gt;
&lt;p&gt;We know that:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial \textbf{z}^{[1]}}{\partial \textbf{b}^{[1]}} = 
\begin{bmatrix}
    \\
    1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\\
    0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \\\\
    0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 \\\\
    0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 \\\\
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;Which is just the identity matrix. So the partial derivative just simplfies to become:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{J}}{\partial{\textbf{b}^{[1]}}} = \boldsymbol{\delta}^{[1]T}$$&lt;/div&gt;
&lt;p&gt;And the gradient &lt;span class="math"&gt;\(d\textbf{b}^{[1]}\)&lt;/span&gt; is therefore just the transpose of the Jacobian, or:&lt;/p&gt;
&lt;div class="math"&gt;$$
d\textbf{b}^{[1]} = \boldsymbol{\delta}^{[1]}
$$&lt;/div&gt;
&lt;p&gt;Which is a &lt;span class="math"&gt;\((4,1)\)&lt;/span&gt; column vector, and matches the dimensions of &lt;span class="math"&gt;\(\textbf{b}^{[1]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h2&gt;In Summary&lt;/h2&gt;
&lt;p&gt;Our aim was to calculate the gradients for our cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to each of our parameters that we wanted to update using stochastic gradient descent.&lt;/p&gt;
&lt;div class="math"&gt;$$\bigg( d\textbf{W}^{[1]}, d\textbf{b}^{[1]}, d\textbf{W}^{[2]}, d\textbf{b}^{[2]}, 
d\textbf{W}^{[3]}, db^{[3]} \bigg)$$&lt;/div&gt;
&lt;p&gt;Below is a summary of the results:&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h4&gt;Third Layer&lt;/h4&gt;
&lt;div class="math"&gt;$$d\textbf{W}^{[3]} =  \delta^{[3]}a^{[2]T}$$&lt;/div&gt;
&lt;div class="math"&gt;$$d b^{[3]} = 
\delta^{[3]}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\boldsymbol{\delta}^{[3]} = a^{[3]} - y$$&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h4&gt;Second Layer&lt;/h4&gt;
&lt;div class="math"&gt;$$d\textbf{W}^{[2]} = \boldsymbol{\delta}^{[2]}\textbf{a}^{[1]T}$$&lt;/div&gt;
&lt;div class="math"&gt;$$d\textbf{b}^{[2]} = \boldsymbol{\delta}^{[2]}$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\boldsymbol{\delta}^{[2]}= 
\textbf{W}^{[3]T}
\delta^{[3]}
*
g'(\textbf{z}^{[2]})
$$&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h4&gt;First Layer&lt;/h4&gt;
&lt;div class="math"&gt;$$d\textbf{W}^{[1]} = \boldsymbol{\delta}^{[1]}\textbf{x}^{T}$$&lt;/div&gt;
&lt;div class="math"&gt;$$d\textbf{b}^{[1]} = \boldsymbol{\delta}^{[1]}$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\boldsymbol{\delta}^{[1]}= 
\textbf{W}^{[2]T}
\delta^{[2]}
*
g'(\textbf{z}^{[1]})
$$&lt;/div&gt;
&lt;p&gt;So all that work for a set of simple, predictable equations. Yes, and I think most people will just apply these equations when implementing a neural network in numpy. Or forgo manually architecting backpropagation and gradient descent and using a useful, automatic package like tensorflow. Which is unfortunate, because despite the arduous process of painstakingly producing these equations, I think I've learned a lot by doing it and I hope you, the reader have as well. Thank you!&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="posts"></category><category term="neural networks"></category><category term="machine learning"></category></entry><entry><title>The Math behind Neural Networks - Forward Propagation</title><link href="http://www.jasonosajima.com/forwardprop.html" rel="alternate"></link><published>2018-07-18T00:00:00-07:00</published><updated>2018-07-18T00:00:00-07:00</updated><author><name>Jason Osajima</name></author><id>tag:www.jasonosajima.com,2018-07-18:/forwardprop.html</id><summary type="html">
&lt;p&gt;When I started learning about neural networks, I found several articles and courses that guided you through their implementation in &lt;code&gt;numpy&lt;/code&gt;. But when I started my research, I couldn't see past these basic implementations. In other words, I couldn't understand the concepts in research papers and I couldn't think of any interesting research ideas.&lt;/p&gt;
&lt;p&gt;In order to go forward I had to go backwards. I had to relearn many fundamental concepts. The two concepts that are probably the most fundamental to neural networks are forward propagation and backpropagation. I decided to write two blog posts explaining in depth how these two concepts work. My hope is that by the end of this two part series you will have a deeper understanding of the fundamental underpinnings of both.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;&lt;em&gt;This is part one in a two-part series on the math behind neural networks. Part one is about forward propagation. Part two is about backpropagation and can be found &lt;a href="/backprop"&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;


&lt;p&gt;When I started learning about neural networks, I found several articles and courses that guided you through their implementation in &lt;code&gt;numpy&lt;/code&gt;. But when I started my research, I couldn't see past these basic implementations. In other words, I couldn't understand the concepts in research papers and I couldn't think of any interesting research ideas.&lt;/p&gt;
&lt;p&gt;In order to go forward I had to go backwards. I had to relearn many fundamental concepts. The two concepts that are probably the most fundamental to neural networks are forward propagation and backpropagation. I decided to write two blog posts explaining in depth how these two concepts work. My hope is that by the end of this two part series you will have a deeper understanding of the fundamental underpinnings of both.&lt;/p&gt;


&lt;p&gt;I found three resources helpful. The first is the &lt;a href="http://cs231n.github.io/neural-networks-1/"&gt;Neural Network module&lt;/a&gt; in the Stanford CS231n Convolutional Neural Networks for Visual Recognition course. The course materials are written by &lt;a href="http://karpathy.github.io"&gt;Andrej Karpathy&lt;/a&gt;. I enjoy reading Karpathy's work. He has a great conversational tone when describing concepts and it feels like you are traveling together on a roadtrip towards a better understanding of deep learning.&lt;/p&gt;
&lt;p&gt;The second resource I would recommend is the &lt;a href="http://www.deeplearningbook.org"&gt;Deep Learning Book&lt;/a&gt; written by Ian Goodfellow, Yoshua Bengio and Aaron Courville. It is an exahaustive resource of all the facts you will need to understand deep learning. It's on my to-do list to read and take notes on the entire book. In the meantime, I have used it frequently to learn about specific concepts that I wanted more information about.&lt;/p&gt;
&lt;p&gt;The third is Andrew Ng's &lt;a href="https://www.coursera.org/specializations/deep-learning"&gt;Deep Learning Specialization&lt;/a&gt;, available on Coursera. Ng has a special talent for explaining difficult ideas in simple ways. His ability to do this comes from his insistence on clear notation. He doesn't allow any notational detail to be lost. &lt;/p&gt;
&lt;p&gt;I recommend starting with the Stanford course and then moving on to Andrew Ng's course, all the while using the Deep Learning Book as a reference.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;What is the problem we are trying to solve?&lt;/h3&gt;
&lt;p&gt;I probably don't have to convince you that neural networks have shown success in several &lt;a href="http://www.wildml.com/2017/12/ai-and-deep-learning-in-2017-a-year-in-review/"&gt;domains&lt;/a&gt;. In our example we will be focused on a binary classification problem.&lt;/p&gt;
&lt;p&gt;A binary classifier is a supervised learning algorithm. We are given an input and our task is to predict which one of two classes the input belongs to. Each training example we use can be represented as &lt;span class="math"&gt;\((\textbf{x}, y)\)&lt;/span&gt;, where &lt;span class="math"&gt;\(\textbf{x} \in \mathbb{R}^{n_x}\)&lt;/span&gt; and &lt;span class="math"&gt;\(y \in (1, 0)\)&lt;/span&gt;. If you aren't familiar with this notation, it just means that &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; is a &lt;span class="math"&gt;\(n_x\)&lt;/span&gt;-dimensional feature vector and &lt;span class="math"&gt;\(y\)&lt;/span&gt; can take on values &lt;span class="math"&gt;\(1\)&lt;/span&gt; or &lt;span class="math"&gt;\(0\)&lt;/span&gt;.  Let's say we are trying to predict whether a person was happy (&lt;span class="math"&gt;\(0\)&lt;/span&gt;) or sad (&lt;span class="math"&gt;\(1\)&lt;/span&gt;) using features (1) how much sleep the person gets (2) how many times the person exercises in a week and (3) how many times the person hangs out with friends. Since we have three features, &lt;span class="math"&gt;\(n_x = 3\)&lt;/span&gt;, and each of our &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; training examples would be a &lt;span class="math"&gt;\(3\)&lt;/span&gt;-dimensional vector. The values for each of the features can be represented with a subscript. So, &lt;span class="math"&gt;\(x_1\)&lt;/span&gt; would be the value for how much sleep a person gets, &lt;span class="math"&gt;\(x_2\)&lt;/span&gt; would be the value for how much a person exercises in a week, etc.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A quick note on notation: For these blog posts, any time we define a vector or matrix, we will bold it. Anytime we define a scalar, we will keep it normal.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;span class="math"&gt;\(m\)&lt;/span&gt; is equal to the number of training examples we have. So we end up with m pairs of training examples, and it can be written in this form:&lt;/p&gt;
&lt;div class="math"&gt;$${(\textbf{x}^{(1)}, y^{(1)}), (\textbf{x}^{(2)}, y^{(2)}), (\textbf{x}^{(3)}, y^{(3)}),\ ..., (\textbf{x}^{(m)}, y^{(m)})}$$&lt;/div&gt;
&lt;p&gt;Notice that we are using the superscript &lt;span class="math"&gt;\((i)\)&lt;/span&gt; to denote the ith training example. So the third training example is &lt;span class="math"&gt;\((\textbf{x}^{(3)}, y^{(3)})\)&lt;/span&gt;. Next, we can take all of these &lt;span class="math"&gt;\(\textbf{x}^{(i)}\)&lt;/span&gt; vectors and line them up to create a matrix like so:&lt;/p&gt;
&lt;div class="math"&gt;$$\textbf{X} = \begin{bmatrix}
    | &amp;amp; | &amp;amp; ... &amp;amp; | \\
   \textbf{x}^{(1)} &amp;amp; \textbf{x}^{(2)} &amp;amp; ... &amp;amp; \textbf{x}^{(m)} \\
   | &amp;amp; | &amp;amp; ... &amp;amp; |
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;The shape of &lt;span class="math"&gt;\(\textbf{X}\)&lt;/span&gt; is &lt;span class="math"&gt;\((n_x, m)\)&lt;/span&gt;, or &lt;span class="math"&gt;\(X \in \mathbb{R}^{n_x, m}\)&lt;/span&gt;. Each row are the values for a given feature, and each column is a training example.&lt;/p&gt;
&lt;p&gt;Similarly, we can group all of the output values &lt;span class="math"&gt;\(y\)&lt;/span&gt; for each training example into a vector:&lt;/p&gt;
&lt;div class="math"&gt;$$\textbf{Y} = 
\begin{bmatrix}
y^{(1)} &amp;amp;
y^{(2)} &amp;amp;
... &amp;amp;
y^{(m)}
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;The shape of &lt;span class="math"&gt;\(\textbf{Y}\)&lt;/span&gt; is &lt;span class="math"&gt;\((1, m)\)&lt;/span&gt; or &lt;span class="math"&gt;\(\textbf{Y} \in \mathbb{R}^{1, m}\)&lt;/span&gt;.
&amp;nbsp;&lt;/p&gt;
&lt;h2&gt;Defining the Architecture&lt;/h2&gt;
&lt;p&gt;For the two blog posts, I decided to use a neural network with three layers. A ReLU activation function connects the input and two hidden layers and a sigmoid function connects the final hidden layer and the output layer.&lt;/p&gt;
&lt;p&gt;There are a lot of great resources illustrating how forward propagation and backpropagation work for a one hidden-layer neural network or logistic regression, but I think the sweet spot for understanding both concepts occurs when you use a neural network with two hidden layers. So our example will focus on a three-layer hidden network. Here's what our lovely neural network looks like without labels.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/nn_1.png" title="[nn_1]" alt="[nn_1]"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Diagram of a Neural Network with Two Hidden Layers&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The first question that may come to mind is what does the output of the neural network represent? The input of a neural network is a feature vector from a training example (&lt;span class="math"&gt;\(\textbf{x}^{(i)}\)&lt;/span&gt;). The output is our prediction &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt;. What does &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt; represent?&lt;/p&gt;
&lt;p&gt;Given a feature vector &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt;, we want to predict whether the training example is a 1 or 0. We can think of our prediction as the probability that &lt;span class="math"&gt;\(y\)&lt;/span&gt; is equal to 1 given the feature vector &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; for our training example, or &lt;span class="math"&gt;\(\hat{y} = P(y=1 | \textbf{x})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/nn_2.png" title="[nn_2]" alt="[nn_2]"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Diagram with input &lt;span class="math"&gt;\(\textbf{x}^{(i)}\)&lt;/span&gt; and output &lt;span class="math"&gt;\(\hat{y}^{(i)}\)&lt;/span&gt; for a given training example &lt;span class="math"&gt;\(i\)&lt;/span&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We define the first layer of the neural network as a feature vector from a given training example (&lt;span class="math"&gt;\(i\)&lt;/span&gt;). In the diagram, each entry in the feature vector represents a scalar value. For example, &lt;span class="math"&gt;\(x^{(i)}_1\)&lt;/span&gt; is the value for the 1st feature for the ith training example.&lt;/p&gt;
&lt;p&gt;Notice that the last layer of our neural network contains &lt;span class="math"&gt;\(\hat{y}^{(i)}\)&lt;/span&gt;, which is our prediction for what we think the label should be for the ith training example.&lt;/p&gt;
&lt;p&gt;Notice that our neural network has 4 layers of nodes, but we said in the beginning that our neural network has 2 hidden layers. What's the reasoning behind this? We treat our output and input layers as layers, so technically &lt;span class="math"&gt;\(x^{(i)} = a^{(i)[0]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(a^{(i)[3]} = \hat{y}^{(i)}\)&lt;/span&gt;. We define &lt;span class="math"&gt;\(a\)&lt;/span&gt; as a vector for the given layer, and the superscript &lt;span class="math"&gt;\([j]\)&lt;/span&gt; tells us the layer number, so the input layer is the 0th layer and the output layer is the 3rd layer. Given that, how do we describe the hidden layers in between?&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/nn_3.png" title="[nn_3]" alt="[nn_3]"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Diagram with hidden layers.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In our diagram, we now have hidden layers &lt;span class="math"&gt;\(\textbf{a}^{(i)[1]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{a}^{(i)[2]}\)&lt;/span&gt;, and output layer &lt;span class="math"&gt;\(\textbf{a}^{(i)[3]} = \hat{y}^{(i)}\)&lt;/span&gt; represented and our 3 layer hidden network is defined in our diagram. This confused me from the beginning because it's a 3-layer Neural Network with 2 hidden layers. So the number of hidden layers is number of layers - 1, since we count the output as a layer.&lt;/p&gt;
&lt;p&gt;We can also vectorize our hidden layers the same way we vectorized our input (&lt;span class="math"&gt;\(\textbf{X}\)&lt;/span&gt;) and output (&lt;span class="math"&gt;\(\hat{\textbf{Y}}\)&lt;/span&gt;) by lining up the vectors for a hidden layer &lt;span class="math"&gt;\(j\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$\textbf{A}^{[j]} = \begin{bmatrix}
    | &amp;amp; | &amp;amp; ... &amp;amp; | \\
   \textbf{a}^{(1)[j]} &amp;amp; \textbf{a}^{(2)[j]} &amp;amp; ... &amp;amp; \textbf{a}^{(m)[j]} \\
   | &amp;amp; | &amp;amp; ... &amp;amp; |
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;This matrix &lt;span class="math"&gt;\(\textbf{A}^{[j]}\)&lt;/span&gt; becomes a &lt;span class="math"&gt;\(n^{\textbf{a}^{[j]}}\)&lt;/span&gt; by &lt;span class="math"&gt;\(m\)&lt;/span&gt; matrix, where &lt;span class="math"&gt;\(n^{a^{[j]}} =\)&lt;/span&gt; # of hidden units (or nodes) for layer j and &lt;span class="math"&gt;\(m\)&lt;/span&gt; is the number of training examples. Relating this back to our diagram, if &lt;span class="math"&gt;\(j = 1\)&lt;/span&gt;, &lt;span class="math"&gt;\(n^{a^{[1]}} = 4\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{A}^{[1]}\)&lt;/span&gt; has the shape  &lt;span class="math"&gt;\((4,m)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;What's interesting about the diagram (and something I didn't understand at first) is that it doesn't show any of the parameters for the model. The parameters are actually represented by the edges of the model. I'll talk about this next.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h2&gt;Going from layer to layer&lt;/h2&gt;
&lt;p&gt;Let's break down what's happening when we calculate &lt;span class="math"&gt;\(a_1^{(i)[1]}\)&lt;/span&gt;, which is the first entry for the first hidden layer for the &lt;span class="math"&gt;\(ith\)&lt;/span&gt; training example.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/nn_4.png" title="[nn_4]" alt="[nn_4]"&gt;&lt;/p&gt;
&lt;p&gt;From the diagram, you can see that the input consists of all the entries from the previous layer (in this case the input layer from the &lt;span class="math"&gt;\(ith\)&lt;/span&gt; training example &lt;span class="math"&gt;\(\textbf{x}^{(i)}\)&lt;/span&gt; and the output is the entry for the first hidden layer &lt;span class="math"&gt;\(a_1^{(i)[1]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/nn_5.png" title="[nn_5]" alt="[nn_5]"&gt;&lt;/p&gt;
&lt;p&gt;In order to calculate &lt;span class="math"&gt;\(a_1^{(i)[1]}\)&lt;/span&gt;, we take each entry from &lt;span class="math"&gt;\(\textbf{x}^{(i)}\)&lt;/span&gt; and multiply it by a weight. The notation can be a little tricky, so let's break that down. Let's say we have &lt;span class="math"&gt;\(W^{[1]}_{13}\)&lt;/span&gt;. We multiply this guy by the third entry in &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt;, or &lt;span class="math"&gt;\(x^{(i)}_3\)&lt;/span&gt; to get the first entry in the &lt;span class="math"&gt;\(1st\)&lt;/span&gt; l ayer.&lt;/p&gt;
&lt;p&gt;Let's breakdown the weights corresponding to &lt;span class="math"&gt;\(a_1^{(i)[1]}\)&lt;/span&gt; in our diagram. &lt;span class="math"&gt;\(\textbf{W}^{[1]}\)&lt;/span&gt; is a &lt;span class="math"&gt;\((4, 3)\)&lt;/span&gt; matrix:&lt;/p&gt;
&lt;div class="math"&gt;$$
\textbf{W}^{[1]} = 
\begin{bmatrix}
W^{[1]}_{11} &amp;amp; 
W^{[1]}_{12} &amp;amp;
W^{[1]}_{13} \\\\
W^{[1]}_{21} &amp;amp; 
W^{[1]}_{22} &amp;amp;
W^{[1]}_{23} \\\\
W^{[1]}_{31} &amp;amp; 
W^{[1]}_{32} &amp;amp;
W^{[1]}_{33} \\\\
W^{[1]}_{41} &amp;amp; 
W^{[1]}_{42} &amp;amp;
W^{[1]}_{43}
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;The weights that we use to calculate &lt;span class="math"&gt;\(a_1^{(i)[1]}\)&lt;/span&gt; are the weights in the first row, or &lt;span class="math"&gt;\(\textbf{W}^{[1]}_{1-}\)&lt;/span&gt;. &lt;span class="math"&gt;\(\textbf{W}^{[1]}_{1-}\)&lt;/span&gt; is a &lt;span class="math"&gt;\((1, 3)\)&lt;/span&gt; row vector:&lt;/p&gt;
&lt;div class="math"&gt;$$
\textbf{W}^{[1]}_{1-} = 
\begin{bmatrix}
W^{[1]}_{11} &amp;amp; 
W^{[1]}_{12} &amp;amp;
W^{[1]}_{13}
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;We can then multiply this vector by &lt;span class="math"&gt;\(\textbf{x}^{(i)}\)&lt;/span&gt; and we get a nicer, compact representation:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/nn_6.png" title="[nn_6]" alt="[nn_6]"&gt;&lt;/p&gt;
&lt;p&gt;Note that we add a bias &lt;span class="math"&gt;\(b^{[1]}_{1}\)&lt;/span&gt; to &lt;span class="math"&gt;\(\textbf{W}^{[1]}_{1-}\textbf{x}^{(i)}\)&lt;/span&gt;. The bias are other parameters besides the weights that our model learns. Why do we add a bias? In order to answer that, let's talk about our activation function &lt;span class="math"&gt;\(g()\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Different neural network architectures make different choices for activation functions, but in our example to keep it simple we will use a rectified linear unit, or ReLU function.&lt;/p&gt;
&lt;p&gt;The ReLU function is defined as the following:&lt;/p&gt;
&lt;div class="math"&gt;$$
g(z) = \begin{cases}
   z &amp;amp;\text{if } z &amp;gt; 0  \\
   0 &amp;amp;\text{otherwise}
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;What is the role of activation functions in Neural Networks? Activation functions introduce non-linearity into the neural network. Without activation functions, neural networks would simplify to linear functions. Let's see how that works with respect to our example. If we simplified our neural network by taking out the bias terms and activation functions, the neural network becomes:&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{y}^{(i)} = \textbf{W}^{[2]}\textbf{W}^{[1]}\textbf{A}^{(i)[0]}$$&lt;/div&gt;
&lt;p&gt;Notice that if we multiply a matrix of weights (&lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt;) by another matrix of weights (&lt;span class="math"&gt;\(\textbf{W}^{[1]}\)&lt;/span&gt;), we get one matrix of weights (&lt;span class="math"&gt;\(\textbf{W} = \textbf{W}^{[2]}\textbf{W}^{[1]}\)&lt;/span&gt;). So our example simplifies to a linear function:&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{y}^{(i)} = \textbf{W}\textbf{A}^{(i)[0]}$$&lt;/div&gt;
&lt;p&gt;Now back to our discussion about the bias term. We add a bias term in order to shift our activation function (in our case, the ReLU) to the left or right, which is usually important for learning because it makes the model more flexible.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h2&gt;Forward propagation in a 3-layer Network&lt;/h2&gt;
&lt;p&gt;Now that we discussed some of the elements of a 3-layer network, let's (finally) introduce the concept of forward propagation. &lt;/p&gt;
&lt;p&gt;Forward propagation is basically the process of taking some feature vector &lt;span class="math"&gt;\(\textbf{x}^{(i)}\)&lt;/span&gt; and getting an output &lt;span class="math"&gt;\(\hat{y}^{(i)}\)&lt;/span&gt;. Let's breakdown what's happening in our example.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/nn_8.png" title="[nn_8]" alt="[nn_8]"&gt;&lt;/p&gt;
&lt;p&gt;As you can see, we take a (3 x 1) training example &lt;span class="math"&gt;\(\textbf{x}^{(i)}\)&lt;/span&gt;, get the (4 x 1) activations from the first hidden layer &lt;span class="math"&gt;\(\textbf{a}^{(i)[1]}\)&lt;/span&gt;. Next, we get the (1 x 2) activations from the second hidden layer &lt;span class="math"&gt;\(\textbf{a}^{(i)[2]}\)&lt;/span&gt; and the final (1 x 1) output &lt;span class="math"&gt;\(\hat{y}^{(i)}\)&lt;/span&gt;. As we mentioned earlier, &lt;span class="math"&gt;\(\hat{y}^{(i)}\)&lt;/span&gt; is the probability that &lt;span class="math"&gt;\(y^{(i)}\)&lt;/span&gt; is of the positive class given the information we know in the form of &lt;span class="math"&gt;\(\textbf{x}^{(i)}\)&lt;/span&gt;, or &lt;span class="math"&gt;\(\hat{y}^{(i)} = P(y^{(i)} = 1 | x^{(i)})\)&lt;/span&gt;. In summary, forward propagation looks like this:&lt;/p&gt;
&lt;div class="math"&gt;$$\textbf{x}^{(i)} \rightarrow \textbf{a}^{(i)[1]} \rightarrow \textbf{a}^{(i)[2]} \rightarrow \hat{y}^{(i)}$$&lt;/div&gt;
&lt;p&gt;Next, let's discuss the inner-workings of each of these transitions.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;Input &lt;span class="math"&gt;\(\rightarrow\)&lt;/span&gt; 1st Hidden Layer (&lt;span class="math"&gt;\(\textbf{x}^{(i)} \rightarrow \textbf{a}^{(i)[1]}\)&lt;/span&gt;)&lt;/h3&gt;
&lt;p&gt;First, what's happening when we transition from our vector of features for the first training example &lt;span class="math"&gt;\(i\)&lt;/span&gt; to the activations from our first hidden layer. We start by multiplying &lt;span class="math"&gt;\(\textbf{x}^{(i)}\)&lt;/span&gt; by the weights and bias of the first hidden layer, &lt;span class="math"&gt;\(\textbf{W}^{[1]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{b}^{[1]}\)&lt;/span&gt; to get &lt;span class="math"&gt;\(\textbf{z}^{(i)[1]}\)&lt;/span&gt;. People sometimes call &lt;span class="math"&gt;\(\textbf{z}^{(i)[1]}\)&lt;/span&gt; the activity of the hidden layer 1 for training example &lt;span class="math"&gt;\(i\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$\textbf{z}^{(i)[1]} = \textbf{W}^{[1]}\textbf{x}^{(i)} + \textbf{b}^{[1]}$$&lt;/div&gt;
&lt;p&gt;So we start by multiplying &lt;span class="math"&gt;\(\textbf{x}^{(i)}\)&lt;/span&gt; by &lt;span class="math"&gt;\(\textbf{W}^{[1]}\)&lt;/span&gt;. &lt;span class="math"&gt;\(\textbf{x}^{(i)}\)&lt;/span&gt; is a (3 x 1) matrix, and &lt;span class="math"&gt;\(\textbf{W}^{[1]}\)&lt;/span&gt; is a (4 x 3) matrix. We then add the bias, &lt;span class="math"&gt;\(\textbf{b}^{[1]}\)&lt;/span&gt;. The dimensions of the bias &lt;span class="math"&gt;\(\textbf{b}^{[1]}\)&lt;/span&gt; match the dimensions of &lt;span class="math"&gt;\(\textbf{z}^{(i)[1]}\)&lt;/span&gt; which are (4, 1). Once we get the activity matrix &lt;span class="math"&gt;\(\textbf{z}^{(i)[1]}\)&lt;/span&gt;, we apply the activation function to each element in &lt;span class="math"&gt;\(\textbf{z}^{(i)[1]}\)&lt;/span&gt;. Recall that the activation function that we chose is ReLU, which we defined as:&lt;/p&gt;
&lt;p&gt;So we get:&lt;/p&gt;
&lt;div class="math"&gt;$$\textbf{a}^{(i)[1]} = g(\textbf{z}^{(i)[1]})$$&lt;/div&gt;
&lt;p&gt;Which just indicates that the ReLU function &lt;span class="math"&gt;\(g()\)&lt;/span&gt; is applied elementwise to &lt;span class="math"&gt;\(\textbf{z}^{(i)[1]}\)&lt;/span&gt; to get &lt;span class="math"&gt;\(\textbf{a}^{(i)[1]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\textbf{a}^{(i)[1]}\)&lt;/span&gt; has the same dimensions as &lt;span class="math"&gt;\(\textbf{z}^{(i)[1]}\)&lt;/span&gt;, so it's (4,1).&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;1st Hidden Layer &lt;span class="math"&gt;\(\rightarrow\)&lt;/span&gt; 2nd Hidden Layer (&lt;span class="math"&gt;\(\textbf{a}^{(i)[1]} \rightarrow \textbf{a}^{(i)[2]}\)&lt;/span&gt;)&lt;/h3&gt;
&lt;p&gt;This section is going to be almost identical to the previous section. We start by multiplying &lt;span class="math"&gt;\(\textbf{a}^{(i)[1]}\)&lt;/span&gt; by &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt;. &lt;span class="math"&gt;\(\textbf{a}^{(i)[1]}\)&lt;/span&gt; is a (4 x 1) matrix, and &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt; is a (2 x 4) matrix. We then add the bias, &lt;span class="math"&gt;\(\textbf{b}^{[2]}\)&lt;/span&gt;. The dimensions of the bias &lt;span class="math"&gt;\(\textbf{b}^{[2]}\)&lt;/span&gt; match the dimensions of &lt;span class="math"&gt;\(\textbf{z}^{(i)[2]}\)&lt;/span&gt; which are (2, 1). Once we get the activity matrix &lt;span class="math"&gt;\(\textbf{z}^{(i)[2]}\)&lt;/span&gt;, we apply the ReLU activation function to each element in &lt;span class="math"&gt;\(\textbf{z}^{(i)[2]}\)&lt;/span&gt;. So we get:&lt;/p&gt;
&lt;div class="math"&gt;$$\textbf{a}^{(i)[2]} = g(\textbf{z}^{(i)[2]})$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\textbf{a}^{(i)[2]}\)&lt;/span&gt; has the same dimensions as &lt;span class="math"&gt;\(\textbf{z}^{(i)[2]}\)&lt;/span&gt;, so it's (2,1).&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;2nd Hidden Layer &lt;span class="math"&gt;\(\rightarrow\)&lt;/span&gt; Output (&lt;span class="math"&gt;\(\textbf{a}^{(i)[2]} \rightarrow \hat{y}^{(i)}\)&lt;/span&gt;)&lt;/h3&gt;
&lt;p&gt;So now we have &lt;span class="math"&gt;\(\textbf{a}^{(i)[2]}\)&lt;/span&gt;. We again start by multiplying &lt;span class="math"&gt;\(\textbf{a}^{(i)[2]}\)&lt;/span&gt; by &lt;span class="math"&gt;\(\textbf{W}^{[3]}\)&lt;/span&gt;. &lt;span class="math"&gt;\(\textbf{a}^{(i)[2]}\)&lt;/span&gt; is a (2 x 1) matrix, and &lt;span class="math"&gt;\(\textbf{W}^{[3]}\)&lt;/span&gt; is a (1 x 2) matrix. We then add the bias, &lt;span class="math"&gt;\(b^{[3]}\)&lt;/span&gt;. The dimensions of the bias &lt;span class="math"&gt;\(b^{[3]}\)&lt;/span&gt; match the dimensions of &lt;span class="math"&gt;\(z^{(i)[3]}\)&lt;/span&gt; which are (1, 1). Once we get the activity matrix &lt;span class="math"&gt;\(z^{(i)[3]}\)&lt;/span&gt;, we apply the activation function to each element in &lt;span class="math"&gt;\(z^{(i)[3]}\)&lt;/span&gt;. Since this is the final layer of our neural network, we will use a sigmoid activation function:&lt;/p&gt;
&lt;div class="math"&gt;$$
\sigma(z) = \dfrac{1}{1+e^{-z}}
$$&lt;/div&gt;
&lt;p&gt;The sigmoid activation function is rarely used in modern neural networks because it suffers from the vanishing gradient problem, but it is often used as the final activation function before the output. The reason is that it is able to squash values to be between 0 and 1, which is what we want since recall we want &lt;span class="math"&gt;\(\hat{y}^{(i)}\)&lt;/span&gt; to be between 0 and 1 since &lt;span class="math"&gt;\(\hat{y}^{(i)} = P(y^{(i)} = 1 | x^{(i)})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;So we get:&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{y}^{(i)} = a^{(i)[3]} = \sigma(z^{(i)[3]})$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\hat{y}^{(i)}\)&lt;/span&gt; has the same dimensions as &lt;span class="math"&gt;\(z^{(i)[3]}\)&lt;/span&gt;, so it's (1,1).&lt;/p&gt;
&lt;p&gt;And that's it!&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;In this blog post, I used a 3-layer neural network example to help us deconstruct the math involved in forward propagation. One of the hardest parts of this process was making sure the dimensions of all the matricies match up, so some parting thoughts on dimensions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you think about just one training example &lt;span class="math"&gt;\(i\)&lt;/span&gt; like we did, the dimensions of activations &lt;span class="math"&gt;\(\textbf{a}\)&lt;/span&gt; will always be &lt;span class="math"&gt;\((n_{a}, 1)\)&lt;/span&gt;, where &lt;span class="math"&gt;\(n_a\)&lt;/span&gt; is equal to the number of nodes in the layer. So for example, if we had 100 nodes in the 5th hidden layer, &lt;span class="math"&gt;\(\textbf{a}^{(i)[5]}\)&lt;/span&gt; would have dimensions (100, 1).&lt;/li&gt;
&lt;li&gt;If you think about &lt;span class="math"&gt;\(m\)&lt;/span&gt; training examples, you simply switch the 2nd dimension from &lt;span class="math"&gt;\(1\)&lt;/span&gt; to &lt;span class="math"&gt;\(m\)&lt;/span&gt;. So for example, if we had 100 nodes in the 5th hidden layer, for m-training examples &lt;span class="math"&gt;\(\textbf{a}^{(i)[5]}\)&lt;/span&gt; would have dimensions (100, m).&lt;/li&gt;
&lt;li&gt;The weights &lt;span class="math"&gt;\(\textbf{W}^{l}\)&lt;/span&gt; for layer &lt;span class="math"&gt;\(l\)&lt;/span&gt; will have dimensions &lt;span class="math"&gt;\((n_{a}^{[l]}, n_{a}^{[l-1]})\)&lt;/span&gt; Notice that the weights don't care about the second dimension of activations &lt;span class="math"&gt;\(\textbf{a}^{[l]}\)&lt;/span&gt;, they just care about that 1st dimension.&lt;/li&gt;
&lt;li&gt;The final output layer is also our &lt;span class="math"&gt;\(\hat{y}^{(i)}\)&lt;/span&gt;. We count this layer when we label a neural network along with the hidden layers, so a 3-layer Neural Network will only have 2 hidden layers. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now that we have the forward propagation figured out, we can generate a prediction &lt;span class="math"&gt;\(\hat{y}^{(i)}\)&lt;/span&gt; given a feature vector for the ith training example &lt;span class="math"&gt;\(\textbf{x}^{(i)}\)&lt;/span&gt;. But is this a good prediction? How does it compare to the actual label, &lt;span class="math"&gt;\(y^{(i)}\)&lt;/span&gt;? In order to come up with a good prediction not just for the ith training example but for all examples, we need an algorithm to find the best values for our weights &lt;span class="math"&gt;\(\textbf{W}\)&lt;/span&gt; and biases &lt;span class="math"&gt;\(\textbf{b}\)&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;The most popular algorithm to use is called backpropagation, which we will discuss in the &lt;a href="/backprop"&gt;next post&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="posts"></category><category term="neural networks"></category><category term="machine learning"></category></entry></feed>