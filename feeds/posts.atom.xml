<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Jason {osa-jima} - posts</title><link href="http://www.jasonosajima.com/" rel="alternate"></link><link href="http://www.jasonosajima.com/feeds/posts.atom.xml" rel="self"></link><id>http://www.jasonosajima.com/</id><updated>2018-07-18T00:00:00-07:00</updated><entry><title>The Math behind Neural Networks - Backpropagation</title><link href="http://www.jasonosajima.com/backprop.html" rel="alternate"></link><published>2018-07-18T00:00:00-07:00</published><updated>2018-07-18T00:00:00-07:00</updated><author><name>Jason Osajima</name></author><id>tag:www.jasonosajima.com,2018-07-18:/backprop.html</id><summary type="html">

&lt;p&gt;The hardest part about deep learning for me was backpropagation. Forward propagation made sense; basically you do a bunch of matrix multiplications, add some bias terms, and throw in non-linearities so it doesn't turn into one large matrix multiplication. Gradient descent also intuitively made sense to me as well; we want to use the partial derivatives of our parameters with respect to our cost function (&lt;span class="math"&gt;\(J\)&lt;/span&gt;) to update our parameters in order to minimize &lt;span class="math"&gt;\(J\)&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;The objective of backpropagation is pretty clear: we need to calculate the partial derivatives of our parameters with respect to cost function (&lt;span class="math"&gt;\(J\)&lt;/span&gt;) in order to use it for gradient descent. The difficult part lies in keeping track of the calculations, since each partial derivative of parameters in each layer rely on inputs from the previous layer. Maybe it's also the fact that we are going backwards makes it hard for my brain to wrap my head around it.&lt;/p&gt;
&lt;script type='text/javascript'&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;This is part two in a two-part series on the math behind neural networks. Part two is about backpropagation. Part one is about forward propagation and can be found &lt;a href="/forwardprop"&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;


&lt;p&gt;The hardest part about deep learning for me was backpropagation. Forward propagation made sense; basically you do a bunch of matrix multiplications, add some bias terms, and throw in non-linearities so it doesn't turn into one large matrix multiplication. Gradient descent also intuitively made sense to me as well; we want to use the partial derivatives of our parameters with respect to our cost function (&lt;span class="math"&gt;\(J\)&lt;/span&gt;) to update our parameters in order to minimize &lt;span class="math"&gt;\(J\)&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;The objective of backpropagation is pretty clear: we need to calculate the partial derivatives of our parameters with respect to cost function (&lt;span class="math"&gt;\(J\)&lt;/span&gt;) in order to use it for gradient descent. The difficult part lies in keeping track of the calculations, since each partial derivative of parameters in each layer rely on inputs from the previous layer. Maybe it's also the fact that we are going backwards makes it hard for my brain to wrap my head around it.&lt;/p&gt;


&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Why do we need to learn the math behind backpropagation? Today we have great deep learning frameworks like Tensorflow and PyTorch that do backpropagation automatically, so why do we need to know how it works under the hood?&lt;/p&gt;
&lt;p&gt;Josh Waitzkin is an internationally-recognized chess player (and the movie and book "Searching for Bobby Fischer" were both based on his career) and world champion in Tai Chi Chuan. &lt;/p&gt;
&lt;p&gt;Most kids start their chess career by learning opening variations of the game. Kids learn strong opening positions in order to gain a decisive advantage in the beginning of a match. This advantage proves too much for most of their opponents and they end up winning. &lt;/p&gt;
&lt;p&gt;Josh Waitzkin's teacher did not focus on opening variations. Instead, he focused on end game scenarios, such as King v. King or King v. King Pawn. These end game scenarios were simple enough for Josh to develop an intuitive understanding of how pieces interact with one another. &lt;/p&gt;
&lt;p&gt;Kids that learn opening variations couldn't internalize these first principles about the interactions between pieces because there was too much stuff going on. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A critical challenge for all practical martial artists is to make their diverse techniques take on the efficiency of the jab. When I watched William Chen spar, he was incredibly understated and exuded shocking power. While some are content to call such abilities chi and stand in awe, I wanted to to understand what was going on. The next phase of my martial growth would involve turning the large into the small. My understanding of this process, in the spirit of my numbers to leave numbers method of chess study, is to touch the essence (for example, highly refined and deeply internalized body mechanics or feeling) of a technique, and then to incrementally condense the external manifestation of the tecnhique while keeping true to its essence. Over time expanding decreases while potency increases. I call this method 'making smaller circles'.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I think there are several ways that we can interpret this quote. For someone who is a machine learning practitioner, it may be good to take a model that you have a pretty good understanding of and try and simplify it to try to understand everything at its most basic level.&lt;/p&gt;
&lt;p&gt;The best resource for me to learn backpropagation has been Andrew Ng's [Deep Learning Specialization in Coursera]((https://www.coursera.org/specializations/deep-learning). He is always very clear about understanding the intutition behind the problem, defining the problem and then laying out the mathematical notation. In this blog post I will rely heavily on the notation and concepts used in his course.&lt;/p&gt;
&lt;p&gt;Let's make smaller circles.&lt;/p&gt;
&lt;p&gt;Backpropagation can be used in different ways, but for our purposes we will use it to train a binary classifier. In my &lt;a href="/forwardprop"&gt;previous post&lt;/a&gt; on forward propagation, I layout the architecture for a 3 layer Neural Network, which we will rely on for this example.&lt;/p&gt;
&lt;p&gt;Backpropagation starts with our loss function, so we will introduce this idea first. But before we get into the math, let's define what notation we will use through the course of this blog post.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h2&gt;Some Notation&lt;/h2&gt;
&lt;p&gt;For this blog post, any time we define a vector or matrix, we will bold it. Anytime we define a scalar, we will keep it normal. In the previous blog post on forward propagation, we introduced a 3-layer neural network architecture:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/nn_3.png" title="[nn_3]" alt="[nn_3]"&gt;&lt;/p&gt;
&lt;p&gt;So for example, &lt;span class="math"&gt;\(\textbf{a}^{(i)[1]}\)&lt;/span&gt; is a vector and is therefore bolded. The third entry for &lt;span class="math"&gt;\(\textbf{a}^{(i)[1]}\)&lt;/span&gt; is &lt;span class="math"&gt;\(a^{(i)[1]}_{31}\)&lt;/span&gt;. Since it is a scalar, it is not bolded. &lt;/p&gt;
&lt;p&gt;You may have noticed that &lt;span class="math"&gt;\(a^{(i)[3]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\hat{y}^{(i)}\)&lt;/span&gt; are not bolded. That's because they are scalars &lt;span class="math"&gt;\(a^{(i)[3]} = \hat{y}^{(i)} \in (0, 1)\)&lt;/span&gt;, and represent the probability that we think the &lt;span class="math"&gt;\(ith\)&lt;/span&gt; example belongs to the positive class, &lt;span class="math"&gt;\(y = 1\)&lt;/span&gt;. More on this later.&lt;/p&gt;
&lt;p&gt;The &lt;span class="math"&gt;\(i\)&lt;/span&gt; denotes that &lt;span class="math"&gt;\(\textbf{a}^{(i)[1]}\)&lt;/span&gt; is the activation in the &lt;span class="math"&gt;\(L = 1\)&lt;/span&gt; layer for the &lt;span class="math"&gt;\(ith\)&lt;/span&gt; example. For simplicity, we will get rid of the &lt;span class="math"&gt;\((i)\)&lt;/span&gt; notation and assume that we are working with the &lt;span class="math"&gt;\(ith\)&lt;/span&gt; training example. Our architecture therefore becomes:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/nn_simplified.png" title="[nn_simplified]" alt="[nn_simplified]"&gt;&lt;/p&gt;
&lt;p&gt;Lovely, that looks much simpler. You might be wondering why we decided to define all of our vectors as column vectors instead of row vectors. If a vector has &lt;span class="math"&gt;\(m\)&lt;/span&gt; entries, a column vector is defined to be a &lt;span class="math"&gt;\((m,1)\)&lt;/span&gt; dimensional matrix and a row vector is a &lt;span class="math"&gt;\((1, m)\)&lt;/span&gt; dimensional matrix.&lt;/p&gt;
&lt;p&gt;Some people use row vectors and others use column vectors. Most of the resources I used to write this blog post use column vectors to define the activations for each layer, so that's what we will roll with.&lt;/p&gt;
&lt;p&gt;When we define a column vector for the outputs from different layers, we will bold it and use a lowercase letter to represent it. When we define a weight matrix, we will bold it and use an uppercase letter to define it. So for example, the weight matrix we use to transition from the 2nd layer (1st hidden layer) to the 3rd layer (2nd hidden layer) would be &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt; with dimensions &lt;span class="math"&gt;\((2, 4)\)&lt;/span&gt; that match the 1st dimension of the layer it's transitioning to (2) and the 1st dimension of the layer it comes from (4). Each entry is a scalar, and therefore is not bolded.&lt;/p&gt;
&lt;div class="math"&gt;$$\textbf{W}^{[2]} = 
\begin{bmatrix}
    W^{[2]}_{11} &amp;amp; W^{[2]}_{12} &amp;amp; W^{[2]}_{13} &amp;amp; W^{[2]}_{14} \\\\
    W^{[2]}_{21} &amp;amp; W^{[2]}_{22} &amp;amp; W^{[2]}_{23} &amp;amp; W^{[2]}_{24} \\
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h2&gt;Understanding the Loss Function&lt;/h2&gt;
&lt;p&gt;In the previous post we used forward propagation to go from an input vector for the &lt;span class="math"&gt;\(ith\)&lt;/span&gt; training example &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; to &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt;. Recall that &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt; is our best guess for the class &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; belongs to. In our example, &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; could belong to either happy (&lt;span class="math"&gt;\(0\)&lt;/span&gt;) or sad (&lt;span class="math"&gt;\(1\)&lt;/span&gt;).  &lt;span class="math"&gt;\(y\)&lt;/span&gt; is the class (either 0 or 1) that &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; actually belongs to. So how can we measure how well our model is doing, i.e. how can we measure how close the prediction &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt; is to the actual &lt;span class="math"&gt;\(y\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;In order to measure the error between these two values, we use what's called a loss function. When I was introduced to the concept of a loss function, I immediately thought we should use this one:&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{L}(\hat{y}, y) = \dfrac{1}{2}(\hat{y} - y)^2$$&lt;/div&gt;
&lt;p&gt;This is a pretty simple loss function: just subtract the actual from the predicted, square it so it isn't negative, and then divide it by 2 (so the derivative looks prettier). It turns out that people don't use this loss function in logistic regression because when you try to learn the parameters the optimization problem is non-convex.&lt;/p&gt;
&lt;p&gt;A better loss function to use is this:
&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{L}(\hat{y}, y) = -ylog(\hat{y}) - (1-y)log(1 -\hat{y})$$&lt;/div&gt;
&lt;p&gt;There are several choices for what we can use for our loss function. Let's spend some time to understand how we use this loss function. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;One thing that I didn't get when I first started working on this: In high school math if you had for example &lt;span class="math"&gt;\(log \ 2\)&lt;/span&gt;, this was shorthand for log base 10, or &lt;span class="math"&gt;\(log_{10} \ 2\)&lt;/span&gt;. However, you'll find that most people that work on stats and computer science problems actually use &lt;span class="math"&gt;\(log \ 2\)&lt;/span&gt; to mean &lt;span class="math"&gt;\(ln \ 2\)&lt;/span&gt;. So in this blog post, whenever I use &lt;span class="math"&gt;\(log\)&lt;/span&gt;, I mean &lt;span class="math"&gt;\(ln\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Our objective is to try to get &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; to be as low as possible, since &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; represents the error between our prediction &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt; and the actual &lt;span class="math"&gt;\(y\)&lt;/span&gt;. Notice that when &lt;span class="math"&gt;\(y = 1\)&lt;/span&gt;, our equation turns into this:&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{L}(\hat{y},  y = 1) = -1log(\hat{y}) - (1-1)log(1 - \hat{y})$$&lt;/div&gt;
&lt;div class="math"&gt;$$\mathcal{L}(\hat{y},  y = 1) = -1log(\hat{y})$$&lt;/div&gt;
&lt;p&gt;So to minimize &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; we want &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt; to be as large as possible, which makes sense since in the final layer we put each entry &lt;span class="math"&gt;\(z^{[3]}_{1j}\)&lt;/span&gt; in the activity &lt;span class="math"&gt;\(\mathbf{z}^{[3]}\)&lt;/span&gt; through the sigmoid function, like &lt;span class="math"&gt;\(\sigma(\mathbf{z}^{[3]})\)&lt;/span&gt;. The range of the sigmoid function is &lt;span class="math"&gt;\((0, 1)\)&lt;/span&gt;, so the greatest value that &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt; can take is a number super close to 1. Conversely, when &lt;span class="math"&gt;\(y = 0\)&lt;/span&gt;, our equation turns into this:&lt;/p&gt;
&lt;div class="math"&gt;$$\mathcal{L}(\hat{y},  y = 0) = -0log(\hat{y}) - (1-0)log(1 - \hat{y})$$&lt;/div&gt;
&lt;div class="math"&gt;$$\mathcal{L}(\hat{y},  y = 0) = -1log(1 - \hat{y})$$&lt;/div&gt;
&lt;p&gt;In this case, in order to minimize &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt;, we want &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt; to be close to 0 as possible.&lt;/p&gt;
&lt;p&gt;This seems like it works, but where does this loss function come from? I'm glad you asked, it's kind of fun to figure out how it works.&lt;/p&gt;
&lt;p&gt;So in the previous post we talked about how &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt; is the probability that the example &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt; is from either the class 1, or &lt;span class="math"&gt;\(y = 1\)&lt;/span&gt;. And also keep in mind that all of these technically should have a superscript &lt;span class="math"&gt;\((i)\)&lt;/span&gt; attached to them. So more formally, &lt;span class="math"&gt;\(\hat{y} = P(y = 1 | \mathbf{x})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We can think of &lt;span class="math"&gt;\(y\)&lt;/span&gt; as a Bernoulli random variable that can take on 1 with probability &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt; and 0 with probability &lt;span class="math"&gt;\(1 - \hat{y}\)&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;The probability mass function for a Bernoulli random variable looks like this:&lt;/p&gt;
&lt;div class="math"&gt;$$p(k | p) = k^p(1-k)^{(1-p)}$$&lt;/div&gt;
&lt;p&gt;Which tells you the probability that &lt;span class="math"&gt;\(k\)&lt;/span&gt; is equal to a particular value. We want to calculate the probability that &lt;span class="math"&gt;\(y\)&lt;/span&gt; takes on a particular value so, thinking about our example, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$p(y | \hat{y}, \mathbf{x}) = y^{\hat{y}}(1-y)^{(1-\hat{y})}$$&lt;/div&gt;
&lt;p&gt;We want to maximize &lt;span class="math"&gt;\(p\)&lt;/span&gt;, or maximize the probability that given our training example feature vector &lt;span class="math"&gt;\(\mathbf{x}\)&lt;/span&gt; and prediction outputted from our neural network &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt;, we get the value &lt;span class="math"&gt;\(y\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Ok so hopefully that makes sense so far. Why can't we just use this as our loss function, since the function shows how close how our prediction (&lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt;) is to our actual (&lt;span class="math"&gt;\(y\)&lt;/span&gt;)? The reason is in backpropagation we need to take the derivative of our loss function, and it gets a little messy to take the derivative of this function.&lt;/p&gt;
&lt;p&gt;But notice that if we take the log of both sides, our objective stays the same. Instead of maximizing &lt;span class="math"&gt;\(p\)&lt;/span&gt;, we still just need to maximize the &lt;span class="math"&gt;\(log\)&lt;/span&gt; of &lt;span class="math"&gt;\(p\)&lt;/span&gt;, since the &lt;span class="math"&gt;\(log\)&lt;/span&gt; function increases monotonically. If we take the &lt;span class="math"&gt;\(log\)&lt;/span&gt; of both sides, and do a little math, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$log \ p(y | \hat{y}, \mathbf{x}) = log \ (y^{\hat{y}}(1-y)^{(1-\hat{y})})$$&lt;/div&gt;
&lt;div class="math"&gt;$$log \ p(y | \hat{y}, \mathbf{x}) = \hat{y}log \ y + (1-\hat{y})log \ (1-y)$$&lt;/div&gt;
&lt;p&gt;Notice that the left side just becomes &lt;span class="math"&gt;\(-\mathcal{L}(\hat{y}, y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$log \ p(y | \hat{y}, x) = -\mathcal{L}(\hat{y}, y)$$&lt;/div&gt;
&lt;div class="math"&gt;$$\mathcal{L}(\hat{y}, y) = -log \ p(y | \hat{y}, x)$$&lt;/div&gt;
&lt;p&gt;So when we say we want to minimize &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt;, we really mean we want to maximize the probability that &lt;span class="math"&gt;\(y\)&lt;/span&gt; is equal to it's value given our prediction &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt; and feature vector &lt;span class="math"&gt;\(x\)&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;So we figured out what &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; is equal to, which represents our loss for one training example &lt;span class="math"&gt;\(i\)&lt;/span&gt;. We could just use the gradients of &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to each scalar entry in each of our parameters. We then could use those gradients to update the values of our parameters in gradient descent. In that case, our loss function &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; would be the same as our cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt; or:&lt;/p&gt;
&lt;div class="math"&gt;$$ J = \mathcal{L}(\hat{y},y) $$&lt;/div&gt;
&lt;p&gt;In this case, we call our optimization algorithm stochastic gradient descent. We could also take a batch of training examples, say &lt;span class="math"&gt;\(m\)&lt;/span&gt; training examples and define our cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt; to be the average of the loss &lt;span class="math"&gt;\(\mathcal{L}(\hat{y}^{(i)},y^{(i)})\)&lt;/span&gt; for &lt;span class="math"&gt;\(m\)&lt;/span&gt; training examples, or:&lt;/p&gt;
&lt;div class="math"&gt;$$J = \dfrac{1}{m}\sum^m_{i = 1} \mathcal{L(\hat{y}^{(i)},y^{(i)})}$$&lt;/div&gt;
&lt;p&gt;If &lt;span class="math"&gt;\(m\)&lt;/span&gt; is equal to the number of training examples we have access to, we usually call our optimization algorithm batch gradient descent. If &lt;span class="math"&gt;\(m\)&lt;/span&gt; is less than the number of training examples, we call it mini-batch gradient descent.&lt;/p&gt;
&lt;p&gt;For simplicity, in this blog post we will focus on stochastic gradient descent, so our cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt; is:&lt;/p&gt;
&lt;div class="math"&gt;$$ J = \mathcal{L}(\hat{y},y) $$&lt;/div&gt;
&lt;p&gt;Great, so now we have defined &lt;span class="math"&gt;\(J\)&lt;/span&gt; and we want to minimize it. How do we do that? Most people use gradient descent, which requires us to calculate the gradient for &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to the parameters that we can change. Calculating these gradients is the objective of backpropagation.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h2&gt;Introducting Backpropagation&lt;/h2&gt;
&lt;p&gt;In backpropagation, our objective is to calculate the gradients of &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to what we can change in our neural network. In our three layer network, we can change the value of our parameters. Recall that the architecture of our network looked like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/nn_3.png" title="[nn_3]" alt="[nn_3]"&gt;&lt;/p&gt;
&lt;p&gt;We can think of the nodes from the two layers connecting the input to the output layer as the intermediate products of the model.&lt;/p&gt;
&lt;p&gt;Notice that this diagram doesn't include any of the parameters of our model. The parameters are the weights (&lt;span class="math"&gt;\(\textbf{W}^{[j]}\)&lt;/span&gt;) and biases (&lt;span class="math"&gt;\(\textbf{b}^{[j]}\)&lt;/span&gt;) associated with the j-th layer. Because in order to go from one layer to the next, we multiply the nodes from the previous layer by the weights, add a bias, and send it through an activation function.&lt;/p&gt;
&lt;p&gt;We can think of the lines that connect the nodes of each layer to represent these transformations. In the diagram, there are three sets of lines connecting the four layers, and unsurprisingly there are three sets of weights and biases to go along with them:&lt;/p&gt;
&lt;div class="math"&gt;$$(\textbf{W}^{[1]}, \textbf{b}^{[1]}, \textbf{W}^{[2]}, \textbf{b}^{[2]}, \textbf{W}^{[3]}, b^{[3]})$$&lt;/div&gt;
&lt;p&gt;Let's understand the dimensions of each of these parameters. In the previous post, we talked about how the dimensions of the weights that connect layers are equal to the number of entries in those layers, represented by column vectors. So for example, &lt;span class="math"&gt;\(\textbf{W}^{[1]}\)&lt;/span&gt; connects &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; a column vector with 3 entries to &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt;, a column vector with 3 entires. So the dimensions of &lt;span class="math"&gt;\(\textbf{W}^{[1]}\)&lt;/span&gt; will be &lt;span class="math"&gt;\((4, 3)\)&lt;/span&gt;. Similarly, the dimensions of &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt; will be &lt;span class="math"&gt;\((2, 4)\)&lt;/span&gt; and the dimensions of &lt;span class="math"&gt;\(\textbf{W}^{[3]}\)&lt;/span&gt; will be &lt;span class="math"&gt;\((1, 2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Biases are simpler, since they match the dimensions of the layer that we are headed towards. So for example, &lt;span class="math"&gt;\(\textbf{b}^{[1]}\)&lt;/span&gt; is &lt;span class="math"&gt;\((4, 1)\)&lt;/span&gt;, &lt;span class="math"&gt;\(\textbf{b}^{[2]}\)&lt;/span&gt; is &lt;span class="math"&gt;\((2, 1)\)&lt;/span&gt;, and &lt;span class="math"&gt;\(b^{[3]}\)&lt;/span&gt; is a scalar value.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;Gradients for Activation Functions?&lt;/h3&gt;
&lt;p&gt;Do we need to worry about parameters in the activation functions we use? Let's first recall the activation functions that we are using in our example. We use a ReLU function &lt;span class="math"&gt;\(g()\)&lt;/span&gt; to go from the input layer &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; to the first hidden layer &lt;span class="math"&gt;\(\textbf{a}^{[1]}\)&lt;/span&gt;, a ReLU function &lt;span class="math"&gt;\(g()\)&lt;/span&gt; to go from the first hidden layer &lt;span class="math"&gt;\(\textbf{a}^{[1]}\)&lt;/span&gt; to the second hidden layer &lt;span class="math"&gt;\(\textbf{a}^{[2]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
g(z) = \begin{cases}
   x &amp;amp;\text{if } z &amp;gt; 0  \\
   0 &amp;amp;\text{otherwise}
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;Where &lt;span class="math"&gt;\(z\)&lt;/span&gt; is a scalar value.&lt;/p&gt;
&lt;p&gt;Notice that the ReLU function doesn't include any parameters that we would need to optimize in our model.&lt;/p&gt;
&lt;p&gt;We use a sigmoid function &lt;span class="math"&gt;\(\sigma()\)&lt;/span&gt; to go from the second hidden layer &lt;span class="math"&gt;\(\textbf{a}^{[2]}\)&lt;/span&gt; to the final output layer &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt;. &lt;/p&gt;
&lt;div class="math"&gt;$$
\sigma(z) = \dfrac{1}{1+e^{-z}}
$$&lt;/div&gt;
&lt;p&gt;Same as the ReLU function, there are no parameters that we need to optimize in this function.&lt;/p&gt;
&lt;p&gt;So we talked about how in backpropagation we calculate the gradient with respect to each of the parameters that we are interested in optimizing. The gradient is just the partial derivative with respect to each parameter. So for example, the gradient of the cost function (&lt;span class="math"&gt;\(J\)&lt;/span&gt;) with respect to the weight that connects the third node in the input layer to the second node in the first hidden layer will be:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{J}}{\partial{W^{[1]}_{23}}}$$&lt;/div&gt;
&lt;p&gt;Each time we do backpropagation, we need to not only calculate this gradient, but the gradients for all of our parameters. How many gradients do we need to calculate? If we multiply the dimensions for each of our weights &lt;span class="math"&gt;\((12 + 8 + 2 = 22)\)&lt;/span&gt; and biases &lt;span class="math"&gt;\((4+2+1 = 7)\)&lt;/span&gt;, we get 22 + 7 = 29 parameters and therefore 29 gradients (like the one above).&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h4&gt;Vectorizing the Gradients&lt;/h4&gt;
&lt;p&gt;In the same way we combined the feature vectors &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; with dimensions &lt;span class="math"&gt;\((3,1)\)&lt;/span&gt; of &lt;span class="math"&gt;\(m\)&lt;/span&gt; training examples into a matrix &lt;span class="math"&gt;\(X\)&lt;/span&gt; with dimensions &lt;span class="math"&gt;\((3, m)\)&lt;/span&gt;, we can take our 29 gradients and combine them into gradient matricies to make our notation a little easier to follow. For example, we could represent the gradient of the cost function (&lt;span class="math"&gt;\(J\)&lt;/span&gt;) with respect to &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt; as just a matrix of the partial derivatives of &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to each entry &lt;span class="math"&gt;\(W^{[2]}_{ij}\)&lt;/span&gt; like so:&lt;/p&gt;
&lt;div class="math"&gt;$$d\textbf{W}^{[2]} = 
\begin{bmatrix}
    \dfrac{\partial{J}}{\partial{W^{[2]}_{11}}} &amp;amp; 
    \dfrac{\partial{J}}{\partial{W^{[2]}_{12}}} &amp;amp;
    \dfrac{\partial{J}}{\partial{W^{[2]}_{13}}} &amp;amp;
    \dfrac{\partial{J}}{\partial{W^{[2]}_{14}}} \\\\
    \dfrac{\partial{J}}{\partial{W^{[2]}_{21}}} &amp;amp;
    \dfrac{\partial{J}}{\partial{W^{[2]}_{22}}} &amp;amp;
    \dfrac{\partial{J}}{\partial{W^{[2]}_{23}}} &amp;amp;
    \dfrac{\partial{J}}{\partial{W^{[2]}_{24}}} \\
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;Notice that the dimensions of the matrix &lt;span class="math"&gt;\(d\textbf{W}^{[2]}\)&lt;/span&gt; match &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt;, which makes the gradient update a very simple elementwise operation.&lt;/p&gt;
&lt;p&gt;So why isn't that matrix of partial derivatives equal to:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{J}}{\partial{\textbf{W}^{[2]}}}$$&lt;/div&gt;
&lt;p&gt;There are tons of resources online that treat these two things as the same thing. In fact, it wasn't until I took Andrew Ng's Deep Learning Specialization on Coursera that I was introduced to the notation &lt;span class="math"&gt;\(d\textbf{W}^{[2]}\)&lt;/span&gt;. The problem is that for simple examples, they are equal to each other, but for more complex examples they won't be equal to each other. We will think of them as separate, and the partial derivative of &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to  &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt; we can sometimes use as an intermediate calculation to arrive at &lt;span class="math"&gt;\(d\textbf{W}^{[2]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We started with 29 gradients, and we can now collapse those gradients into 6 gradient matricies:&lt;/p&gt;
&lt;div class="math"&gt;$$\bigg( d\textbf{W}^{[1]}, d\textbf{b}^{[1]}, d\textbf{W}^{[2]}, d\textbf{b}^{[2]}, 
d\textbf{W}^{[3]}, db^{[3]} \bigg)$$&lt;/div&gt;
&lt;p&gt;Keep in mind that each of these gradient matricies should match the dimensions of the parameter they correspond to.&lt;/p&gt;
&lt;p&gt;Next, let's talk about how we implement backpropagation to calculate these gradient matricies.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h2&gt;Implementing Backpropagation&lt;/h2&gt;
&lt;p&gt;In forward propagation, given a feature vector &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; for the &lt;span class="math"&gt;\(ith\)&lt;/span&gt; example, our goal was to calculate one output, &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt; which is our best guess for what class the example &lt;span class="math"&gt;\(i\)&lt;/span&gt; belongs to.&lt;/p&gt;
&lt;p&gt;In backpropagation, for our 3 layer neural network example our goal is to calculate the 6 gradient matricies.&lt;/p&gt;
&lt;p&gt;We do this (unsurprisingly) by working backwards. So we will start by calculating:&lt;/p&gt;
&lt;div class="math"&gt;$$d\textbf{W}^{[3]}, db^{[3]}$$&lt;/div&gt;
&lt;p&gt;What are the equations that connect our cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt; to our weights and biases &lt;span class="math"&gt;\(\textbf{W}^{[3]}, b^{[3]}\)&lt;/span&gt; ? We are going to use &lt;span class="math"&gt;\(a^{[3]}\)&lt;/span&gt; instead of &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt;, but if it tickles your fancy you can feel free to use &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$z^{[3]} = \textbf{W}^{[3]}a^{[2]} + b^{[3]}$$&lt;/div&gt;
&lt;div class="math"&gt;$$a^{[3]} = \sigma(z^{[3]})$$&lt;/div&gt;
&lt;div class="math"&gt;$$J(a^{[3]}, y) = -ylog(a^{[3]}) - (1-y)log(1 -a^{[3]})$$&lt;/div&gt;
&lt;p&gt;So using the chain rule from calculus, we can think of &lt;span class="math"&gt;\(J\)&lt;/span&gt; as the composition of two other functions, &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(a^{[3]}\)&lt;/span&gt; and thefore write the two gradient matricies as:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{J}}{\partial{\textbf{W}^{[3]}}} = 
\dfrac{d{J}}{d{a^{[3]}}}
\dfrac{d{a^{[3]}}}{d{z^{[3]}}}
\dfrac{\partial{z^{[3]}}}{\partial{\textbf{W}^{[3]}}}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{J}}{\partial{b^{[3]}}} =
\dfrac{d{J}}{d{a^{[3]}}}
\dfrac{d{a^{[3]}}}{d{z^{[3]}}}
\dfrac{\partial{z^{[3]}}}{\partial{b^{[3]}}}$$&lt;/div&gt;
&lt;p&gt;Before moving on, notice that the first equation calculates the partial derivative of &lt;span class="math"&gt;\(J\)&lt;/span&gt; (a scalar value) with respect to &lt;span class="math"&gt;\(\textbf{W}^{[3]}\)&lt;/span&gt;, which is a row vector with dimensions &lt;span class="math"&gt;\((1,2)\)&lt;/span&gt;. So we are definitely going down the dark path of calculating vector and matrix derivatives. &lt;/p&gt;
&lt;p&gt;If you threw up in a little bit in your mouth at the prospect of taking a derivative with respect to a row vector, I found these &lt;a href="https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf"&gt;two&lt;/a&gt; &lt;a href="http://cs231n.stanford.edu/vecDerivs.pdf"&gt;resources&lt;/a&gt; helpful to understand the math a bit better.&lt;/p&gt;
&lt;p&gt;Ok great, so now let's figure out what these 2 derivatives and 2 partial derivatives are equal to, so we can ultimately calculate the gradient of &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{W}^{[3]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(b^{[3]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;So why do some of the derivatives use &lt;span class="math"&gt;\(d\)&lt;/span&gt; and others use &lt;span class="math"&gt;\(\partial\)&lt;/span&gt;? The simplest answer is that the ones that have a &lt;span class="math"&gt;\(d\)&lt;/span&gt; only depend on one variable, whereas the ones that use &lt;span class="math"&gt;\(\partial\)&lt;/span&gt; rely on more than one variable. So for example, when we think of the derivative of &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(a^{[3]}\)&lt;/span&gt;, we think of this equation:&lt;/p&gt;
&lt;div class="math"&gt;$$J(a^{[3]} | y) = -ylog(a^{[3]}) - (1-y)log(1 -a^{[3]})$$&lt;/div&gt;
&lt;p&gt;Which only uses one variable, &lt;span class="math"&gt;\(a^{[3]}\)&lt;/span&gt;. Recall that &lt;span class="math"&gt;\(y\)&lt;/span&gt; is constant, since that is the label for the class that the training example &lt;span class="math"&gt;\(i\)&lt;/span&gt; belongs to. Since we only consider one variable, we use &lt;span class="math"&gt;\(d\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;But remember that &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; is a composition of two other functions, &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(a^{[3]}\)&lt;/span&gt;. So technically if we wrote everything out we would get:&lt;/p&gt;
&lt;div class="math"&gt;$$J(\textbf{W}^{[3]}, b^{[3]} \ | \ y) = -ylog(\sigma(\textbf{W}^{[3]}a^{[2]} + b^{[3]})) - (1-y)log(1 - \sigma(\textbf{W}^{[3]}a^{[2]} + b^{[3]}))$$&lt;/div&gt;
&lt;p&gt;Because we are now working with two variables, &lt;span class="math"&gt;\(\textbf{W}^{[3]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{W}^{[3]}\)&lt;/span&gt;, we use &lt;span class="math"&gt;\(\partial\)&lt;/span&gt; to represent their derivatives instead of &lt;span class="math"&gt;\(d\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;I'm sure you can appreciate how I and I imagine a lot of other people get lost in the complexity of backpropagation. To calculate the first gradient matricies for a very simple network, we are already having to calculate 4 other gradients. My hope is that by going through this simple, 3-layer example you can scale to more complex models much easier than if you hand-waved your way through backpropagation. &lt;/p&gt;
&lt;p&gt;Let's start by calculating the derivative of &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(a^{[3]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$J(a^{[3]}, y) = -ylog(a^{[3]}) - (1-y)log(1 -a^{[3]})$$&lt;/div&gt;
&lt;p&gt;We can take the derivative of both sides with respect to &lt;span class="math"&gt;\(a^{[3]}\)&lt;/span&gt; and end up with:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{dJ}{d{a^{[3]}}} = \dfrac{-y}{a^{[3]}} - \dfrac{(1-y)}{1-a^{[3]}}(-1) $$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{dJ}{d{a^{[3]}}} = \dfrac{-y}{a^{[3]}} + \dfrac{(1-y)}{1-a^{[3]}} $$&lt;/div&gt;
&lt;p&gt;Let's do the same for the derivative of &lt;span class="math"&gt;\(a^{[3]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$a^{[3]} = \sigma(z^{[3]})$$&lt;/div&gt;
&lt;div class="math"&gt;$$a^{[3]} = \dfrac{1}{1+e^{-z^{[3]}}}$$&lt;/div&gt;
&lt;div class="math"&gt;$$a^{[3]} = (1+e^{-z^{[3]}})^{-1}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{d{a^{[3]}}}{d{z^{[3]}}} = -(1+e^{-z^{[3]}})^{-2}(e^{-z^{[3]}})(-1)$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{d{a^{[3]}}}{d{z^{[3]}}} = \dfrac{e^{-z^{[3]}}}{(1+e^{-z^{[3]}})^2}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{d{a^{[3]}}}{d{z^{[3]}}} = \dfrac{1}{1+e^{-z^{[3]}}}\dfrac{1}{1+e^{-z^{[3]}}}(e^{-z^{[3]}})$$&lt;/div&gt;
&lt;p&gt;Note that:&lt;/p&gt;
&lt;div class="math"&gt;$$a^{[3]} = \dfrac{1}{1+e^{-z^{[3]}}}$$&lt;/div&gt;
&lt;div class="math"&gt;$$1+e^{-z^{[3]}} = \dfrac{1}{a^{[3]}}$$&lt;/div&gt;
&lt;div class="math"&gt;$$e^{-z^{[3]}} = \dfrac{1-a^{[3]}}{a^{[3]}}$$&lt;/div&gt;
&lt;p&gt;So back to the derivative, we can substitue things in and get:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{d{a^{[3]}}}{d{z^{[3]}}} = (a^{[3]})^2\bigg(\dfrac{1-a^{[3]}}{a^{[3]}}\bigg)$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{d{a^{[3]}}}{d{z^{[3]}}} = a^{[3]}(1-a^{[3]})$$&lt;/div&gt;
&lt;p&gt;Finally, we need to calculate the partial derivatives of &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{W}^{[3]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(b^{[3]}\)&lt;/span&gt;. Up to this point, we've been calculating the deriviatives of scalars, and this is the first time we will calculate a gradient matrix. So the first gradient matrix we need to solve for is this guy:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{z^{[3]}}}{\partial{\textbf{W}^{[3]}}} = 
\begin{bmatrix}
    \dfrac{\partial{z^{[3]}}}{\partial{W^{[3]}_{11}}} &amp;amp; \dfrac{\partial{z^{[3]}}}{\partial{W^{[3]}_{12}}}
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;We call this matrix of partial derivatives a Jacobian matrix. What is a Jacobian Matrix? Let's spend a little bit of time deconstructing that.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h4&gt;A simple Jacobian Matrix Explanation&lt;/h4&gt;
&lt;p&gt;Let's say we have a function &lt;span class="math"&gt;\(\mathcal{f}: \mathbb{R}^m \rightarrow \mathbb{R}^n\)&lt;/span&gt; that maps either a column or row vector with &lt;span class="math"&gt;\(m\)&lt;/span&gt; entries to one that has &lt;span class="math"&gt;\(n\)&lt;/span&gt; entries. Let's make the first vector a column vector called &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; with &lt;span class="math"&gt;\(m\)&lt;/span&gt; entries.&lt;/p&gt;
&lt;div class="math"&gt;$$f(\textbf{x}) = 
\begin{bmatrix}
    \\
    f_1(x_{11}, x_{21}, \dotsc, x_{m1}) \\\\
    f_2(x_{11}, x_{21}, \dotsc, x_{m1}) \\\\
    \vdots \\\\
    f_n((x_{11}, x_{21}, \dotsc, x_{m1})) \\\\
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;So &lt;span class="math"&gt;\(f(\textbf{x})\)&lt;/span&gt; is a (n, 1) column vector. It could also be a row vector, depending on how you define it.&lt;/p&gt;
&lt;p&gt;So then, the Jacobian Matrix of &lt;span class="math"&gt;\(f\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; is defined to be: &lt;/p&gt;
&lt;div class="math"&gt;$$ \dfrac{\partial\mathcal{f}}{\partial \mathbf{x}} = 
\begin{bmatrix}
\\
\dfrac{\partial\mathcal{f_1}}{\partial x_{11}} &amp;amp;
\dotsc &amp;amp;
\dfrac{\partial\mathcal{f_1}}{\partial x_{m1}} \\\\
\vdots &amp;amp; \ddots &amp;amp; \vdots \\\\
\dfrac{\partial\mathcal{f_n}}{\partial x_{11}} &amp;amp;
\dotsc &amp;amp;
\dfrac{\partial\mathcal{f_n}}{\partial x_{m1}} \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Notice that the matrix has dimensions &lt;span class="math"&gt;\((n, m)\)&lt;/span&gt;. It gets its first dimension from the number of entries in the output vector &lt;span class="math"&gt;\(\mathcal{f}(\textbf{x})\)&lt;/span&gt; and its second dimensions from the number of entries in the input vector &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt;, regardless of whether those vectors are column or row vectors. So actually, if &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; was a row vector, we would get:&lt;/p&gt;
&lt;div class="math"&gt;$$ \dfrac{\partial\mathcal{f}}{\partial\mathbf{x}} = 
\begin{bmatrix}
\\
\dfrac{\partial\mathcal{f_1}}{\partial x_{11}} &amp;amp;
\dotsc &amp;amp;
\dfrac{\partial\mathcal{f_1}}{\partial x_{1m}} \\\\
\vdots &amp;amp; \ddots &amp;amp; \vdots \\\\
\dfrac{\partial\mathcal{f_n}}{\partial x_{11}} &amp;amp;
\dotsc &amp;amp;
\dfrac{\partial\mathcal{f_n}}{\partial x_{1m}} \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;See the slight difference? This kind of tripped me out when I first started thinking about it.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h4&gt;Back to the Gradients&lt;/h4&gt;
&lt;p&gt;In order to solve for the two partial derivatives in the matrix, let's deconstruct our equation for &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt; so we use &lt;span class="math"&gt;\(W^{[3]}_{11}\)&lt;/span&gt; and &lt;span class="math"&gt;\(W^{[3]}_{12}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$z^{[3]} = \textbf{W}^{[3]}a^{[2]} + b^{[3]}$$&lt;/div&gt;
&lt;div class="math"&gt;$$z^{[3]} = 
\begin{bmatrix}
    W^{[3]}_{11} &amp;amp; W^{[3]}_{12}
\end{bmatrix}
\begin{bmatrix}
    a^{[2]}_{11} \\
    a^{[2]}_{21}
\end{bmatrix}+ b^{[3]}$$&lt;/div&gt;
&lt;div class="math"&gt;$$z^{[3]} = W^{[3]}_{11}a^{[2]}_{11} + W^{[3]}_{12}a^{[2]}_{21}+ b^{[3]}$$&lt;/div&gt;
&lt;p&gt;Now we can calculate the partial derivative of &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(W^{[3]}_{11}\)&lt;/span&gt; -&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{z^{[3]}}}{\partial{W^{[3]}_{11}}} = (1)a^{[2]}_{11} + 0 + 0$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{z^{[3]}}}{\partial{W^{[3]}_{11}}} = a^{[2]}_{11}$$&lt;/div&gt;
&lt;p&gt;And the partial derivative of &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(W^{[3]}_{12}\)&lt;/span&gt; -&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{z^{[3]}}}{\partial{W^{[3]}_{12}}} = (1)a^{[2]}_{21} + 0 + 0$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{z^{[3]}}}{\partial{W^{[3]}_{12}}} = a^{[2]}_{21}$$&lt;/div&gt;
&lt;p&gt;So interestingly, the partial gradients with respect to the elements of the weight matrix &lt;span class="math"&gt;\(\textbf{W}^{[3]}\)&lt;/span&gt; are equal to the elements of the activations vector &lt;span class="math"&gt;\(a^{[2]}\)&lt;/span&gt; that they multiply with. This is a key point going forward when we try to generalize this process computing the larger gradient matricies.&lt;/p&gt;
&lt;p&gt;Ok so now we will update our Jacobian matrix of &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{W}^{[3]}\)&lt;/span&gt; and get the following:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{z^{[3]}}}{\partial{\textbf{W}^{[3]}}} = 
\begin{bmatrix}
    a^{[2]}_{11} &amp;amp; a^{[2]}_{21}
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;And just like a good Jacobian, the partial derivative has dimensions &lt;span class="math"&gt;\((1,2)\)&lt;/span&gt;, which match the number of entries in &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt; (1 since it's a scalar value) and the number of entries in &lt;span class="math"&gt;\(\textbf{W}^{[3]}\)&lt;/span&gt; (2).&lt;/p&gt;
&lt;p&gt;Notice that &lt;span class="math"&gt;\(a^{[2]}\)&lt;/span&gt; has dimensions (2, 1), and the derivative has dimensions (1,2). On closer investigation, it's just the transpose of &lt;span class="math"&gt;\(a^{[2]}\)&lt;/span&gt;. So another way to write it would be - &lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{z^{[3]}}}{\partial{\textbf{W}^{[3]}}} = a^{[2]T}$$&lt;/div&gt;
&lt;p&gt;Great! Let's move on to the calculating the partial derivative of &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(b^{[3]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$z^{[3]} = W^{[3]}_{11}a^{[2]}_{11} + W^{[3]}_{12}a^{[2]}_{21}+ b^{[3]}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{z^{[3]}}}{\partial{b^{[3]}}} = 0 + 0 + 1$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{z^{[3]}}}{\partial{b^{[3]}}} = 1$$&lt;/div&gt;
&lt;p&gt;And it's just 1! That's nice and simple. Now let's summarize all of our results from previous calculations and combine them to solve our original problem. So recall that our original problem was to calculate the partial derivative of the cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to the weights &lt;span class="math"&gt;\(W^{[3]}\)&lt;/span&gt; and biases &lt;span class="math"&gt;\(b^{[3]}\)&lt;/span&gt; connecting the second hidden layer &lt;span class="math"&gt;\(a^{[2]}\)&lt;/span&gt; to the output layer. We were able to deconstruct the derivative using the chain rule, and the results looked like this:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{J}}{\partial{\textbf{W}^{[3]}}} = 
\dfrac{d{J}}{d{a^{[3]}}}
\dfrac{d{a^{[3]}}}{d{z^{[3]}}}
\dfrac{\partial{z^{[3]}}}{\partial{\textbf{W}^{[3]}}}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{\mathcal{L}}}{\partial{b^{[3]}}} =
\dfrac{d{\mathcal{L}}}{d{a^{[3]}}}
\dfrac{d{a^{[3]}}}{d{z^{[3]}}}
\dfrac{\partial{z^{[3]}}}{\partial{b^{[3]}}}$$&lt;/div&gt;
&lt;p&gt;We spent the last few sections calculating the 4 intermediate derivatives needed to calculate the partial derivative of the loss function &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to the weights &lt;span class="math"&gt;\(\textbf{W}^{[3]}\)&lt;/span&gt; and biases &lt;span class="math"&gt;\(b^{[3]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{d{\mathcal{L}}}{d{a^{[3]}}} = \dfrac{-y}{a^{[3]}} + \dfrac{(1-y)}{1-a^{[3]}} $$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{d{a^{[3]}}}{d{z^{[3]}}} = a^{[3]}(1-a^{[3]})$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{z^{[3]}}}{\partial{\textbf{W}^{[3]}}} = a^{[2]T}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{z^{[3]}}}{\partial{b^{[3]}}} = 1$$&lt;/div&gt;
&lt;p&gt;Let's simplify the derivative of the cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt;.
&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{dJ}{d z^{[3]}} = \dfrac{dJ}{d{a^{[3]}}}\dfrac{d{a^{[3]}}}{d{z^{[3]}}}$$&lt;/div&gt;
&lt;div class="math"&gt;$$ = \bigg(\dfrac{-y}{a^{[3]}} + \dfrac{(1-y)}{1-a^{[3]}}\bigg)a^{[3]}(1-a^{[3]})$$&lt;/div&gt;
&lt;div class="math"&gt;$$= -y(1-a^{[3]}) + (1-y)a^{[3]}$$&lt;/div&gt;
&lt;div class="math"&gt;$$= -y+ya^{[3]} + a^{[3]} -ya^{[3]}$$&lt;/div&gt;
&lt;div class="math"&gt;$$=a^{[3]} - y$$&lt;/div&gt;
&lt;p&gt;If we substitute these values in, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{J}}{\partial{\textbf{W}^{[3]}}} = (a^{[3]} - y) a^{[2]T}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{J}}{\partial{b^{[3]}}} = a^{[3]} - y$$&lt;/div&gt;
&lt;p&gt;And that's it! And luckily, the dimensions of our partial derivatives, &lt;span class="math"&gt;\((1,2)\)&lt;/span&gt; and &lt;span class="math"&gt;\(1\)&lt;/span&gt; respectively, match the gradient matricies we wanted to calculate. So we get:&lt;/p&gt;
&lt;div class="math"&gt;$$d\textbf{W}^{[3]} = (a^{[3]} - y) a^{[2]T}$$&lt;/div&gt;
&lt;div class="math"&gt;$$d b^{[3]} = a^{[3]} - y$$&lt;/div&gt;
&lt;div class="math"&gt;$$\bigg( d\textbf{W}^{[1]}, d\textbf{b}^{[1]}, d\textbf{W}^{[2]}, d\textbf{b}^{[2]}, 
d\textbf{W}^{[3]}, db^{[3]} \bigg)$$&lt;/div&gt;
&lt;p&gt;We solved the first two gradient matricies &lt;span class="math"&gt;\((d\textbf{W}^{[3]}, db^{[3]})\)&lt;/span&gt;. Our goal was to solve for 29 gradients, and we just finished 3, since the partial derivative with respect to &lt;span class="math"&gt;\(\textbf{W}^{[3]}\)&lt;/span&gt; is a &lt;span class="math"&gt;\((1, 2)\)&lt;/span&gt; matrix and therefore has two elements and the partial derivative with respect to &lt;span class="math"&gt;\(b^{[3]}\)&lt;/span&gt; has only one element.&lt;/p&gt;
&lt;p&gt;Next, we continue to move backwards and focus on calculating the derivative of the loss function &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{b}^{[2]}\)&lt;/span&gt;. Starting with breaking it up using the chain rule, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial{J}}{\partial{\textbf{W}^{[2]}}} = 
\dfrac{d{J}}{d{z^{[3]}}}
\dfrac{d{z^{[3]}}}{d{\textbf{a}^{[2]}}}
\dfrac{d{\textbf{a}^{[2]}}}{d{\textbf{z}^{[2]}}}
\dfrac{\partial{\textbf{z}^{[2]}}}{\partial{\textbf{W}^{[2]}}}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial{J}}{\partial{\textbf{b}^{[2]}}} = 
\dfrac{d{\mathcal{L}}}{d{z^{[3]}}}
\dfrac{d{z^{[3]}}}{d{\textbf{a}^{[2]}}}
\dfrac{d{\textbf{a}^{[2]}}}{d{\textbf{z}^{[2]}}}
\dfrac{\partial{\textbf{z}^{[2]}}}{\partial{\textbf{b}^{[2]}}}
$$&lt;/div&gt;
&lt;p&gt;Notice that since:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{d{J}}{d{z^{[3]}}} = 
\dfrac{d{J}}{d{a^{[3]}}}
\dfrac{d{a^{[3]}}}{d{z^{[3]}}}
$$&lt;/div&gt;
&lt;p&gt;We already know that it is equal to:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{d{J}}{d{z^{[3]}}} = a^{[3]} - y$$&lt;/div&gt;
&lt;p&gt;And don't need to calculate it again.&lt;/p&gt;
&lt;p&gt;We sometimes refer to the partial derivative of the cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt; as &lt;span class="math"&gt;\(\delta^{[3]}\)&lt;/span&gt;, and will use that notation for the rest of this blog post.&lt;/p&gt;
&lt;p&gt;Let's focus on what we need to calculate. Let's start with the derivative of &lt;span class="math"&gt;\(\textbf{z}^{[3]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt;, which we can break down into two steps:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{d{z^{[3]}}}{d{\textbf{z}^{[2]}}} = 
\dfrac{d{z^{[3]}}}{d{\textbf{a}^{[2]}}}
\dfrac{d{\textbf{a}^{[2]}}}{d{\textbf{z}^{[2]}}}
$$&lt;/div&gt;
&lt;p&gt;We will start with calculating the derivative of &lt;span class="math"&gt;\(\textbf{z}^{[3]}\)&lt;/span&gt; with respect to the activation &lt;span class="math"&gt;\(\textbf{a}^{[2]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Recall that &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt; matches the number of nodes in the output layer and is therefore a scalar value. &lt;span class="math"&gt;\(\textbf{a}^{[2]}\)&lt;/span&gt; is a &lt;span class="math"&gt;\((2,1)\)&lt;/span&gt; column vector and its number of entries match the number of nodes in the 3rd layer or the 2nd hidden layer. So the Jacobian matrix looks like:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{d{z^{[3]}}}{d{\textbf{a}^{[2]}}} = 
\begin{bmatrix}
    \dfrac{\partial{z^{[3]}}}{\partial{a^{[2]}_{11}}} &amp;amp;
    \dfrac{\partial{z^{[3]}}}{\partial{a^{[2]}_{21}}} 
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;And the equation involving &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{a}^{[2]}\)&lt;/span&gt; is:&lt;/p&gt;
&lt;div class="math"&gt;$$z^{[3]} = \textbf{W}^{[3]}\textbf{a}^{[2]} + b^{[3]}$$&lt;/div&gt;
&lt;p&gt;Let's break this equation down:&lt;/p&gt;
&lt;div class="math"&gt;$$z^{[3]} = 
W^{[3]}_{11}a^{[2]}_{11} + 
W^{[3]}_{12}a^{[2]}_{21} +
b^{[3]}
$$&lt;/div&gt;
&lt;p&gt;If we take the partial derivative of &lt;span class="math"&gt;\(z^{[3]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(a^{[2]}_{21}\)&lt;/span&gt;, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{z^{[3]}}}{\partial{a^{[2]}_{21}}} =
0 + 
W^{[3]}_{12}(1) +
0
$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{z^{[3]}}}{\partial{a^{[2]}_{21}}} =
W^{[3]}_{12}
$$&lt;/div&gt;
&lt;p&gt;Applying this logic to every partial derivative in the vector, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{d{z^{[3]}}}{d{\textbf{a}^{[2]}}} = 
\begin{bmatrix}
\\
W^{[3]}_{11} &amp;amp; 
W^{[3]}_{12}
\\\\
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;Next, we need to calculate the derivative of &lt;span class="math"&gt;\(\textbf{a}^{[2]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;Both &lt;span class="math"&gt;\(\textbf{a}^{[2]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; are column vectors with dimensions &lt;span class="math"&gt;\((2,1)\)&lt;/span&gt; and therefore the Jacobian matrix of &lt;span class="math"&gt;\(\textbf{a}^{[2]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; will be: &lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{d{\textbf{a}^{[2]}}}{d{\textbf{z}^{[2]}}} = 
\begin{bmatrix}
\\
    \dfrac{\partial{a^{[3]}_{11}}}{\partial{z^{[2]}_{11}}} &amp;amp;
    \dfrac{\partial{a^{[3]}_{11}}}{\partial{z^{[2]}_{21}}} \\\\
    \dfrac{\partial{a^{[3]}_{21}}}{\partial{z^{[2]}_{11}}} &amp;amp;
    \dfrac{\partial{a^{[3]}_{21}}}{\partial{z^{[2]}_{21}}} \\\\
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;Notice that in order to go from &lt;span class="math"&gt;\(\textbf{z}^{[2]} \rightarrow \textbf{a}^{[2]}\)&lt;/span&gt;, we apply the ReLU function &lt;span class="math"&gt;\(g(z)\)&lt;/span&gt; to each element in &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt;. So for example, to calculate &lt;span class="math"&gt;\(a^{[2]}_{21}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$a^{[2]}_{21} = g(z^{[2]}_{21})$$&lt;/div&gt;
&lt;p&gt;Applying this logic to every partial derivative in the vector, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{d{\textbf{a}^{[2]}}}{d{\textbf{z}^{[2]}}} = 
\begin{bmatrix}
\\
    g'(z^{[2]}_{11}) &amp;amp; 0 \\\\ 
    0 &amp;amp; g'(z^{[2]}_{21})
\\\\
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;Where &lt;span class="math"&gt;\(g'(z)\)&lt;/span&gt; is the derivative of the ReLU function &lt;span class="math"&gt;\(g(z)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;What is the derivative of ReLU equal to?&lt;/p&gt;
&lt;div class="math"&gt;$$
g'(z) = \begin{cases}
   1 &amp;amp;\text{if } z &amp;gt; 0  \\
   \text{Undefined} &amp;amp;\text{if } z = 0  \\
   0 &amp;amp;\text{if } z &amp;lt; 0
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;Why is the derivative of &lt;span class="math"&gt;\(g\)&lt;/span&gt; undefined when &lt;span class="math"&gt;\(z = 0\)&lt;/span&gt;? For a function  to be differentiable at a point, it has to be continuous at that point. In order for a point to be continuous at a point, the limit of the function as it approaches that point has to defined. In order for the limit to be defined, the left and right hand limits have to equal. In this case, the right hand limit is equal to 1 and the left hand limit is equal to 0. Therefore, ReLU is not differentiable at 0.&lt;/p&gt;
&lt;p&gt;Does it matter that the derivative of the ReLU function is undefined at &lt;span class="math"&gt;\(z=0\)&lt;/span&gt; for backpropagation? In practice, no since &lt;span class="math"&gt;\(z\)&lt;/span&gt; will never be truly equal to &lt;span class="math"&gt;\(0\)&lt;/span&gt; - software implementations will have a rounding error for float points.&lt;/p&gt;
&lt;p&gt;Finally, we will need to calculate the partial derivatives of &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{b}^{[2]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let's first focus on the partial derivative of &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt;. &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; is a one-dimensional array &lt;span class="math"&gt;\((2,1)\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt; is a two-dimensional matrix &lt;span class="math"&gt;\((2,4)\)&lt;/span&gt;. Notice that each entry in &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; can be though of as the result of a function involving all 8 of the weight entries in &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt;. So for example, &lt;span class="math"&gt;\(z^{[2]}_{11}\)&lt;/span&gt; can be though of as:&lt;/p&gt;
&lt;div class="math"&gt;$$z^{[2]}_{11} = f_1(W^{[2]}_{11}, \dotsc, W^{[2]}_{14},W^{[2]}_{21},\dotsc W^{[2]}_{24} | \textbf{a}^{[2]}, \textbf{b}^{[2]})$$&lt;/div&gt;
&lt;div class="math"&gt;$$z^{[2]}_{11} = W^{[2]}_{11}a^{[2]}_{11} + \dotsc + W^{[2]}_{14}a^{[2]}_{41} +  b^{[2]}_{11}$$&lt;/div&gt;
&lt;p&gt;If we represented the entries in &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; in this way, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$\textbf{z}^{[2]} = 
\begin{bmatrix}
f_1(W^{[2]}_{11}, W^{[2]}_{12}, W^{[2]}_{13},W^{[2]}_{14},W^{[2]}_{21},\dotsc W^{[2]}_{24} | \textbf{a}^{[2]}, \textbf{b}^{[2]}) \\\\
f_1(W^{[2]}_{11}, W^{[2]}_{12}, W^{[2]}_{13},W^{[2]}_{14},W^{[2]}_{21},\dotsc W^{[2]}_{24} | \textbf{a}^{[2]}, \textbf{b}^{[2]})
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;And the Jacobian matrix would look like:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial \textbf{z}^{[2]}}{\partial \textbf{W}^{[2]}} = 
\begin{bmatrix}
\dfrac{\partial z^{[2]}_{11}}{\partial W^{[2]}_{11}} &amp;amp;
\dotsc &amp;amp; 
\dfrac{\partial z^{[2]}_{11}}{\partial W^{[2]}_{14}} &amp;amp;
\dfrac{\partial z^{[2]}_{11}}{\partial W^{[2]}_{21}} &amp;amp;
\dotsc &amp;amp;
\dfrac{\partial z^{[2]}_{11}}{\partial W^{[2]}_{24}} \\\\
\dfrac{\partial z^{[2]}_{21}}{\partial W^{[2]}_{11}} &amp;amp;
\dotsc &amp;amp; 
\dfrac{\partial z^{[2]}_{21}}{\partial W^{[2]}_{14}} &amp;amp;
\dfrac{\partial z^{[2]}_{21}}{\partial W^{[2]}_{21}} &amp;amp;
\dotsc &amp;amp;
\dfrac{\partial z^{[2]}_{21}}{\partial W^{[2]}_{24}} \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;So the Jacobian matrix has dimensions &lt;span class="math"&gt;\((2, 8)\)&lt;/span&gt;, which is equal to the number of entries in &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; and the number of entries in &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;But do we need to calculate all 16 derivatives? Luckily, no. The reason is that most of the derivatives will be equal to &lt;span class="math"&gt;\(0\)&lt;/span&gt;, and will stay &lt;span class="math"&gt;\(0\)&lt;/span&gt; regardless if we change the values of any of the variables.&lt;/p&gt;
&lt;p&gt;In order to illustrate this point, let's look at the first element of &lt;span class="math"&gt;\(z^{[2]}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$z^{[2]}_{11} = W^{[2]}_{11}a^{[1]}_{11} + W^{[2]}_{12}a^{[1]}_{21} + W^{[2]}_{13}a^{[1]}_{31} + W^{[2]}_{14}a^{[1]}_{41} $$&lt;/div&gt;
&lt;p&gt;Notice that in order to calculate the partial derivative of &lt;span class="math"&gt;\(z^{[2]}_1\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(W^{[2]}\)&lt;/span&gt;, we only need to worry about &lt;span class="math"&gt;\(W^{[2]}_{11}\)&lt;/span&gt;, &lt;span class="math"&gt;\(W^{[2]}_{12}\)&lt;/span&gt;, &lt;span class="math"&gt;\(W^{[2]}_{13}\)&lt;/span&gt;, and &lt;span class="math"&gt;\(W^{[2]}_{14}\)&lt;/span&gt; and not the other 4 scalar variables in &lt;span class="math"&gt;\(W^{[2]}\)&lt;/span&gt;. As a general rule of thumb, the partial derivative of a scalar element in vector &lt;span class="math"&gt;\(a\)&lt;/span&gt; with respect to a scalar element in matrix &lt;span class="math"&gt;\(W\)&lt;/span&gt; will be nonzero when the x-dimension of &lt;span class="math"&gt;\(W\)&lt;/span&gt; matches the dimension of &lt;span class="math"&gt;\(a\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;And from a previous calculation, we know that the derivative of a scalar component of &lt;span class="math"&gt;\(z\)&lt;/span&gt; with respect to a scalar component of &lt;span class="math"&gt;\(W\)&lt;/span&gt; is just that scalar component of &lt;span class="math"&gt;\(W\)&lt;/span&gt; if it's included in the calculation of the scalar component of &lt;span class="math"&gt;\(z\)&lt;/span&gt;, so we get this fun matrix:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial \textbf{z}^{[2]}}{\partial \textbf{W}^{[2]}} = 
\begin{bmatrix}
a^{[2]}_{11} &amp;amp;
\dotsc &amp;amp; 
a^{[2]}_{41} &amp;amp;
0 &amp;amp;
\dotsc &amp;amp;
0 \\\\
0 &amp;amp;
\dotsc &amp;amp; 
0 &amp;amp;
a^{[2]}_{11} &amp;amp; 
\dotsc &amp;amp;
a^{[2]}_{41} &amp;amp; \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Now, let's calculate the Jacobian matrix of &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{b}^{[2]}\)&lt;/span&gt;. It's pretty simple, just like last time the derivatives that involve the entry of &lt;span class="math"&gt;\(\textbf{b}^{[2]}\)&lt;/span&gt; are equal to 1.&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial \textbf{z}^{[2]}}{\partial \textbf{b}^{[2]}} = 
\begin{bmatrix}
    1 &amp;amp; 0 \\\\
    0 &amp;amp; 1 
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;The dimensions &lt;span class="math"&gt;\((2,2)\)&lt;/span&gt; of the Jacobian matrix are equal to the number of entries in &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{b}^{[2]}\)&lt;/span&gt;, respectively.&lt;/p&gt;
&lt;p&gt;We have all the pieces to finally calculate the derivative of the cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\mathbf{W}^{[2]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\mathbf{b}^{[2]}\)&lt;/span&gt;. &lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial{J}}{\partial{\mathbf{W}^{[2]}}} = 
\delta^{[3]}
\dfrac{d{z^{[3]}}}{d{\mathbf{z}^{[2]}}}
\dfrac{\partial{\mathbf{z}^{[2]}}}{\partial{\mathbf{W}^{[2]}}}
$$&lt;/div&gt;
&lt;p&gt;And this is how we deconstructed the derivative of the loss function &lt;span class="math"&gt;\(\mathcal{L}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(b^{[2]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial{J}}{\partial{\mathbf{b}^{[2]}}} = 
\delta^{[3]}
\dfrac{d{z^{[3]}}}{d{\mathbf{z}^{[2]}}}
\dfrac{\partial{\mathbf{z}^{[2]}}}{\partial{\mathbf{b}^{[2]}}}
$$&lt;/div&gt;
&lt;p&gt;To summarize our calculations and the dimensions of each:&lt;/p&gt;
&lt;div class="math"&gt;$$
\delta^{[3]} = 
a^{[3]} - y
$$&lt;/div&gt;
&lt;p&gt;
Is a scalar value.&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{d{z^{[3]}}}{d{\textbf{a}^{[2]}}} = 
\begin{bmatrix}
\\
    W^{[3]}_{11} &amp;amp; 
    W^{[3]}_{12}
\\\\
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;Is a &lt;span class="math"&gt;\((1,2)\)&lt;/span&gt; matrix.&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{d{\textbf{a}^{[2]}}}{d{\textbf{z}^{[2]}}} = 
\begin{bmatrix}
\\
    g'(z^{[2]}_{11}) &amp;amp; 0 \\\\ 
    0 &amp;amp; g'(z^{[2]}_{21})
\\\\
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;Is a &lt;span class="math"&gt;\((2, 2)\)&lt;/span&gt; matrix.&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial \textbf{z}^{[2]}}{\partial \textbf{W}^{[2]}} = 
\begin{bmatrix}
a^{[2]}_{11} &amp;amp;
\dotsc &amp;amp; 
a^{[2]}_{41} &amp;amp;
0 &amp;amp;
\dotsc &amp;amp;
0 \\\\
0 &amp;amp;
\dotsc &amp;amp; 
0 &amp;amp;
a^{[2]}_{11} &amp;amp; 
\dotsc &amp;amp;
a^{[2]}_{41} &amp;amp; \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Is a &lt;span class="math"&gt;\((2,8)\)&lt;/span&gt; matrix.&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial \textbf{z}^{[2]}}{\partial \textbf{b}^{[2]}} = 
\begin{bmatrix}
    1 &amp;amp; 0 \\\\
    0 &amp;amp; 1 
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;Is a &lt;span class="math"&gt;\((2,2)\)&lt;/span&gt; matrix.&lt;/p&gt;
&lt;p&gt;Let's first try and calculate the derivative of &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt;. We can start by looking at:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{J}}{\partial{\mathbf{z}^{[2]}}} = 
\delta^{[3]}
\dfrac{d{z^{[3]}}}{d{\mathbf{a}^{[2]}}}
\dfrac{d{a^{[3]}}}{d{\mathbf{z}^{[2]}}}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{J}}{\partial{\mathbf{z}^{[2]}}} = \delta^{[3]}
\begin{bmatrix}
W^{[3]}_{11} &amp;amp; 
W^{[3]}_{12}
\end{bmatrix}
\begin{bmatrix}
g'(z^{[2]}_{11}) &amp;amp; 0 \\\\ 
0 &amp;amp; g'(z^{[2]}_{21})
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;So this is the Jacobian Matrix of &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt;. But notice that this matrix is &lt;span class="math"&gt;\((1,2)\)&lt;/span&gt; but &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; is &lt;span class="math"&gt;\((2,1)\)&lt;/span&gt;. We can define a gradient matrix similar to what we did for the weights and bias parameters that match the dimensions of &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; called &lt;span class="math"&gt;\(\boldsymbol{\delta}^{[2]}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{\delta}^{[2]} = 
\bigg( \dfrac{\partial{J}}{\partial{\mathbf{z}^{[2]}}}\bigg)^T
$$&lt;/div&gt;
&lt;p&gt;Recall that given three matricies &lt;span class="math"&gt;\(\textbf{A}\)&lt;/span&gt;, &lt;span class="math"&gt;\(\textbf{B}\)&lt;/span&gt;, and &lt;span class="math"&gt;\(\textbf{d}\)&lt;/span&gt; the &lt;span class="math"&gt;\((\textbf{ABC})^T = \textbf{C}^T\textbf{B}^T\textbf{A}^T\)&lt;/span&gt;&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{\delta}^{[2]} = 
\begin{bmatrix}
g'(z^{[2]}_{11}) &amp;amp; 0 \\\\ 
0 &amp;amp; g'(z^{[2]}_{21})
\end{bmatrix}
\begin{bmatrix}
W^{[3]}_{11} \\\\ 
W^{[3]}_{12}
\end{bmatrix}
\delta^{[3]}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\boldsymbol{\delta}^{[2]} = 
\begin{bmatrix}
g'(z^{[2]}_{11})W^{[3]}_{11}\delta^{[3]} \\\\ 
g'(z^{[2]}_{21})W^{[3]}_{12}\delta^{[3]}
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Notice when we rearranged the products in the matrix, we get:&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{\delta}^{[2]} = 
\begin{bmatrix}
W^{[3]}_{11}\delta^{[3]}g'(z^{[2]}_{11}) \\\\ 
W^{[3]}_{12}\delta^{[3]}g'(z^{[2]}_{21})
\end{bmatrix}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\boldsymbol{\delta}^{[2]} = 
\begin{bmatrix}
W^{[3]}_{11}\delta^{[3]} \\\\ 
W^{[3]}_{12}\delta^{[3]}
\end{bmatrix}
*
g'(\textbf{z}^{[2]})
$$&lt;/div&gt;
&lt;p&gt;Where &lt;span class="math"&gt;\(*\)&lt;/span&gt; indicates elementwise multiplication between two matricies. &lt;span class="math"&gt;\(g'(\textbf{z}^{[2]})\)&lt;/span&gt; is a columnwise vector of the derivative of ReLU &lt;span class="math"&gt;\(g'(z)\)&lt;/span&gt; applied to each entry of &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Next, we can decompose the result into:&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{\delta}^{[2]}= 
\begin{bmatrix}
W^{[3]}_{11} \\\\ 
W^{[3]}_{12}
\end{bmatrix}
\delta^{[3]}
*
g'(\textbf{z}^{[2]})
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\boldsymbol{\delta}^{[2]}= 
\textbf{W}^{[3]T}
\delta^{[3]}
*
g'(\textbf{z}^{[2]})
$$&lt;/div&gt;
&lt;p&gt;This final result the gradient of the cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt;. It is a &lt;span class="math"&gt;\((2,1)\)&lt;/span&gt; column vector with dimensions that match &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Plugging that result into our equation we get:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial{J}}{\partial{\mathbf{W}^{[2]}}} = 
\dfrac{\partial{J}}{\partial{\mathbf{z}^{[2]}}}
\dfrac{\partial{\mathbf{z}^{[2]}}}{\partial{\mathbf{W}^{[2]}}}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial{J}}{\partial{\mathbf{W}^{[2]}}} = 
\boldsymbol{\delta}^{[2]T}
\begin{bmatrix}
a^{[2]}_{11} &amp;amp;
\dotsc &amp;amp; 
a^{[2]}_{41} &amp;amp;
0 &amp;amp;
\dotsc &amp;amp;
0 \\\\
0 &amp;amp;
\dotsc &amp;amp; 
0 &amp;amp;
a^{[2]}_{11} &amp;amp; 
\dotsc &amp;amp;
a^{[2]}_{41} &amp;amp; \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
 = 
\begin{bmatrix}
\delta^{[2]}_{11} &amp;amp;
\delta^{[2]}_{21}
\end{bmatrix}
\begin{bmatrix}
a^{[2]}_{11} &amp;amp;
\dotsc &amp;amp; 
a^{[2]}_{41} &amp;amp;
0 &amp;amp;
\dotsc &amp;amp;
0 \\\\
0 &amp;amp;
\dotsc &amp;amp; 
0 &amp;amp;
a^{[2]}_{11} &amp;amp; 
\dotsc &amp;amp;
a^{[2]}_{41} &amp;amp; \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;And we get this fun &lt;span class="math"&gt;\((1, 8)\)&lt;/span&gt; Jacobian matrix:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{J}}{\partial{\textbf{W}^{[2]}}} = 
\begin{bmatrix}
\delta^{[2]}_{11}a^{[2]}_{11} &amp;amp; 
\delta^{[2]}_{11}a^{[2]}_{21} &amp;amp;
\delta^{[2]}_{11}a^{[2]}_{31} &amp;amp; 
\delta^{[2]}_{11}a^{[2]}_{41} &amp;amp;
\delta^{[2]}_{21}a^{[2]}_{11} &amp;amp; 
\delta^{[2]}_{21}a^{[2]}_{21} &amp;amp;
\delta^{[2]}_{21}a^{[2]}_{31} &amp;amp; 
\delta^{[2]}_{21}a^{[2]}_{41}
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;So this is our Jacobian. But we need our gradient matrix &lt;span class="math"&gt;\(d\textbf{W}^{[2]}\)&lt;/span&gt; to have dimensions that match &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt;. So we will reshape the Jacobian into a &lt;span class="math"&gt;\((2,4)\)&lt;/span&gt; matrix.&lt;/p&gt;
&lt;div class="math"&gt;$$d\textbf{W}^{[2]} = 
\begin{bmatrix}
\delta^{[2]}_{11}a^{[2]}_{11} &amp;amp; 
\dotsc &amp;amp;
\delta^{[2]}_{11}a^{[2]}_{41} \\\\
\delta^{[2]}_{21}a^{[2]}_{11} &amp;amp; 
\dotsc &amp;amp; 
\delta^{[2]}_{21}a^{[2]}_{41} \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;And, interestingly, we can break this apart into two matricies.&lt;/p&gt;
&lt;div class="math"&gt;$$d\textbf{W}^{[2]} = 
\begin{bmatrix}
\delta^{[2]}_{11} \\\\
\delta^{[2]}_{21} \\\\
\end{bmatrix}
\begin{bmatrix}
a^{[2]}_{11} &amp;amp; a^{[2]}_{21} &amp;amp;  a^{[2]}_{31} &amp;amp; a^{[2]}_{41} \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Which becomes:&lt;/p&gt;
&lt;div class="math"&gt;$$d\textbf{W}^{[2]} = \boldsymbol{\delta}^{[2]}\textbf{a}^{[2]T}$$&lt;/div&gt;
&lt;p&gt;Alright, so you might be wondering, why did we need to go through all that work to arrive at that simple result? The reason is because most tutorials, blog posts, and courses skip the math and arrive at this result. But I think it's important to work through how we arrive there step by step. When we go through it step by step, we begin to understand how each of these operations relates to linear algebra and multivariate calculus. When we are just presented with the final result, we tend to just memorize it.&lt;/p&gt;
&lt;p&gt;Let's now calculate the partial derivative of the loss &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{b}^{[2]}\)&lt;/span&gt;, which luckily is a lot easier.&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial J}{\partial{\textbf{b}^{[2]}}} = \dfrac{dJ}{d{\textbf{z}^{[2]}}}
\dfrac{\partial{\textbf{z}^{[2]}}}{\partial{\textbf{b}^{[2]}}}$$&lt;/div&gt;
&lt;p&gt;We know that:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial \textbf{z}^{[2]}}{\partial \textbf{b}^{[2]}} = 
\begin{bmatrix}
    1 &amp;amp; 0 \\\\
    0 &amp;amp; 1 
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;Which is just the identity matrix. So the partial derivative just simplfies to become:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{J}}{\partial{\textbf{b}^{[2]}}} = \boldsymbol{\delta}^{[2]T}$$&lt;/div&gt;
&lt;p&gt;And the gradient &lt;span class="math"&gt;\(d\textbf{b}^{[2]}\)&lt;/span&gt; is therefore just the transpose of the Jacobian, or:&lt;/p&gt;
&lt;div class="math"&gt;$$
d\textbf{b}^{[2]} = \boldsymbol{\delta}^{[2]}
$$&lt;/div&gt;
&lt;p&gt;Which is a &lt;span class="math"&gt;\((2,1)\)&lt;/span&gt; column vector, and matches the dimensions of &lt;span class="math"&gt;\(\textbf{b}^{[2]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Great! So we calculated two more gradient matricies (four in total), and we still have two more to go.&lt;/p&gt;
&lt;div class="math"&gt;$$\bigg( d\textbf{W}^{[1]}, d\textbf{b}^{[1]}, d\textbf{W}^{[2]}, d\textbf{b}^{[2]}, 
d\textbf{W}^{[3]}, db^{[3]} \bigg)$$&lt;/div&gt;
&lt;p&gt;We also said that by breaking down all the gradient matricies into their respective partial derivatives, we needed to solve for 29. We solved for &lt;span class="math"&gt;\(3\)&lt;/span&gt; in layer 3, and &lt;span class="math"&gt;\(8 + 2 = 10\)&lt;/span&gt; in the second layer. 16 more to go!&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;Calculating &lt;span class="math"&gt;\(d\textbf{W}^{[1]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(d\textbf{b}^{[1]}\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;So by this point you should know the drill. In order to calculate our gradient matricies, we need to calculate the Jacobian Matricies of &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{W}^{[1]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{b}^{[1]}\)&lt;/span&gt; &lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial{J}}{\partial{\textbf{W}^{[1]}}} = 
\dfrac{d{J}}{d{\textbf{z}^{[2]}}}
\dfrac{d{\textbf{z}^{[2]}}}{d{\textbf{a}^{[1]}}}
\dfrac{d\textbf{a}^{[1]}}{d{\textbf{z}^{[1]}}}
\dfrac{\partial{\textbf{z}^{[1]}}}{\partial{\textbf{W}^{[1]}}}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial{J}}{\partial{\textbf{b}^{[1]}}} = 
\dfrac{d{J}}{d{\textbf{z}^{[2]}}}
\dfrac{d{\textbf{z}^{[2]}}}{d{\textbf{a}^{[1]}}}
\dfrac{d{\textbf{a}^{[1]}}}{d{\textbf{a}^{[1]}}}
\dfrac{\partial{\textbf{z}^{[1]}}}{\partial{\textbf{b}^{[1]}}}
$$&lt;/div&gt;
&lt;p&gt;We already know that:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{d{J}}{d{\mathbf{z}^{[2]}}} = \delta^{[2]T}$$&lt;/div&gt;
&lt;p&gt;Which is a &lt;span class="math"&gt;\((1,2)\)&lt;/span&gt; matrix.&lt;/p&gt;
&lt;p&gt;We are going to lump a couple of derivatives together.&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{d{\textbf{z}^{[2]}}}{d{\textbf{z}^{[1]}}} =
\dfrac{d{\textbf{z}^{[2]}}}{d{\textbf{a}^{[1]}}}
\dfrac{d\textbf{a}^{[1]}}{d{\textbf{z}^{[1]}}}
$$&lt;/div&gt;
&lt;p&gt;And just solve for the Jacobian matrix of &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt;. &lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{d{\textbf{z}^{[2]}}}{d{\textbf{z}^{[1]}}} = 
\begin{bmatrix}
\\
    \dfrac{\partial{z^{[2]}_{11}}}{\partial{z^{[1]}_{11}}} &amp;amp; 
    \dfrac{\partial{z^{[2]}_{11}}}{\partial{z^{[1]}_{21}}} &amp;amp;
    \dfrac{\partial{z^{[2]}_{11}}}{\partial{z^{[1]}_{31}}} &amp;amp;
    \dfrac{\partial{z^{[2]}_{11}}}{\partial{z^{[1]}_{41}}} \\\\
    \dfrac{\partial{z^{[2]}_{21}}}{\partial{z^{[1]}_{11}}} &amp;amp; 
    \dfrac{\partial{z^{[2]}_{21}}}{\partial{z^{[1]}_{21}}} &amp;amp;
    \dfrac{\partial{z^{[2]}_{21}}}{\partial{z^{[1]}_{31}}} &amp;amp;
    \dfrac{\partial{z^{[2]}_{21}}}{\partial{z^{[1]}_{41}}} \\\\
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;With dimensions &lt;span class="math"&gt;\((2,4)\)&lt;/span&gt;. Again, the first dimension matches the number of entries in &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; and the second dimension mathces the number of entries in &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt;  Similar to the previous layer, when we plug in values we get this matrix:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{d{\textbf{z}^{[2]}}}{d{\textbf{z}^{[1]}}} = 
\begin{bmatrix}\\
    W^{[2]}_{11} &amp;amp; 
    \dotsc &amp;amp;
    W^{[2]}_{14}\\\\
    W^{[2]}_{21} &amp;amp; 
    \dotsc &amp;amp;
    W^{[2]}_{24} \\\\
\end{bmatrix}
\begin{bmatrix}\\
    g'(z^{[1]}_{11}) &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\\
    0 &amp;amp; g'(z^{[1]}_{21}) &amp;amp; 0 &amp;amp; 0 \\\\
    0 &amp;amp; 0 &amp;amp; g'(z^{[1]}_{31}) &amp;amp; 0 \\\\
    0 &amp;amp; 0 &amp;amp; 0 &amp;amp;g'(z^{[1]}_{41}) \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Alright, now we need to calculate the partial derivative of &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{W}^{[1]}\)&lt;/span&gt;. Like last time, we can construct the Jacobian matrix:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial \textbf{z}^{[1]}}{\partial \textbf{W}^{[1]}} = 
\begin{bmatrix}
\dfrac{\partial z^{[1]}_{11}}{\partial W^{[1]}_{11}} &amp;amp;
\dotsc &amp;amp; 
\dfrac{\partial z^{[1]}_{11}}{\partial W^{[1]}_{43}} \\\\
\dfrac{\partial z^{[1]}_{21}}{\partial W^{[1]}_{11}} &amp;amp;
\dotsc &amp;amp; 
\dfrac{\partial z^{[1]}_{21}}{\partial W^{[1]}_{43}} \\\\
\dfrac{\partial z^{[1]}_{31}}{\partial W^{[1]}_{11}} &amp;amp;
\dotsc &amp;amp; 
\dfrac{\partial z^{[1]}_{31}}{\partial W^{[1]}_{43}} \\\\
\dfrac{\partial z^{[1]}_{41}}{\partial W^{[1]}_{11}} &amp;amp;
\dotsc &amp;amp; 
\dfrac{\partial z^{[1]}_{41}}{\partial W^{[1]}_{43}} \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;So the Jacobian matrix has dimensions &lt;span class="math"&gt;\((4, 12)\)&lt;/span&gt;, which is equal to the number of entries in &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt; and the number of entries in &lt;span class="math"&gt;\(\textbf{W}^{[1]}\)&lt;/span&gt; &lt;span class="math"&gt;\((4*3 = 12)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;And similar to our previous calculation, it simplifies to become:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial \textbf{z}^{[1]}}{\partial \textbf{W}^{[1]}} = 
\begin{bmatrix}
x_{11} &amp;amp;
x_{21} &amp;amp; 
x_{31} &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 \\\\
0 &amp;amp;
0 &amp;amp; 
0 &amp;amp;
x_{11} &amp;amp;
x_{21} &amp;amp; 
x_{31} &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 \\\\
0 &amp;amp;
0 &amp;amp; 
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
x_{11} &amp;amp;
x_{21} &amp;amp; 
x_{31} &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 \\\\
0 &amp;amp;
0 &amp;amp; 
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
x_{11} &amp;amp;
x_{21} &amp;amp; 
x_{31} &amp;amp; \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Like last time, the Jacobian matrix of &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{b}^{[1]}\)&lt;/span&gt; is equal to the identity matrix:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial \textbf{z}^{[1]}}{\partial \textbf{b}^{[1]}} = 
\begin{bmatrix}
\\
    1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\\
    0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \\\\
    0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 \\\\
    0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 \\\\
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;The dimensions &lt;span class="math"&gt;\((4,4)\)&lt;/span&gt; of the Jacobian matrix are equal to the number of entries in &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{b}^{[1]}\)&lt;/span&gt;, respectively.&lt;/p&gt;
&lt;p&gt;Now we have the intermediate pieces to calculate the partial derivative of &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{W}^{[1]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{b}^{[1]}\)&lt;/span&gt;, which will eventually allow us to calculate the gradient matrix.&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial{J}}{\partial{\textbf{W}^{[1]}}} = 
\dfrac{d{J}}{d{\textbf{z}^{[2]}}}
\dfrac{d{\textbf{z}^{[2]}}}{d{\textbf{z}^{[1]}}}
\dfrac{\partial{\textbf{z}^{[1]}}}{\partial{\textbf{W}^{[1]}}}
$$&lt;/div&gt;
&lt;h3&gt;&lt;/h3&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial{J}}{\partial{\textbf{b}^{[1]}}} = 
\dfrac{d{J}}{d{\textbf{z}^{[2]}}}
\dfrac{d{\textbf{z}^{[2]}}}{d{\textbf{z}^{[1]}}}
\dfrac{\partial{\textbf{z}^{[1]}}}{\partial{\textbf{b}^{[1]}}}
$$&lt;/div&gt;
&lt;p&gt;Let's start by calculating the derivative of &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{d{J}}{d{\textbf{z}^{[1]}}} = 
\dfrac{d{J}}{d{\textbf{z}^{[2]}}}
\dfrac{d{\textbf{z}^{[2]}}}{d{\textbf{z}^{[1]}}}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\dfrac{d{J}}{d{\mathbf{z}^{[2]}}} = 
\boldsymbol{\delta}^{[2]T}
\dfrac{d{\textbf{z}^{[2]}}}{d{\textbf{z}^{[1]}}}$$&lt;/div&gt;
&lt;p&gt;So this is the Jacobian Matrix of &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt;. Let's figure out the dimensions for this matrix based on the dimensions of its components. &lt;span class="math"&gt;\(\boldsymbol{\delta}^{[2]T}\)&lt;/span&gt; has dimensions (1, 2), and the derivative of &lt;span class="math"&gt;\(\textbf{z}^{[2]}\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt; has dimensions &lt;span class="math"&gt;\((2,4)\)&lt;/span&gt;. Doing the matrix multiply, we get a matrix of dimensions &lt;span class="math"&gt;\((1,4)\)&lt;/span&gt;, which matches what we would expect. &lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{dJ}{d{\textbf{z}^{[1]}}} = 
\boldsymbol{\delta}^{[2]T}
\begin{bmatrix}\\
    W^{[2]}_{11} &amp;amp; 
    \dotsc &amp;amp;
    W^{[2]}_{14} \\\\
    W^{[2]}_{21} &amp;amp; 
    \dotsc &amp;amp;
    W^{[2]}_{24} \\\\
\end{bmatrix}
\begin{bmatrix}\\
    g'(z^{[1]}_{11}) &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\\
    0 &amp;amp; g'(z^{[1]}_{21}) &amp;amp; 0 &amp;amp; 0 \\\\
    0 &amp;amp; 0 &amp;amp; g'(z^{[1]}_{31}) &amp;amp; 0 \\\\
    0 &amp;amp; 0 &amp;amp; 0 &amp;amp;g'(z^{[1]}_{41}) \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;To calculate the gradient of the cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{z}^{[1]}\)&lt;/span&gt;, we need to take the transpose of the Jacobian matrix.&lt;/p&gt;
&lt;div class="math"&gt;$$
\boldsymbol{\delta}^{[1]} = 
\begin{bmatrix}\\
    g'(z^{[1]}_{11}) &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\\
    0 &amp;amp; g'(z^{[1]}_{21}) &amp;amp; 0 &amp;amp; 0 \\\\
    0 &amp;amp; 0 &amp;amp; g'(z^{[1]}_{31}) &amp;amp; 0 \\\\
    0 &amp;amp; 0 &amp;amp; 0 &amp;amp;g'(z^{[1]}_{41}) \\\\
\end{bmatrix}
\begin{bmatrix}\\
    W^{[2]}_{11} &amp;amp; 
    W^{[2]}_{21} \\\\
    \vdots &amp;amp; \vdots \\\\
    W^{[2]}_{14} &amp;amp; 
    W^{[2]}_{24} \\\\
\end{bmatrix}
\begin{bmatrix}\\
    \delta^{[1]}_{11} \\\\
    \delta^{[1]}_{21} \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
 = 
\begin{bmatrix}\\
    g'(z^{[1]}_{11})W^{[2]}_{11} &amp;amp; 
    g'(z^{[1]}_{11})W^{[2]}_{21} \\\\
    \vdots &amp;amp; \vdots \\\\
    g'(z^{[1]}_{41})W^{[2]}_{14} &amp;amp; 
    g'(z^{[1]}_{41})W^{[2]}_{24} \\\\
\end{bmatrix}
\begin{bmatrix}\\
    \delta^{[1]}_{11} \\\\
    \delta^{[1]}_{21} \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
 = 
\begin{bmatrix}\\
    g'(z^{[1]}_{11})W^{[2]}_{11}\delta^{[1]}_{11} + 
    g'(z^{[1]}_{11})W^{[2]}_{21}\delta^{[1]}_{21} \\\\
    \vdots &amp;amp; \\\\
    g'(z^{[1]}_{41})W^{[2]}_{14}\delta^{[1]}_{11} + 
    g'(z^{[1]}_{41})W^{[2]}_{24}\delta^{[1]}_{21} \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
 = 
\begin{bmatrix}\\
    g'(z^{[1]}_{11})(W^{[2]}_{11}\delta^{[1]}_{11} + 
    W^{[2]}_{21}\delta^{[1]}_{21}) \\\\
    \vdots &amp;amp; \\\\
    g'(z^{[1]}_{41})(W^{[2]}_{14}\delta^{[1]}_{11} + 
    W^{[2]}_{24}\delta^{[1]}_{21}) \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
 = 
\begin{bmatrix}\\
    W^{[2]}_{11}\delta^{[1]}_{11} + 
    W^{[2]}_{21}\delta^{[1]}_{12} \\\\
    \vdots &amp;amp; \\\\
    W^{[2]}_{14}\delta^{[1]}_{11} + 
    W^{[2]}_{24}\delta^{[1]}_{12} \\\\
\end{bmatrix}
\begin{bmatrix}\\
    g'(z^{[1]}_{11}) \\\\
    \vdots &amp;amp; \\\\
    g'(z^{[1]}_{41}) \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
 = 
\begin{bmatrix}\\
    W^{[2]}_{11}\delta^{[1]}_{11} + 
    W^{[2]}_{21}\delta^{[1]}_{12} \\\\
    \vdots &amp;amp; \\\\
    W^{[2]}_{14}\delta^{[1]}_{11} + 
    W^{[2]}_{24}\delta^{[1]}_{12} \\\\
\end{bmatrix}
* g'(\textbf{z}^{[1]})
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
 = 
\begin{bmatrix}\\
    W^{[2]}_{11} &amp;amp; 
    W^{[2]}_{21} \\\\
    \vdots &amp;amp; \vdots \\\\
    W^{[2]}_{14} &amp;amp; 
    W^{[2]}_{24} \\\\
\end{bmatrix}
\begin{bmatrix}\\
    \delta^{[1]}_{11} \\\\
    \delta^{[1]}_{12} \\\\
\end{bmatrix}
* g'(\textbf{z}^{[1]})
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\boldsymbol{\delta}^{[1]} = 
\textbf{W}^{[2]T}\boldsymbol{\delta}^{[1]}
* g'(\textbf{z}^{[1]})
$$&lt;/div&gt;
&lt;p&gt;Great, now we can use &lt;span class="math"&gt;\(\boldsymbol{\delta}^{[1]}\)&lt;/span&gt; to calculate the derivative of &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{W}^{[1]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial{J}}{\partial{\textbf{W}^{[1]}}} = 
\boldsymbol{\delta}^{[1]T}
\dfrac{\partial{\textbf{z}^{[1]}}}{\partial{\textbf{W}^{[1]}}}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial{J}}{\partial{\textbf{W}^{[1]}}} = 
\begin{bmatrix}
\delta^{[1]}_{11} &amp;amp; \delta^{[1]}_{21} &amp;amp; \delta^{[1]}_{31} &amp;amp; \delta^{[1]}_{41}
\end{bmatrix}
\begin{bmatrix}
x_{11} &amp;amp;
x_{21} &amp;amp; 
x_{31} &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 \\\\
0 &amp;amp;
0 &amp;amp; 
0 &amp;amp;
x_{11} &amp;amp;
x_{21} &amp;amp; 
x_{31} &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 \\\\
0 &amp;amp;
0 &amp;amp; 
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
x_{11} &amp;amp;
x_{21} &amp;amp; 
x_{31} &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 \\\\
0 &amp;amp;
0 &amp;amp; 
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
0 &amp;amp;
x_{11} &amp;amp;
x_{21} &amp;amp; 
x_{31} &amp;amp; \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\dfrac{\partial{J}}{\partial{\textbf{W}^{[1]}}} =
\begin{bmatrix}
\delta^{[1]}_{11}x_{11} &amp;amp; 
\delta^{[1]}_{11}x_{21} &amp;amp;
\delta^{[1]}_{11}x_{31} &amp;amp;
\dotsc
\delta^{[1]}_{41}x_{11} &amp;amp; 
\delta^{[1]}_{41}x_{21} &amp;amp;
\delta^{[1]}_{41}x_{31} 
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Which yields a &lt;span class="math"&gt;\((1, 12)\)&lt;/span&gt; Jacobian matrix. Like before, we need our gradient matrix &lt;span class="math"&gt;\(d\textbf{W}^{[1]}\)&lt;/span&gt; to have dimensions that match &lt;span class="math"&gt;\(\textbf{W}^{[1]}\)&lt;/span&gt;. So we will reshape the Jacobian into a &lt;span class="math"&gt;\((4,3)\)&lt;/span&gt; matrix.&lt;/p&gt;
&lt;div class="math"&gt;$$d\textbf{W}^{[1]} = 
\begin{bmatrix}
\\
\delta^{[1]}_{11}x_{11} &amp;amp; 
\dotsc &amp;amp;
\delta^{[1]}_{11}x_{31} \\\\
\vdots &amp;amp; \vdots \\\\
\delta^{[1]}_{41}x_{11} &amp;amp; 
\dotsc &amp;amp; 
\delta^{[1]}_{41}x_{31} \\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;And like last time, this breaks apart into two matricies and we get:&lt;/p&gt;
&lt;div class="math"&gt;$$d\textbf{W}^{[1]} = 
\begin{bmatrix}
\\
\delta^{[1]}_{11} \\\\
\delta^{[1]}_{21} \\\\
\delta^{[1]}_{31} \\\\
\delta^{[1]}_{41} \\\\
\end{bmatrix}
\begin{bmatrix}
\\
x_{11} &amp;amp; x_{21} &amp;amp;  x_{31}\\\\
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;Which becomes:&lt;/p&gt;
&lt;div class="math"&gt;$$d\textbf{W}^{[1]} = \boldsymbol{\delta}^{[1]}\textbf{x}^{T}$$&lt;/div&gt;
&lt;p&gt;Let's now calculate the partial derivative of the loss &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to &lt;span class="math"&gt;\(\textbf{b}^{[1]}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial J}{\partial{\textbf{b}^{[1]}}} = \dfrac{dJ}{d{\textbf{z}^{[1]}}}
\dfrac{\partial{\textbf{z}^{[1]}}}{\partial{\textbf{b}^{[1]}}}$$&lt;/div&gt;
&lt;p&gt;We know that:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial \textbf{z}^{[1]}}{\partial \textbf{b}^{[1]}} = 
\begin{bmatrix}
    \\
    1 &amp;amp; 0 &amp;amp; 0 &amp;amp; 0 \\\\
    0 &amp;amp; 1 &amp;amp; 0 &amp;amp; 0 \\\\
    0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0 \\\\
    0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 \\\\
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;Which is just the identity matrix. So the partial derivative just simplfies to become:&lt;/p&gt;
&lt;div class="math"&gt;$$\dfrac{\partial{J}}{\partial{\textbf{b}^{[1]}}} = \boldsymbol{\delta}^{[1]T}$$&lt;/div&gt;
&lt;p&gt;And the gradient &lt;span class="math"&gt;\(d\textbf{b}^{[1]}\)&lt;/span&gt; is therefore just the transpose of the Jacobian, or:&lt;/p&gt;
&lt;div class="math"&gt;$$
d\textbf{b}^{[1]} = \boldsymbol{\delta}^{[1]}
$$&lt;/div&gt;
&lt;p&gt;Which is a &lt;span class="math"&gt;\((4,1)\)&lt;/span&gt; column vector, and matches the dimensions of &lt;span class="math"&gt;\(\textbf{b}^{[1]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h2&gt;In Summary&lt;/h2&gt;
&lt;p&gt;Our aim was to calculate the gradients for our cost function &lt;span class="math"&gt;\(J\)&lt;/span&gt; with respect to each of our parameters that we wanted to update using stochastic gradient descent.&lt;/p&gt;
&lt;div class="math"&gt;$$\bigg( d\textbf{W}^{[1]}, d\textbf{b}^{[1]}, d\textbf{W}^{[2]}, d\textbf{b}^{[2]}, 
d\textbf{W}^{[3]}, db^{[3]} \bigg)$$&lt;/div&gt;
&lt;p&gt;Below is a summary of the results:&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h4&gt;Third Layer&lt;/h4&gt;
&lt;div class="math"&gt;$$d\textbf{W}^{[3]} =  \delta^{[3]}a^{[2]T}$$&lt;/div&gt;
&lt;div class="math"&gt;$$d b^{[3]} = 
\delta^{[3]}$$&lt;/div&gt;
&lt;div class="math"&gt;$$\boldsymbol{\delta}^{[3]} = a^{[3]} - y$$&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h4&gt;Second Layer&lt;/h4&gt;
&lt;div class="math"&gt;$$d\textbf{W}^{[2]} = \boldsymbol{\delta}^{[2]}\textbf{a}^{[1]T}$$&lt;/div&gt;
&lt;div class="math"&gt;$$d\textbf{b}^{[2]} = \boldsymbol{\delta}^{[2]}$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\boldsymbol{\delta}^{[2]}= 
\textbf{W}^{[3]T}
\delta^{[3]}
*
g'(\textbf{z}^{[2]})
$$&lt;/div&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h4&gt;First Layer&lt;/h4&gt;
&lt;div class="math"&gt;$$d\textbf{W}^{[1]} = \boldsymbol{\delta}^{[1]}\textbf{x}^{T}$$&lt;/div&gt;
&lt;div class="math"&gt;$$d\textbf{b}^{[1]} = \boldsymbol{\delta}^{[1]}$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\boldsymbol{\delta}^{[1]}= 
\textbf{W}^{[2]T}
\delta^{[2]}
*
g'(\textbf{z}^{[1]})
$$&lt;/div&gt;
&lt;p&gt;So all that work for a set of simple, predictable equations. Yes, and I think most people will just apply these equations when implementing a neural network in numpy. Or forgo manually architecting backpropagation and gradient descent and using a useful, automatic package like tensorflow. Which is unfortunate, because despite the arduous process of painstakingly producing these equations, I think I've learned a lot by doing it and I hope you, the reader have as well. Thank you!&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="neural networks"></category><category term="machine learning"></category></entry><entry><title>The Math behind Neural Networks - Forward Propagation</title><link href="http://www.jasonosajima.com/forwardprop.html" rel="alternate"></link><published>2018-07-18T00:00:00-07:00</published><updated>2018-07-18T00:00:00-07:00</updated><author><name>Jason Osajima</name></author><id>tag:www.jasonosajima.com,2018-07-18:/forwardprop.html</id><summary type="html">

&lt;p&gt;When I started learning about neural networks, I found several articles and courses that guided you through their implementation in &lt;code&gt;numpy&lt;/code&gt;. But when I started my research, I couldn't see past these basic implementations. In other words, I couldn't understand the concepts in research papers and I couldn't think of any interesting research ideas.&lt;/p&gt;
&lt;p&gt;In order to go forward I had to go backwards. I had to relearn many fundamental concepts. The two concepts that are probably the most fundamental to neural networks are forward propagation and backpropagation. I decided to write two blog posts explaining in depth how these two concepts work. My hope is that by the end of this two part series you will have a deeper understanding of the fundamental underpinnings of both.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;&lt;em&gt;This is part one in a two-part series on the math behind neural networks. Part one is about forward propagation. Part two is about backpropagation and can be found &lt;a href="/backprop"&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;


&lt;p&gt;When I started learning about neural networks, I found several articles and courses that guided you through their implementation in &lt;code&gt;numpy&lt;/code&gt;. But when I started my research, I couldn't see past these basic implementations. In other words, I couldn't understand the concepts in research papers and I couldn't think of any interesting research ideas.&lt;/p&gt;
&lt;p&gt;In order to go forward I had to go backwards. I had to relearn many fundamental concepts. The two concepts that are probably the most fundamental to neural networks are forward propagation and backpropagation. I decided to write two blog posts explaining in depth how these two concepts work. My hope is that by the end of this two part series you will have a deeper understanding of the fundamental underpinnings of both.&lt;/p&gt;


&lt;p&gt;I found three resources helpful. The first is the &lt;a href="http://cs231n.github.io/neural-networks-1/"&gt;Neural Network module&lt;/a&gt; in the Stanford CS231n Convolutional Neural Networks for Visual Recognition course. The course materials are written by &lt;a href="http://karpathy.github.io"&gt;Andrej Karpathy&lt;/a&gt;. I enjoy reading Karpathy's work. He has a great conversational tone when describing concepts and it feels like you are traveling together on a roadtrip towards a better understanding of deep learning.&lt;/p&gt;
&lt;p&gt;The second resource I would recommend is the &lt;a href="http://www.deeplearningbook.org"&gt;Deep Learning Book&lt;/a&gt; written by Ian Goodfellow, Yoshua Bengio and Aaron Courville. It is an exahaustive resource of all the facts you will need to understand deep learning. It's on my to-do list to read and take notes on the entire book. In the meantime, I have used it frequently to learn about specific concepts that I wanted more information about.&lt;/p&gt;
&lt;p&gt;The third is Andrew Ng's &lt;a href="https://www.coursera.org/specializations/deep-learning"&gt;Deep Learning Specialization&lt;/a&gt;, available on Coursera. Ng has a special talent for explaining difficult ideas in simple ways. His ability to do this comes from his insistence on clear notation. He doesn't allow any notational detail to be lost. &lt;/p&gt;
&lt;p&gt;I recommend starting with the Stanford course and then moving on to Andrew Ng's course, all the while using the Deep Learning Book as a reference.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;What is the problem we are trying to solve?&lt;/h3&gt;
&lt;p&gt;I probably don't have to convince you that neural networks have shown success in several &lt;a href="http://www.wildml.com/2017/12/ai-and-deep-learning-in-2017-a-year-in-review/"&gt;domains&lt;/a&gt;. In our example we will be focused on a binary classification problem.&lt;/p&gt;
&lt;p&gt;A binary classifier is a supervised learning algorithm. We are given an input and our task is to predict which one of two classes the input belongs to. Each training example we use can be represented as &lt;span class="math"&gt;\((\textbf{x}, y)\)&lt;/span&gt;, where &lt;span class="math"&gt;\(\textbf{x} \in \mathbb{R}^{n_x}\)&lt;/span&gt; and &lt;span class="math"&gt;\(y \in (1, 0)\)&lt;/span&gt;. If you aren't familiar with this notation, it just means that &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; is a &lt;span class="math"&gt;\(n_x\)&lt;/span&gt;-dimensional feature vector and &lt;span class="math"&gt;\(y\)&lt;/span&gt; can take on values &lt;span class="math"&gt;\(1\)&lt;/span&gt; or &lt;span class="math"&gt;\(0\)&lt;/span&gt;.  Let's say we are trying to predict whether a person was happy (&lt;span class="math"&gt;\(0\)&lt;/span&gt;) or sad (&lt;span class="math"&gt;\(1\)&lt;/span&gt;) using features (1) how much sleep the person gets (2) how many times the person exercises in a week and (3) how many times the person hangs out with friends. Since we have three features, &lt;span class="math"&gt;\(n_x = 3\)&lt;/span&gt;, and each of our &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; training examples would be a &lt;span class="math"&gt;\(3\)&lt;/span&gt;-dimensional vector. The values for each of the features can be represented with a subscript. So, &lt;span class="math"&gt;\(x_1\)&lt;/span&gt; would be the value for how much sleep a person gets, &lt;span class="math"&gt;\(x_2\)&lt;/span&gt; would be the value for how much a person exercises in a week, etc.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A quick note on notation: For these blog posts, any time we define a vector or matrix, we will bold it. Anytime we define a scalar, we will keep it normal.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;span class="math"&gt;\(m\)&lt;/span&gt; is equal to the number of training examples we have. So we end up with m pairs of training examples, and it can be written in this form:&lt;/p&gt;
&lt;div class="math"&gt;$${(\textbf{x}^{(1)}, y^{(1)}), (\textbf{x}^{(2)}, y^{(2)}), (\textbf{x}^{(3)}, y^{(3)}),\ ..., (\textbf{x}^{(m)}, y^{(m)})}$$&lt;/div&gt;
&lt;p&gt;Notice that we are using the superscript &lt;span class="math"&gt;\((i)\)&lt;/span&gt; to denote the ith training example. So the third training example is &lt;span class="math"&gt;\((\textbf{x}^{(3)}, y^{(3)})\)&lt;/span&gt;. Next, we can take all of these &lt;span class="math"&gt;\(\textbf{x}^{(i)}\)&lt;/span&gt; vectors and line them up to create a matrix like so:&lt;/p&gt;
&lt;div class="math"&gt;$$\textbf{X} = \begin{bmatrix}
    | &amp;amp; | &amp;amp; ... &amp;amp; | \\
   \textbf{x}^{(1)} &amp;amp; \textbf{x}^{(2)} &amp;amp; ... &amp;amp; \textbf{x}^{(m)} \\
   | &amp;amp; | &amp;amp; ... &amp;amp; |
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;The shape of &lt;span class="math"&gt;\(\textbf{X}\)&lt;/span&gt; is &lt;span class="math"&gt;\((n_x, m)\)&lt;/span&gt;, or &lt;span class="math"&gt;\(X \in \mathbb{R}^{n_x, m}\)&lt;/span&gt;. Each row are the values for a given feature, and each column is a training example.&lt;/p&gt;
&lt;p&gt;Similarly, we can group all of the output values &lt;span class="math"&gt;\(y\)&lt;/span&gt; for each training example into a vector:&lt;/p&gt;
&lt;div class="math"&gt;$$\textbf{Y} = 
\begin{bmatrix}
y^{(1)} &amp;amp;
y^{(2)} &amp;amp;
... &amp;amp;
y^{(m)}
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;The shape of &lt;span class="math"&gt;\(\textbf{Y}\)&lt;/span&gt; is &lt;span class="math"&gt;\((1, m)\)&lt;/span&gt; or &lt;span class="math"&gt;\(\textbf{Y} \in \mathbb{R}^{1, m}\)&lt;/span&gt;.
&amp;nbsp;&lt;/p&gt;
&lt;h2&gt;Defining the Architecture&lt;/h2&gt;
&lt;p&gt;For the two blog posts, I decided to use a neural network with three layers. A ReLU activation function connects the input and two hidden layers and a sigmoid function connects the final hidden layer and the output layer.&lt;/p&gt;
&lt;p&gt;There are a lot of great resources illustrating how forward propagation and backpropagation work for a one hidden-layer neural network or logistic regression, but I think the sweet spot for understanding both concepts occurs when you use a neural network with two hidden layers. So our example will focus on a three-layer hidden network. Here's what our lovely neural network looks like without labels.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/nn_1.png" title="[nn_1]" alt="[nn_1]"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Diagram of a Neural Network with Two Hidden Layers&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The first question that may come to mind is what does the output of the neural network represent? The input of a neural network is a feature vector from a training example (&lt;span class="math"&gt;\(\textbf{x}^{(i)}\)&lt;/span&gt;). The output is our prediction &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt;. What does &lt;span class="math"&gt;\(\hat{y}\)&lt;/span&gt; represent?&lt;/p&gt;
&lt;p&gt;Given a feature vector &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt;, we want to predict whether the training example is a 1 or 0. We can think of our prediction as the probability that &lt;span class="math"&gt;\(y\)&lt;/span&gt; is equal to 1 given the feature vector &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; for our training example, or &lt;span class="math"&gt;\(\hat{y} = P(y=1 | \textbf{x})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/nn_2.png" title="[nn_2]" alt="[nn_2]"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Diagram with input &lt;span class="math"&gt;\(\textbf{x}^{(i)}\)&lt;/span&gt; and output &lt;span class="math"&gt;\(\hat{y}^{(i)}\)&lt;/span&gt; for a given training example &lt;span class="math"&gt;\(i\)&lt;/span&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We define the first layer of the neural network as a feature vector from a given training example (&lt;span class="math"&gt;\(i\)&lt;/span&gt;). In the diagram, each entry in the feature vector represents a scalar value. For example, &lt;span class="math"&gt;\(x^{(i)}_1\)&lt;/span&gt; is the value for the 1st feature for the ith training example.&lt;/p&gt;
&lt;p&gt;Notice that the last layer of our neural network contains &lt;span class="math"&gt;\(\hat{y}^{(i)}\)&lt;/span&gt;, which is our prediction for what we think the label should be for the ith training example.&lt;/p&gt;
&lt;p&gt;Notice that our neural network has 4 layers of nodes, but we said in the beginning that our neural network has 2 hidden layers. What's the reasoning behind this? We treat our output and input layers as layers, so technically &lt;span class="math"&gt;\(x^{(i)} = a^{(i)[0]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(a^{(i)[3]} = \hat{y}^{(i)}\)&lt;/span&gt;. We define &lt;span class="math"&gt;\(a\)&lt;/span&gt; as a vector for the given layer, and the superscript &lt;span class="math"&gt;\([j]\)&lt;/span&gt; tells us the layer number, so the input layer is the 0th layer and the output layer is the 3rd layer. Given that, how do we describe the hidden layers in between?&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/nn_3.png" title="[nn_3]" alt="[nn_3]"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Diagram with hidden layers.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In our diagram, we now have hidden layers &lt;span class="math"&gt;\(\textbf{a}^{(i)[1]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{a}^{(i)[2]}\)&lt;/span&gt;, and output layer &lt;span class="math"&gt;\(\textbf{a}^{(i)[3]} = \hat{y}^{(i)}\)&lt;/span&gt; represented and our 3 layer hidden network is defined in our diagram. This confused me from the beginning because it's a 3-layer Neural Network with 2 hidden layers. So the number of hidden layers is number of layers - 1, since we count the output as a layer.&lt;/p&gt;
&lt;p&gt;We can also vectorize our hidden layers the same way we vectorized our input (&lt;span class="math"&gt;\(\textbf{X}\)&lt;/span&gt;) and output (&lt;span class="math"&gt;\(\hat{\textbf{Y}}\)&lt;/span&gt;) by lining up the vectors for a hidden layer &lt;span class="math"&gt;\(j\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$\textbf{A}^{[j]} = \begin{bmatrix}
    | &amp;amp; | &amp;amp; ... &amp;amp; | \\
   \textbf{a}^{(1)[j]} &amp;amp; \textbf{a}^{(2)[j]} &amp;amp; ... &amp;amp; \textbf{a}^{(m)[j]} \\
   | &amp;amp; | &amp;amp; ... &amp;amp; |
\end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;This matrix &lt;span class="math"&gt;\(\textbf{A}^{[j]}\)&lt;/span&gt; becomes a &lt;span class="math"&gt;\(n^{\textbf{a}^{[j]}}\)&lt;/span&gt; by &lt;span class="math"&gt;\(m\)&lt;/span&gt; matrix, where &lt;span class="math"&gt;\(n^{a^{[j]}} =\)&lt;/span&gt; # of hidden units (or nodes) for layer j and &lt;span class="math"&gt;\(m\)&lt;/span&gt; is the number of training examples. Relating this back to our diagram, if &lt;span class="math"&gt;\(j = 1\)&lt;/span&gt;, &lt;span class="math"&gt;\(n^{a^{[1]}} = 4\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{A}^{[1]}\)&lt;/span&gt; has the shape  &lt;span class="math"&gt;\((4,m)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;What's interesting about the diagram (and something I didn't understand at first) is that it doesn't show any of the parameters for the model. The parameters are actually represented by the edges of the model. I'll talk about this next.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h2&gt;Going from layer to layer&lt;/h2&gt;
&lt;p&gt;Let's break down what's happening when we calculate &lt;span class="math"&gt;\(a_1^{(i)[1]}\)&lt;/span&gt;, which is the first entry for the first hidden layer for the &lt;span class="math"&gt;\(ith\)&lt;/span&gt; training example.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/nn_4.png" title="[nn_4]" alt="[nn_4]"&gt;&lt;/p&gt;
&lt;p&gt;From the diagram, you can see that the input consists of all the entries from the previous layer (in this case the input layer from the &lt;span class="math"&gt;\(ith\)&lt;/span&gt; training example &lt;span class="math"&gt;\(\textbf{x}^{(i)}\)&lt;/span&gt; and the output is the entry for the first hidden layer &lt;span class="math"&gt;\(a_1^{(i)[1]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/nn_5.png" title="[nn_5]" alt="[nn_5]"&gt;&lt;/p&gt;
&lt;p&gt;In order to calculate &lt;span class="math"&gt;\(a_1^{(i)[1]}\)&lt;/span&gt;, we take each entry from &lt;span class="math"&gt;\(\textbf{x}^{(i)}\)&lt;/span&gt; and multiply it by a weight. The notation can be a little tricky, so let's break that down. Let's say we have &lt;span class="math"&gt;\(W^{[1]}_{13}\)&lt;/span&gt;. We multiply this guy by the third entry in &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt;, or &lt;span class="math"&gt;\(x^{(i)}_3\)&lt;/span&gt; to get the first entry in the &lt;span class="math"&gt;\(1st\)&lt;/span&gt; layer.&lt;/p&gt;
&lt;p&gt;Let's breakdown the weights corresponding to &lt;span class="math"&gt;\(a_1^{(i)[1]}\)&lt;/span&gt; in our diagram. &lt;span class="math"&gt;\(\textbf{W}^{[1]}\)&lt;/span&gt; is a &lt;span class="math"&gt;\((4, 3)\)&lt;/span&gt; matrix:&lt;/p&gt;
&lt;div class="math"&gt;$$
\textbf{W}^{[1]} = 
\begin{bmatrix}
W^{[1]}_{11} &amp;amp; 
W^{[1]}_{12} &amp;amp;
W^{[1]}_{13} \\\\
W^{[1]}_{21} &amp;amp; 
W^{[1]}_{22} &amp;amp;
W^{[1]}_{23} \\\\
W^{[1]}_{31} &amp;amp; 
W^{[1]}_{32} &amp;amp;
W^{[1]}_{33} \\\\
W^{[1]}_{41} &amp;amp; 
W^{[1]}_{42} &amp;amp;
W^{[1]}_{43}
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;The weights that we use to calculate &lt;span class="math"&gt;\(a_1^{(i)[1]}\)&lt;/span&gt; are the weights in the first row, or &lt;span class="math"&gt;\(\textbf{W}^{[1]}_{1-}\)&lt;/span&gt;. &lt;span class="math"&gt;\(\textbf{W}^{[1]}_{1-}\)&lt;/span&gt; is a &lt;span class="math"&gt;\((1, 3)\)&lt;/span&gt; row vector:&lt;/p&gt;
&lt;div class="math"&gt;$$
\textbf{W}^{[1]}_{1-} = 
\begin{bmatrix}
W^{[1]}_{11} &amp;amp; 
W^{[1]}_{12} &amp;amp;
W^{[1]}_{13}
\end{bmatrix}
$$&lt;/div&gt;
&lt;p&gt;We can then multiply this vector by &lt;span class="math"&gt;\(\textbf{x}^{(i)}\)&lt;/span&gt; and we get a nicer, compact representation:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/nn_6.png" title="[nn_6]" alt="[nn_6]"&gt;&lt;/p&gt;
&lt;p&gt;Note that we add a bias &lt;span class="math"&gt;\(b^{[1]}_{1}\)&lt;/span&gt; to &lt;span class="math"&gt;\(\textbf{W}^{[1]}_{1-}\textbf{x}^{(i)}\)&lt;/span&gt;. The bias are other parameters besides the weights that our model learns. Why do we add a bias? In order to answer that, let's talk about our activation function &lt;span class="math"&gt;\(g()\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Different neural network architectures make different choices for activation functions, but in our example to keep it simple we will use a rectified linear unit, or ReLU function.&lt;/p&gt;
&lt;p&gt;The ReLU function is defined as the following:&lt;/p&gt;
&lt;div class="math"&gt;$$
g(z) = \begin{cases}
   z &amp;amp;\text{if } z &amp;gt; 0  \\
   0 &amp;amp;\text{otherwise}
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;What is the role of activation functions in Neural Networks? Activation functions introduce non-linearity into the neural network. Without activation functions, neural networks would simplify to linear functions. Let's see how that works with respect to our example. If we simplified our neural network by taking out the bias terms and activation functions, the neural network becomes:&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{y}^{(i)} = \textbf{W}^{[2]}\textbf{W}^{[1]}\textbf{A}^{(i)[0]}$$&lt;/div&gt;
&lt;p&gt;Notice that if we multiply a matrix of weights (&lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt;) by another matrix of weights (&lt;span class="math"&gt;\(\textbf{W}^{[1]}\)&lt;/span&gt;), we get one matrix of weights (&lt;span class="math"&gt;\(\textbf{W} = \textbf{W}^{[2]}\textbf{W}^{[1]}\)&lt;/span&gt;). So our example simplifies to a linear function:&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{y}^{(i)} = \textbf{W}\textbf{A}^{(i)[0]}$$&lt;/div&gt;
&lt;p&gt;Now back to our discussion about the bias term. We add a bias term in order to shift our activation function (in our case, the ReLU) to the left or right, which is usually important for learning because it makes the model more flexible.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h2&gt;Forward propagation in a 3-layer Network&lt;/h2&gt;
&lt;p&gt;Now that we discussed some of the elements of a 3-layer network, let's (finally) introduce the concept of forward propagation. &lt;/p&gt;
&lt;p&gt;Forward propagation is basically the process of taking some feature vector &lt;span class="math"&gt;\(\textbf{x}^{(i)}\)&lt;/span&gt; and getting an output &lt;span class="math"&gt;\(\hat{y}^{(i)}\)&lt;/span&gt;. Let's breakdown what's happening in our example.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/nn_8.png" title="[nn_8]" alt="[nn_8]"&gt;&lt;/p&gt;
&lt;p&gt;As you can see, we take a (3 x 1) training example &lt;span class="math"&gt;\(\textbf{x}^{(i)}\)&lt;/span&gt;, get the (4 x 1) activations from the first hidden layer &lt;span class="math"&gt;\(\textbf{a}^{(i)[1]}\)&lt;/span&gt;. Next, we get the (1 x 2) activations from the second hidden layer &lt;span class="math"&gt;\(\textbf{a}^{(i)[2]}\)&lt;/span&gt; and the final (1 x 1) output &lt;span class="math"&gt;\(\hat{y}^{(i)}\)&lt;/span&gt;. As we mentioned earlier, &lt;span class="math"&gt;\(\hat{y}^{(i)}\)&lt;/span&gt; is the probability that &lt;span class="math"&gt;\(y^{(i)}\)&lt;/span&gt; is of the positive class given the information we know in the form of &lt;span class="math"&gt;\(\textbf{x}^{(i)}\)&lt;/span&gt;, or &lt;span class="math"&gt;\(\hat{y}^{(i)} = P(y^{(i)} = 1 | x^{(i)})\)&lt;/span&gt;. In summary, forward propagation looks like this:&lt;/p&gt;
&lt;div class="math"&gt;$$\textbf{x}^{(i)} \rightarrow \textbf{a}^{(i)[1]} \rightarrow \textbf{a}^{(i)[2]} \rightarrow \hat{y}^{(i)}$$&lt;/div&gt;
&lt;p&gt;Next, let's discuss the inner-workings of each of these transitions.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;Input &lt;span class="math"&gt;\(\rightarrow\)&lt;/span&gt; 1st Hidden Layer (&lt;span class="math"&gt;\(\textbf{x}^{(i)} \rightarrow \textbf{a}^{(i)[1]}\)&lt;/span&gt;)&lt;/h3&gt;
&lt;p&gt;First, what's happening when we transition from our vector of features for the first training example &lt;span class="math"&gt;\(i\)&lt;/span&gt; to the activations from our first hidden layer. We start by multiplying &lt;span class="math"&gt;\(\textbf{x}^{(i)}\)&lt;/span&gt; by the weights and bias of the first hidden layer, &lt;span class="math"&gt;\(\textbf{W}^{[1]}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{b}^{[1]}\)&lt;/span&gt; to get &lt;span class="math"&gt;\(\textbf{z}^{(i)[1]}\)&lt;/span&gt;. People sometimes call &lt;span class="math"&gt;\(\textbf{z}^{(i)[1]}\)&lt;/span&gt; the activity of the hidden layer 1 for training example &lt;span class="math"&gt;\(i\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;$$\textbf{z}^{(i)[1]} = \textbf{W}^{[1]}\textbf{x}^{(i)} + \textbf{b}^{[1]}$$&lt;/div&gt;
&lt;p&gt;So we start by multiplying &lt;span class="math"&gt;\(\textbf{x}^{(i)}\)&lt;/span&gt; by &lt;span class="math"&gt;\(\textbf{W}^{[1]}\)&lt;/span&gt;. &lt;span class="math"&gt;\(\textbf{x}^{(i)}\)&lt;/span&gt; is a (3 x 1) matrix, and &lt;span class="math"&gt;\(\textbf{W}^{[1]}\)&lt;/span&gt; is a (4 x 3) matrix. We then add the bias, &lt;span class="math"&gt;\(\textbf{b}^{[1]}\)&lt;/span&gt;. The dimensions of the bias &lt;span class="math"&gt;\(\textbf{b}^{[1]}\)&lt;/span&gt; match the dimensions of &lt;span class="math"&gt;\(\textbf{z}^{(i)[1]}\)&lt;/span&gt; which are (4, 1). Once we get the activity matrix &lt;span class="math"&gt;\(\textbf{z}^{(i)[1]}\)&lt;/span&gt;, we apply the activation function to each element in &lt;span class="math"&gt;\(\textbf{z}^{(i)[1]}\)&lt;/span&gt;. Recall that the activation function that we chose is ReLU, which we defined as:&lt;/p&gt;
&lt;div class="math"&gt;$$
g(z) = \begin{cases}
   x &amp;amp;\text{if } z &amp;gt; 0  \\
   0 &amp;amp;\text{otherwise}
\end{cases}
$$&lt;/div&gt;
&lt;p&gt;So we get:&lt;/p&gt;
&lt;div class="math"&gt;$$\textbf{a}^{(i)[1]} = g(\textbf{z}^{(i)[1]})$$&lt;/div&gt;
&lt;p&gt;Which just indicates that the ReLU function &lt;span class="math"&gt;\(g()\)&lt;/span&gt; is applied elementwise to &lt;span class="math"&gt;\(\textbf{z}^{(i)[1]}\)&lt;/span&gt; to get &lt;span class="math"&gt;\(\textbf{a}^{(i)[1]}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\textbf{a}^{(i)[1]}\)&lt;/span&gt; has the same dimensions as &lt;span class="math"&gt;\(\textbf{z}^{(i)[1]}\)&lt;/span&gt;, so it's (4,1).&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;1st Hidden Layer &lt;span class="math"&gt;\(\rightarrow\)&lt;/span&gt; 2nd Hidden Layer (&lt;span class="math"&gt;\(\textbf{a}^{(i)[1]} \rightarrow \textbf{a}^{(i)[2]}\)&lt;/span&gt;)&lt;/h3&gt;
&lt;p&gt;This section is going to be almost identical to the previous section. We start by multiplying &lt;span class="math"&gt;\(\textbf{a}^{(i)[1]}\)&lt;/span&gt; by &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt;. &lt;span class="math"&gt;\(\textbf{a}^{(i)[1]}\)&lt;/span&gt; is a (4 x 1) matrix, and &lt;span class="math"&gt;\(\textbf{W}^{[2]}\)&lt;/span&gt; is a (2 x 4) matrix. We then add the bias, &lt;span class="math"&gt;\(\textbf{b}^{[2]}\)&lt;/span&gt;. The dimensions of the bias &lt;span class="math"&gt;\(\textbf{b}^{[2]}\)&lt;/span&gt; match the dimensions of &lt;span class="math"&gt;\(\textbf{z}^{(i)[2]}\)&lt;/span&gt; which are (2, 1). Once we get the activity matrix &lt;span class="math"&gt;\(\textbf{z}^{(i)[2]}\)&lt;/span&gt;, we apply the ReLU activation function to each element in &lt;span class="math"&gt;\(\textbf{z}^{(i)[2]}\)&lt;/span&gt;. So we get:&lt;/p&gt;
&lt;div class="math"&gt;$$\textbf{a}^{(i)[2]} = g(\textbf{z}^{(i)[2]})$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\textbf{a}^{(i)[2]}\)&lt;/span&gt; has the same dimensions as &lt;span class="math"&gt;\(\textbf{z}^{(i)[2]}\)&lt;/span&gt;, so it's (2,1).&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;2nd Hidden Layer &lt;span class="math"&gt;\(\rightarrow\)&lt;/span&gt; Output (&lt;span class="math"&gt;\(\textbf{a}^{(i)[2]} \rightarrow \hat{y}^{(i)}\)&lt;/span&gt;)&lt;/h3&gt;
&lt;p&gt;So now we have &lt;span class="math"&gt;\(\textbf{a}^{(i)[2]}\)&lt;/span&gt;. We again start by multiplying &lt;span class="math"&gt;\(\textbf{a}^{(i)[2]}\)&lt;/span&gt; by &lt;span class="math"&gt;\(\textbf{W}^{[3]}\)&lt;/span&gt;. &lt;span class="math"&gt;\(\textbf{a}^{(i)[2]}\)&lt;/span&gt; is a (2 x 1) matrix, and &lt;span class="math"&gt;\(\textbf{W}^{[3]}\)&lt;/span&gt; is a (1 x 2) matrix. We then add the bias, &lt;span class="math"&gt;\(b^{[3]}\)&lt;/span&gt;. The dimensions of the bias &lt;span class="math"&gt;\(b^{[3]}\)&lt;/span&gt; match the dimensions of &lt;span class="math"&gt;\(z^{(i)[3]}\)&lt;/span&gt; which are (1, 1). Once we get the activity matrix &lt;span class="math"&gt;\(z^{(i)[3]}\)&lt;/span&gt;, we apply the activation function to each element in &lt;span class="math"&gt;\(z^{(i)[3]}\)&lt;/span&gt;. Since this is the final layer of our neural network, we will use a sigmoid activation function:&lt;/p&gt;
&lt;div class="math"&gt;$$
\sigma(z) = \dfrac{1}{1+e^{-z}}
$$&lt;/div&gt;
&lt;p&gt;The sigmoid activation function is rarely used in modern neural networks because it suffers from the vanishing gradient problem, but it is often used as the final activation function before the output. The reason is that it is able to squash values to be between 0 and 1, which is what we want since recall we want &lt;span class="math"&gt;\(\hat{y}^{(i)}\)&lt;/span&gt; to be between 0 and 1 since &lt;span class="math"&gt;\(\hat{y}^{(i)} = P(y^{(i)} = 1 | x^{(i)})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;So we get:&lt;/p&gt;
&lt;div class="math"&gt;$$\hat{y}^{(i)} = a^{(i)[3]} = \sigma(z^{(i)[3]})$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\hat{y}^{(i)}\)&lt;/span&gt; has the same dimensions as &lt;span class="math"&gt;\(z^{(i)[3]}\)&lt;/span&gt;, so it's (1,1).&lt;/p&gt;
&lt;p&gt;And that's it!&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;In this blog post, I used a 3-layer neural network example to help us deconstruct the math involved in forward propagation. One of the hardest parts of this process was making sure the dimensions of all the matricies match up, so some parting thoughts on dimensions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If you think about just one training example &lt;span class="math"&gt;\(i\)&lt;/span&gt; like we did, the dimensions of activations &lt;span class="math"&gt;\(\textbf{a}\)&lt;/span&gt; will always be &lt;span class="math"&gt;\((n_{a}, 1)\)&lt;/span&gt;, where &lt;span class="math"&gt;\(n_a\)&lt;/span&gt; is equal to the number of nodes in the layer. So for example, if we had 100 nodes in the 5th hidden layer, &lt;span class="math"&gt;\(\textbf{a}^{(i)[5]}\)&lt;/span&gt; would have dimensions (100, 1).&lt;/li&gt;
&lt;li&gt;If you think about &lt;span class="math"&gt;\(m\)&lt;/span&gt; training examples, you simply switch the 2nd dimension from &lt;span class="math"&gt;\(1\)&lt;/span&gt; to &lt;span class="math"&gt;\(m\)&lt;/span&gt;. So for example, if we had 100 nodes in the 5th hidden layer, for m-training examples &lt;span class="math"&gt;\(\textbf{a}^{(i)[5]}\)&lt;/span&gt; would have dimensions (100, m).&lt;/li&gt;
&lt;li&gt;The weights &lt;span class="math"&gt;\(\textbf{W}^{l}\)&lt;/span&gt; for layer &lt;span class="math"&gt;\(l\)&lt;/span&gt; will have dimensions &lt;span class="math"&gt;\((n_{a}^{[l]}, n_{a}^{[l-1]})\)&lt;/span&gt; Notice that the weights don't care about the second dimension of activations &lt;span class="math"&gt;\(\textbf{a}^{[l]}\)&lt;/span&gt;, they just care about that 1st dimension.&lt;/li&gt;
&lt;li&gt;The final output layer is also our &lt;span class="math"&gt;\(\hat{y}^{(i)}\)&lt;/span&gt;. We count this layer when we label a neural network along with the hidden layers, so a 3-layer Neural Network will only have 2 hidden layers. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now that we have the forward propagation figured out, we can generate a prediction &lt;span class="math"&gt;\(\hat{y}^{(i)}\)&lt;/span&gt; given a feature vector for the ith training example &lt;span class="math"&gt;\(\textbf{x}^{(i)}\)&lt;/span&gt;. But is this a good prediction? How does it compare to the actual label, &lt;span class="math"&gt;\(y^{(i)}\)&lt;/span&gt;? In order to come up with a good prediction not just for the ith training example but for all examples, we need an algorithm to find the best values for our weights &lt;span class="math"&gt;\(\textbf{W}\)&lt;/span&gt; and biases &lt;span class="math"&gt;\(\textbf{b}\)&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;The most popular algorithm to use is called backpropagation, which we will discuss in the &lt;a href="/backprop"&gt;next post&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="neural networks"></category><category term="machine learning"></category></entry></feed>