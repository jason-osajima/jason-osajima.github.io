<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="author" content="Jason Osajima">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width">
        <title>The Math behind Convolutional Networks - VGG16 Backropagation | Jason {osa-jima}</title>

	<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
	<link rel="icon" href="/favicon.ico" type="image/x-icon">
        <link rel="alternate" type="application/atom+xml" title="Jason {osa-jima} blog atom feed" href="/feeds/all.atom.xml" />
        <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700' rel='stylesheet' type='text/css'>

        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
        <link rel="stylesheet" type="text/css" href="/theme/css/icons.css"/>
        <style>.highlight .hll { background-color: #ffffcc }
.highlight .c { color: #60a0b0; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #007020; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .cm { color: #60a0b0; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #007020 } /* Comment.Preproc */
.highlight .c1 { color: #60a0b0; font-style: italic } /* Comment.Single */
.highlight .cs { color: #60a0b0; background-color: #fff0f0 } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #808080 } /* Generic.Output */
.highlight .gp { color: #c65d09; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0040D0 } /* Generic.Traceback */
.highlight .kc { color: #007020; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #007020; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #007020; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #007020 } /* Keyword.Pseudo */
.highlight .kr { color: #007020; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #902000 } /* Keyword.Type */
.highlight .m { color: #40a070 } /* Literal.Number */
.highlight .s { color: #4070a0 } /* Literal.String */
.highlight .na { color: #4070a0 } /* Name.Attribute */
.highlight .nb { color: #007020 } /* Name.Builtin */
.highlight .nc { color: #0e84b5; font-weight: bold } /* Name.Class */
.highlight .no { color: #60add5 } /* Name.Constant */
.highlight .nd { color: #555555; font-weight: bold } /* Name.Decorator */
.highlight .ni { color: #d55537; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #007020 } /* Name.Exception */
.highlight .nf { color: #06287e } /* Name.Function */
.highlight .nl { color: #002070; font-weight: bold } /* Name.Label */
.highlight .nn { color: #0e84b5; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #062873; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #bb60d5 } /* Name.Variable */
.highlight .ow { color: #007020; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mf { color: #40a070 } /* Literal.Number.Float */
.highlight .mh { color: #40a070 } /* Literal.Number.Hex */
.highlight .mi { color: #40a070 } /* Literal.Number.Integer */
.highlight .mo { color: #40a070 } /* Literal.Number.Oct */
.highlight .sb { color: #4070a0 } /* Literal.String.Backtick */
.highlight .sc { color: #4070a0 } /* Literal.String.Char */
.highlight .sd { color: #4070a0; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #4070a0 } /* Literal.String.Double */
.highlight .se { color: #4070a0; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #4070a0 } /* Literal.String.Heredoc */
.highlight .si { color: #70a0d0; font-style: italic } /* Literal.String.Interpol */
.highlight .sx { color: #c65d09 } /* Literal.String.Other */
.highlight .sr { color: #235388 } /* Literal.String.Regex */
.highlight .s1 { color: #4070a0 } /* Literal.String.Single */
.highlight .ss { color: #517918 } /* Literal.String.Symbol */
.highlight .bp { color: #007020 } /* Name.Builtin.Pseudo */
.highlight .vc { color: #bb60d5 } /* Name.Variable.Class */
.highlight .vg { color: #bb60d5 } /* Name.Variable.Global */
.highlight .vi { color: #bb60d5 } /* Name.Variable.Instance */
.highlight .il { color: #40a070 } /* Literal.Number.Integer.Long */</style>
        <style>body {
  margin: 0;
  padding: 0;
  font: 15px 'Source Sans Pro', sans-serif;
  line-height: 1.6em;
  color: #222;
  text-rendering: optimizeLegibility;
  -webkit-font-smoothing: antialiased;
}
a {
  color: #007EE5;
  text-decoration: none;
}
a:hover {
  color: #007EE5;
  text-decoration: none;
}
header.main-header {
  background: none repeat scroll 0% 0% #205F29;
  margin-bottom: 0px;
}
header.main-header a {
  color: #fff;
}
header.main-header .container {
  max-width: 1000px;
}
header.main-header .container nav a:hover {
  background-color: #5C881C;
}
article {
  margin: 0;
}
article header.about {
  margin-bottom: 0px;
  padding-bottom: 0px;
}
article header {
  margin-bottom: 20px;
  padding-bottom: 20px;
}
article header h1 {
  margin-bottom: 2px;
  font-weight: 700;
  color: #000;
}
article header time {
  color: #9E9E9E;
  font-size: 0.85em;
  float: right;
}
article header time.left {
  color: #9E9E9E;
  font-size: 0.85em;
  float: left;
}
article div.social-links ul {
  padding: 0px;
}
article div.social-links li {
  display: inline;
  font-size: 20px;
}
article div.social-links li a {
  color: #000;
  padding: 10px;
}
article div.social-links li a:hover {
  color: #666;
  text-decoration: none;
}
article p {
  font-size: 16px;
  margin-bottom: 20px;
  line-height: 1.6em;
}
article p.note {
  background: #f5f5f5;
  border: 1px solid #ddd;
  padding: 0.533em 0.733em;
}
article p.update {
  background-color: #FEEFB3;
  border: 1px solid #e6e68a;
  padding: 0.533em 0.733em;
}
article p.alert {
  background-color: #ffe2e2;
  border: 1px solid #ffb2b2;
  padding: 0.533em 0.733em;
}
article ul,
article ol {
  margin-top: 0px;
  margin-bottom: 25px;
}
article li {
  font-size: 16px;
  line-height: 1.6em;
}
article a:hover {
  text-decoration: underline;
}
article blockquote {
  border-left: 2px solid #c7c7cc;
  color: #666;
  margin: 30px 0;
  padding: 0 0 0 25px;
}
article img {
  max-width: 100%;
}
article code {
  color: #333;
  background-color: #EEE;
  border-radius: 0;
  font-size: 13px;
}
article .meta {
  font-size: 11px;
}
article .meta a:hover {
  text-decoration: none;
}
article .meta div {
  margin-bottom: 20px;
  display: block;
}
article .meta a.tag {
  margin: 0 10px 10px 0;
  padding: 1px 12px;
  display: inline-block;
  font-size: 14px;
  color: rgba(0, 0, 0, 0.8);
  background: rgba(0, 0, 0, 0.05);
}
article .meta a.tag:hover {
  background: rgba(0, 0, 0, 0.15);
}
article .meta a.read_more,
article .meta a.comments_btn {
  font-size: 14px;
  font-weight: 800;
  padding: 10px 20px;
  color: #205F29;
  background: #FFF;
  border: 1px solid #205F29;
}
article .meta a.read_more:hover,
article .meta a.comments_btn:hover {
  color: #FFF;
  background: #5C881C;
}
.index {
  max-width: 700px;
}
.index article header h2 {
  font-size: 36px;
  margin-bottom: 2px;
  font-weight: 700;
}
.index article header h2 a {
  color: #000;
}
.index article header h2 a:hover {
  color: #007EE5;
  text-decoration: none;
}
.index .separator {
  padding: 40px 0 0 0;
  margin: 0 0 40px 0;
  height: 10px;
  border-bottom: solid 1px #CCC;
}
.index .pagination {
  display: block;
  margin-bottom: 100px;
}
.index .pagination .left {
  text-align: right;
}
.index .pagination .right {
  text-align: left;
}
.index .pagination a {
  display: inline-block;
  border: 2px solid #5C881C;
  margin: 0 5px;
  padding: 8px 20px;
  font-weight: bold;
  color: #5C881C;
}
.index .pagination a:hover {
  color: #FFF;
  background: #5C881C;
}
.post {
  max-width: 700px;
}
.post h2:before {
  content: "# ";
  font-weight: bold;
  color: #DDD;
}
.post h3:before {
  content: "## ";
  font-weight: bold;
  color: #DDD;
}
.post h4:before {
  content: "### ";
  font-weight: bold;
  color: #DDD;
}
.post article .meta {
  margin: 50px 0 100px;
}
.list {
  max-width: 700px;
}
.list ul.double-list {
  margin: 0 auto 60px;
  padding: 0;
  list-style-type: none;
}
.list ul.double-list li {
  padding: 5px 0;
}
.list ul.double-list li h2 {
  font-size: 1em;
  display: inline;
  font-weight: normal;
}
.list ul.double-list li span {
  font-family: sans-serif;
  text-transform: uppercase;
  text-align: right;
  float: right;
  padding-top: 3px;
  font-size: 12px;
  color: #999;
}
.full-width-content {
  padding-top: 10px;
  padding-left: 0px;
  padding-right: 0px;
  margin-left: -20px;
  margin-right: -20px;
}
.col-xs-1,
.col-sm-1,
.col-md-1,
.col-lg-1,
.col-xs-2,
.col-sm-2,
.col-md-2,
.col-lg-2,
.col-xs-3,
.col-sm-3,
.col-md-3,
.col-lg-3,
.col-xs-4,
.col-sm-4,
.col-md-4,
.col-lg-4,
.col-xs-5,
.col-sm-5,
.col-md-5,
.col-lg-5,
.col-xs-6,
.col-sm-6,
.col-md-6,
.col-lg-6,
.col-xs-7,
.col-sm-7,
.col-md-7,
.col-lg-7,
.col-xs-8,
.col-sm-8,
.col-md-8,
.col-lg-8,
.col-xs-9,
.col-sm-9,
.col-md-9,
.col-lg-9,
.col-xs-10,
.col-sm-10,
.col-md-10,
.col-lg-10,
.col-xs-11,
.col-sm-11,
.col-md-11,
.col-lg-11,
.col-xs-12,
.col-sm-12,
.col-md-12,
.col-lg-12 {
  padding-right: 0px;
  padding-left: 0px;
}</style>

	<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
	<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>


    </head>

    <body>
        <header class="navbar navbar-inverse bs-docs-nav">
            <div class="container-fluid">
                <div class="navbar-header">
		  <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#theNavbar">
		    <span class="icon-bar"></span>
		    <span class="icon-bar"></span>
		    <span class="icon-bar"></span> 
		  </button>
                  <a class="navbar-brand" href="/" title="Home" class="title">Jason {osa-jima}</a>
                </div>
                <nav class="collapse navbar-collapse bs-navbar-collapse" role="navigation" id="theNavbar">
		    <ul class="nav navbar-nav navbar-right">
                            <li><a href="/pages/about.html" title="About">About</a></li>
                    </ul>
                </nav>
            </div>
        </header>

        <div id="wrap">
<div class="container post">
    <article>
        <header>
            <h1>The Math behind Convolutional Networks - VGG16 Backropagation</h1>
            <time datetime="article.date.isoformat()" pubdate>Sun 26 August 2018</time>
        </header>

        <div class="article_content">
            <h2>Introduction</h2>
<p>In my <a href="/vgg_forwardprop">last post</a>, I broke down forward propagation for the VGG16 architecture. In this post, we will break down backpropagation for each type of layer:</p>
<ul>
<li>Softmax Layer</li>
<li>Flat Layer</li>
<li>Pooling Layers</li>
<li>Conv Layers</li>
</ul>
<p>For each layer, our objective is to calculate two things: the partial derivative of the cost function <span class="math">\(J\)</span> with respect to that layer's activations and the partial derivative of the cost function with respect to the trainable parameters associated with that layer.</p>
<p>Before we start caluclating the partial derivatives for each example layer, let's talk about the cost function <span class="math">\(J\)</span>.</p>
<h2>Understanding the Cost Function</h2>
<p>In our previous post on <a href="/backprop">backprop</a> our objective was to predict <span class="math">\(y\)</span>, which could be either <span class="math">\(0\)</span> or <span class="math">\(1\)</span>. Our prediction was a scalar <span class="math">\(\hat{y}\)</span>.</p>
<p>For the Imagenet task however, our prediction <span class="math">\(\hat{y}\)</span> is a vector with dimensions <span class="math">\((1000, 1)\)</span>. Since we are using a softmax activation in our final layer, each value corresponds with the probability we think the training example belongs to the class label.</p>
<div class="math">$$
\hat{\textbf{y}} =
\begin{bmatrix}
\hat{y}_{1} \\\\
\hat{y}_{2} \\\\ 
\vdots \\\\
\hat{y}_{i} \\\\ 
\vdots \\\\
\hat{y}_{1000}
\end{bmatrix}
$$</div>
<p>Where <span class="math">\(\hat{y}_{i} = P(y = i | \textbf{x})\)</span>. <span class="math">\(\hat{\textbf{y}}\)</span> also has the attribute that the elements in it sum to <span class="math">\(1\)</span>.</p>
<p>The loss function that we used for a single training example in our fully connected network was:</p>
<div class="math">$$\mathcal{L}(\hat{y}, y) = -ylog(\hat{y}) - (1-y)log(1 -\hat{y})$$</div>
<p>And the loss function that we use for a single training example for VGG16 is very similar:</p>
<div class="math">$$\mathcal{L}(\hat{\textbf{y}}, \textbf{y}) = -\sum_{i=1}^{1000} y_i log(\hat{y}_i)$$</div>
<p>Let's deconstruct what's happening in this loss function. Basically, for every possible image label (there are 1000 possible labels) we are calculating what is sometimes called the 'cross entropy' <span class="math">\(y_i log(\hat{y}_i)\)</span> between our prediction for that label <span class="math">\(\hat{y}_i\)</span> and the actual value <span class="math">\(y_i\)</span>. If you look at the loss function right above it, it's essentially the same as the one directly preceding but for <span class="math">\(2\)</span> instead of <span class="math">\(1000\)</span> classes. We just choose to define the second class as being <span class="math">\(1-y\)</span>, and our prediction as <span class="math">\(1-\hat{y}\)</span>.</p>
<p>We want to minimize the loss function, which means we want to maximize the cross entropy <span class="math">\(y_i log(\hat{y}_i)\)</span>. Recall that each image belongs to only <span class="math">\(1\)</span> of <span class="math">\(1000\)</span> classes. If our training example image was a dog and belonged to class <span class="math">\(5\)</span>, <span class="math">\(y_5 = 1\)</span> and all other values (<span class="math">\(y_1, ... y_4, y_6,...y_{1000}\)</span>) would be equal to <span class="math">\(0\)</span>:</p>
<div class="math">$$
\textbf{y} =
\begin{bmatrix}
0 \\\\
0 \\\\
0 \\\\
0 \\\\ 
1 \\\\ 
\vdots \\\\
0
\end{bmatrix}
$$</div>
<p>So the cross entropy for the value <span class="math">\(y_5\)</span> becomes <span class="math">\(1 * log(\hat{y}_5)\)</span> and the cross entropy for all other values become <span class="math">\(0 * log(\hat{y}_i)\)</span>. And so, in order to maximize the cross entropy, we just need to maximize <span class="math">\(\hat{y}_5\)</span>, which is the probability that given the data (pixels) of our training example image <span class="math">\(\textbf{x}\)</span> it belongs to class <span class="math">\(5\)</span>. So it makes sense that this would be our loss function, since we want to maximize the probability that our training example comes from the correct class.</p>
<p>Where did this loss function come from? In our <a href="/backprop">fully connected network example</a> we showed how we could derive our loss function for binary classification using the probability mass function for a Bernoulli distribution. We will take a similar approach and look at the probability mass function for a categorical (or multinoulli) distribution:</p>
<div class="math">$$p(\textbf{y} | \hat{\textbf{y}}, \textbf{x}) = \prod_{i=1}^{1000} \hat{y}_i^{y_i}$$</div>
<p>This basically reads as the probability that given our example <span class="math">\(\textbf{x}\)</span> and prediction <span class="math">\(\hat{\textbf{y}}\)</span> we actually have an example with labels <span class="math">\(\textbf{y}\)</span> is equal to the product of <span class="math">\(\hat{y}_i^{y_i}\)</span> for each class label <span class="math">\(i\)</span>. And if we take the log of both sides, we get:</p>
<div class="math">$$log \ p(\textbf{y} | \hat{\textbf{y}}, \textbf{x}) = log \ \prod_{i=1}^{1000} \hat{y}_i^{y_i}$$</div>
<div class="math">$$log \ p(\textbf{y} | \hat{\textbf{y}}, \textbf{x}) = \sum_{i=1}^{1000} log \ \hat{y}_i^{y_i}$$</div>
<p>And notice the right side is equal <span class="math">\(-\mathcal{L}\)</span>:</p>
<div class="math">$$log \ p(\textbf{y} | \hat{\textbf{y}}, \textbf{x}) = -\mathcal{L}(\hat{\textbf{y}}, \textbf{y}) $$</div>
<div class="math">$$\mathcal{L}(\hat{\textbf{y}}, \textbf{y}) = -log \ p(\textbf{y} | \hat{\textbf{y}}, \textbf{x})$$</div>
<p>So when we say we want to minimize <span class="math">\(\mathcal{L}\)</span>, we really mean we want to maximize the probability that <span class="math">\(\textbf{y}\)</span> is equal to it's value given our prediction <span class="math">\(\hat{\textbf{y}}\)</span> and feature vector <span class="math">\(\textbf{x}\)</span>, given that we believe <span class="math">\(\textbf{y}\)</span> belongs to a categorical (or multillouni) distribution.</p>
<blockquote>
<p>Since log increases monotonically, maximizing <span class="math">\(p(\textbf{y} | \hat{\textbf{y}}, \textbf{x})\)</span> and maximizing log \ <span class="math">\(p(\textbf{y} | \hat{\textbf{y}}, \textbf{x})\)</span> is the same. </p>
</blockquote>
<p>So this is our loss function <span class="math">\(\mathcal{L}\)</span> which we use to determine how well our model is predicting the class label for a single training example <span class="math">\(i\)</span> given the pixels of the image <span class="math">\(\textbf{x}^{(i)}\)</span>. But what if we were interested in how well our model was performing for a batch of <span class="math">\(m\)</span> training examples? We could take the average of our losses <span class="math">\(\mathcal{L}\)</span> over the <span class="math">\(m\)</span> training examples and call this our cost function, <span class="math">\(J\)</span>:</p>
<div class="math">$$J = \dfrac{1}{m}\sum^m_{i = 1} \mathcal{L(\hat{\textbf{y}}^{(i)},\textbf{y}^{(i)})}$$</div>
<p>We calculate the cost <span class="math">\(J\)</span> for our <span class="math">\(m\)</span> training examples, and then calculate two things: (1) the partial derivative of the cost function <span class="math">\(J\)</span> with respect to that layer's activations and (2) the partial derivative of the cost function with respect to the trainable parameters associated with that layer. We reshape (2) into gradients and then update the trainable parameters (which is essentially batch gradient descent) and use (1) to calculate the partial derivative of the cost function with respect to the trainable parameters in the previous layer (which is backpropagation).</p>
<p>For VGG16, we use a batch size of <span class="math">\(m = 256\)</span>, so our cost function becomes:</p>
<div class="math">$$J = \dfrac{1}{256}\sum^{256}_{i = 1} \mathcal{L(\hat{\textbf{y}}^{(i)},\textbf{y}^{(i)})}$$</div>
<p>Which simplifies to become:</p>
<div class="math">$$J = -\dfrac{1}{256}\sum^{256}_{i = 1} \sum_{i=1}^{1000} y_i log(\hat{y}_i)$$</div>
<p>Great, so now we've defined our cost function. We now move to calculating the partial derivatives for each type of layer. Before moving forward I wanted to point something out. Let's say we are calculating the partial derivative of <span class="math">\(J\)</span> with respect to the weights in layer <span class="math">\(j\)</span>:</p>
<div class="math">$$\dfrac{\partial J}{\partial \textbf{W}^{[j]}} =  \dfrac{\partial}{\partial \textbf{W}^{[j]}} \dfrac{1}{256}\sum^{256}_{i = 1} \mathcal{L(\hat{\textbf{y}}^{(i)},\textbf{y}^{(i)})}$$</div>
<p>Notice that the differentiation on the right side can be placed inside the summation:</p>
<div class="math">$$\dfrac{\partial J}{\partial \textbf{W}^{[j]}} =  \dfrac{1}{256}\sum^{256}_{i = 1} \dfrac{\partial\mathcal{L}}{\partial \textbf{W}^{[j]}}$$</div>
<p>So for every batch of 256 training examples <span class="math">\(\textbf{x}^{[i]}\)</span>, we calculate the partial derivative with respect to the loss <span class="math">\(\mathcal{L}\)</span> for each trainable parameter, and then take the average of those 256 partial derivatives of the losses <span class="math">\(\mathcal{L}\)</span> to get the partial derivative of our cost function <span class="math">\(J\)</span> with respect to the trainable parameters.</p>
<p>Notice that we can calculate these 256 sets of partial derivatives in parallel, since they don't depend on each other. This is one of the ways we can parallelize backpropagation efficiently using GPUs.</p>
<p>In any case, instead of calculating the partial derivatives of <span class="math">\(J\)</span>, we will just calculate the partial derivative of <span class="math">\(\mathcal{L}\)</span> for a single training example <span class="math">\(\textbf{x}^{[i]}\)</span> with the knowledge that we can just take the average of all the partial derivatives across the 256 examples in our batch to get the partial derivative of <span class="math">\(J\)</span>.</p>
<p>Before moving forward, if you haven't checked out my blog post on <a href="/backprop">backprop</a> it might be useful, since I'm using a lot of the same concepts (e.g. jacobian matrix, distinction between partial derivative and gradients)</p>
<h2>Backprop for the Softmax Layer</h2>
<p><img src="/images/VGG_16.png" title="[VGG_16]" alt="[VGG_16]"></p>
<p><em>Softmax layer is highlighted on the far right.</em></p>
<p>The last layer of VGG-16 is a fully connected layer with a softmax activation. Since it is a fully connected layer, it has trainable parameters <span class="math">\(\textbf{W}^{[16]}\)</span> and <span class="math">\(\textbf{b}^{[16]}\)</span> and we therefore need to calculate the partial derivative of <span class="math">\(\mathcal{L}\)</span> with respect to both <span class="math">\(\textbf{W}^{[16]}\)</span> and <span class="math">\(\textbf{b}^{[16]}\)</span> which we can calculate the gradients as well as the partial derivative of <span class="math">\(\mathcal{L}\)</span> with respect to <span class="math">\(\textbf{z}^{[16]}\)</span>, which can be used to backpropagate the error to the preceding layer:</p>
<div class="math">$$\bigg ( d\textbf{W}^{[16]}, d\textbf{b}^{[16]}, \dfrac{\partial \mathcal{L}}{\partial \textbf{z}^{[16]}} \bigg )$$</div>
<p>Let's focus our attention on calculating the partial derivative of <span class="math">\(\mathcal{L}\)</span> with respect to <span class="math">\(\textbf{z}^{[16]}\)</span>. We can use the chain rule to rewrite this partial derivative as:</p>
<div class="math">$$\dfrac{\partial \mathcal{L}}{\partial \textbf{z}^{[16]}} = \dfrac{\partial \mathcal{L}}{\partial \textbf{a}^{[16]}}\dfrac{\partial \textbf{a}^{[16]}}{\partial \textbf{z}^{[16]}}$$</div>
<p>So step <span class="math">\(1\)</span> is to figure out the partial derivative of <span class="math">\(\mathcal{L}\)</span> with respect to <span class="math">\(\textbf{a}^{[16]}\)</span>. Recall that the softmax layer <span class="math">\(\textbf{a}^{[16]}\)</span> is equal to our prediction for the class labels, <span class="math">\(\hat{\textbf{y}}\)</span>
so we can write our loss function as:</p>
<div class="math">$$\mathcal{L}(\textbf{a}^{[16]}, \textbf{y}) = -\sum_{i=1}^{1000} y_i log(a^{[16]}_{(i,1)})$$</div>
<p>Recall that <span class="math">\(\mathcal{L}\)</span> is a scalar value while <span class="math">\(\textbf{a}^{[16]}\)</span> is a column vector with dimensions <span class="math">\((1000, 1)\)</span>. The partial derivative of <span class="math">\(\mathcal{L}\)</span> with respect to <span class="math">\(\textbf{a}^{[16]}\)</span> as represented as a Jacobian matrix is therefore <span class="math">\((1, 1000)\)</span>. </p>
<div class="math">$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{a}^{[16]}} = 
\begin{bmatrix}
\dfrac{\partial \mathcal{L}}{\partial a^{[16]}_{(1,1)}} &amp;
\dfrac{\partial \mathcal{L}}{\partial a^{[16]}_{(2,1)}} &amp;
\dots &amp;
\dfrac{\partial \mathcal{L}}{\partial a^{[16]}_{(1000,1)}} 
\end{bmatrix}
$$</div>
<p>Notice that the first dimension of the Jacobian matrix is equal to the number of values in our output <span class="math">\(\mathcal{L}\)</span> which is <span class="math">\(1\)</span>, and the second dimension is equal to the number of values in our input <span class="math">\(\textbf{a}^{[16]}\)</span>, which is <span class="math">\(1000\)</span>. We'll continue to use this formulation in the rest of the blog post.</p>
<p>Let's take the first value of the Jacobian, the partial derivative of <span class="math">\(\mathcal{L}\)</span> with respect to <span class="math">\(a^{[16]}_{(1,1)}\)</span>. </p>
<div class="math">$$\mathcal{L}(\textbf{a}^{[16]}, \textbf{y}) = -\sum_{i=1}^{1000} y_i log(a^{[16]}_{(1,1)})$$</div>
<div class="math">$$\mathcal{L}(\textbf{a}^{[16]}, \textbf{y}) = -y_1 log(a^{[16]}_{(1,1)})-y_2 log(a^{[16]}_{(2,1)}) \dots -y_1000 log(a^{[16]}_{(1000,1)})$$</div>
<div class="math">$$\dfrac{\partial \mathcal{L}}{\partial a^{[16]}_{(1,1)}} = -\dfrac{y_1}{a^{[16]}_{(1,1)}} + 0 + \dots + 0$$</div>
<div class="math">$$\dfrac{\partial \mathcal{L}}{\partial a^{[16]}_{(1,1)}} = -\dfrac{y_1}{a^{[16]}_{(1,1)}}$$</div>
<p>So applying this to every partial derivative in the Jacobian, we get:</p>
<div class="math">$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{a}^{[16]}} = 
\begin{bmatrix}
-\dfrac{y_1}{a^{[16]}_{(1,1)}} &amp;
-\dfrac{y_2}{a^{[16]}_{(2,1)}} &amp;
\dots &amp;
-\dfrac{y_{1000}}{a^{[16]}_{(1000,1)}}
\end{bmatrix}
$$</div>
<p>Notice for any given training example <span class="math">\(\textbf{x}\)</span>, its label <span class="math">\(y\)</span> will have a <span class="math">\(1\)</span> for its class and <span class="math">\(0\)</span> for all the others. So let's say for a random training example <span class="math">\(\textbf{x}\)</span> it has the label <span class="math">\(3\)</span>, meaning <span class="math">\(y_3 = 1\)</span> and all the others are <span class="math">\(0\)</span>. So the partial derivative will look like this:</p>
<div class="math">$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{a}^{[16]}} = 
\begin{bmatrix}
0 &amp;
0 &amp;
-\dfrac{y_3}{a^{[16]}_{(3,1)}} &amp;
\dots &amp;
0
\end{bmatrix}
$$</div>
<p>What this means is that we will only update the weights that relate to the third activation of the softmax layer, which makes sense, since we would only want to update the activation that corresponds with the true class label. We just calculated the partial derivative of <span class="math">\(\mathcal{L}\)</span> with respect to <span class="math">\(\textbf{a}^{[16]}\)</span>. Let's now work on the partial derivative of <span class="math">\(\textbf{a}^{[16]}\)</span> with respect to <span class="math">\(\textbf{z}^{[16]}\)</span>. We'll start by analyzing it's Jacobian matrix:</p>
<div class="math">$$ \dfrac{\partial \textbf{a}^{[16]}}{\partial \textbf{z}^{[16]}} = 
\begin{bmatrix}
\\
\dfrac{\partial a^{[16]}_{(1,1)}}{\partial z^{[16]}_{(1, 1)}} &amp;
\dotsc &amp;
\dfrac{\partial a^{[16]}_{(1,1)}}{\partial z^{[16]}_{(1000, 1)}} \\\\
\vdots &amp; \ddots &amp; \vdots \\\\
\dfrac{\partial a^{[16]}_{(1000,1)}}{\partial z^{[16]}_{(1, 1)}} &amp;
\dotsc &amp;
\dfrac{\partial a^{[16]}_{(1000,1)}}{\partial z^{[16]}_{(1000, 1)}} \\\\
\end{bmatrix}
$$</div>
<p>The partial derivative of <span class="math">\(\textbf{a}^{[16]}\)</span> with respect to <span class="math">\(\textbf{z}^{[16]}\)</span> has dimensions <span class="math">\((1000, 1000)\)</span>. Similar to before let's see if we can calculate the first value in the Jacobian Matrix, the partial derivative of <span class="math">\(a^{[16]}_{(1,1)}\)</span> with respect to <span class="math">\(z^{[16]}_{(1,1)}\)</span>. We start with the formula of the softmax activation, that we defined in the previous post on <a href="/vgg_forwardprop">forward propagation</a>.</p>
<div class="math">$$a^{[16]}_{(1,1)} = \sigma_{(1,1)}(\textbf{z}^{[16]})$$</div>
<div class="math">$$a^{[16]}_{(1,1)} = \dfrac{e^{z^{[16]}_{(1,1)}}}{\sum_{j = 1}^{1000}e^{z^{[16]}_{(j,1)}}}$$</div>
<div class="math">$$\dfrac{\partial a^{[16]}_{(1,1)}}{\partial z^{[16]}_{(1,1)}} = \dfrac{\partial}{\partial z^{[16]}_{(1,1)}}\dfrac{e^{z^{[16]}_{(1,1)}}}{\sum_{j = 1}^{1000}e^{z^{[16]}_{(j,1)}}}$$</div>
<p>To calculate this partial derivative, we use the quotient rule for derivatives and get:</p>
<div class="math">$$\dfrac{\partial a^{[16]}_{(1,1)}}{\partial z^{[16]}_{(1,1)}} = \dfrac{e^{z^{[16]}_{(1,1)}} * \sum_{j = 1}^{1000}e^{z^{[16]}_{(j,1)}} - e^{2z^{[16]}_{(1,1)}}}{\big( \sum_{j = 1}^{1000}e^{z^{[16]}_{(j,1)}} \big) ^ 2}$$</div>
<p>We can separate the term on the RHS into two terms:</p>
<div class="math">$$\dfrac{\partial a^{[16]}_{(1,1)}}{\partial z^{[16]}_{(1,1)}} = \dfrac{e^{z^{[16]}_{(1,1)}}}{\sum_{j = 1}^{1000}e^{z^{[16]}_{(j,1)}}} - \dfrac{e^{2z^{[16]}_{(1,1)}}}{\big( \sum_{j = 1}^{1000}e^{z^{[16]}_{(j,1)}} \big) ^ 2}$$</div>
<p>Substituting the softmax function <span class="math">\(\sigma_{(1,1)}\)</span> back in we get:</p>
<div class="math">$$\dfrac{\partial a^{[16]}_{(1,1)}}{\partial z^{[16]}_{(1,1)}} = \sigma_{(1,1)}(\textbf{z}^{[16]}) - \sigma_{(1,1)}(\textbf{z}^{[16]})^2$$</div>
<div class="math">$$\dfrac{\partial a^{[16]}_{(1,1)}}{\partial z^{[16]}_{(1,1)}} = \sigma_{(1,1)}(\textbf{z}^{[16]})(1 - \sigma_{(1,1)}(\textbf{z}^{[16]}))$$</div>
<p>And using the fact that <span class="math">\(a^{[16]}_{(i,1)} = \sigma_{(i,1)}(\textbf{z}^{[16]})\)</span>, we can simplify the partial derivative to become:</p>
<div class="math">$$\dfrac{\partial a^{[16]}_{(1,1)}}{\partial z^{[16]}_{(1,1)}} = a^{[16]}_{(1,1)}(1 - a^{[16]}_{(1,1)})$$</div>
<p>The partial derivative of <span class="math">\(a^{[16]}_{(i,1)}\)</span> with respect to <span class="math">\(z^{[16]}_{(j,1)}\)</span> looks like the calculation above when <span class="math">\(i=j\)</span>. What happens to the partial derivative when <span class="math">\(i \neq j\)</span>?</p>
<p>Let's take the example of the partial derivative of <span class="math">\(a^{[16]}_{(1,1)}\)</span> with respect to <span class="math">\(z^{[16]}_{(2,1)}\)</span>.</p>
<div class="math">$$a^{[16]}_{(1,1)} = \sigma_{(2,1)}(\textbf{z}^{[16]})$$</div>
<div class="math">$$a^{[16]}_{(1,1)} = \dfrac{e^{z^{[16]}_{(2,1)}}}{\sum_{j = 1}^{1000}e^{z^{[16]}_{(j,1)}}}$$</div>
<div class="math">$$\dfrac{\partial a^{[16]}_{(1,1)}}{\partial z^{[16]}_{(2,1)}} = \dfrac{\partial}{\partial z^{[16]}_{(2,1)}}\dfrac{e^{z^{[16]}_{(2,1)}}}{\sum_{j = 1}^{1000}e^{z^{[16]}_{(j,1)}}}$$</div>
<p>To calculate this partial derivative, we use the quotient rule for derivatives and get:</p>
<div class="math">$$\dfrac{\partial a^{[16]}_{(1,1)}}{\partial z^{[16]}_{(2,1)}} = 
\dfrac{0 * \sum_{j = 1}^{1000}e^{z^{[16]}_{(j,1)}} - e^{z^{[16]}_{(1,1)}}e^{z^{[16]}_{(2,1)}}}{\big( \sum_{j = 1}^{1000}e^{z^{[16]}_{(j,1)}} \big) ^ 2}$$</div>
<div class="math">$$\dfrac{\partial a^{[16]}_{(1,1)}}{\partial z^{[16]}_{(2,1)}} = 
\dfrac{- e^{z^{[16]}_{(1,1)}}e^{z^{[16]}_{(2,1)}}}{\big( \sum_{j = 1}^{1000}e^{z^{[16]}_{(j,1)}} \big) ^ 2}$$</div>
<div class="math">$$\dfrac{\partial a^{[16]}_{(1,1)}}{\partial z^{[16]}_{(2,1)}} = 
- a^{[16]}_{(1,1)}a^{[16]}_{(2,1)}$$</div>
<p>Generalizing this, we get:</p>
<div class="math">$$\dfrac{\partial a^{[16]}_{(i,1)}}{\partial z^{[16]}_{(j,1)}} = 
\begin{cases}
   a^{[16]}_{(i,1)}(1 - a^{[16]}_{(i,1)})  &amp; i = j \\\\
   - a^{[16]}_{(i,1)}a^{[16]}_{(j,1)} &amp; i \neq j
\end{cases}
$$</div>
<p>And the Jacobian looks like this:</p>
<div class="math">$$ \dfrac{\partial \textbf{a}^{[16]}}{\partial \textbf{z}^{[16]}} = 
\begin{bmatrix}
\\
a^{[16]}_{(1,1)}(1 - a^{[16]}_{(1,1)}) &amp;
- a^{[16]}_{(1,1)}a^{[16]}_{(2,1)} &amp; 
\dotsc &amp;
- a^{[16]}_{(1,1)}a^{[16]}_{(999,1)} &amp;
- a^{[16]}_{(1,1)}a^{[16]}_{(1000,1)} \\\\
\vdots &amp; 
\vdots &amp;
\ddots &amp;
\vdots &amp;
\vdots \\\\
- a^{[16]}_{(1000,1)}a^{[16]}_{(1,1)} &amp; 
- a^{[16]}_{(1000,1)}a^{[16]}_{(2,1)} &amp; 
\dotsc &amp;
- a^{[16]}_{(1000,1)}a^{[16]}_{(999,1)} &amp;
a^{[16]}_{(1000,1)}(1 - a^{[16]}_{(1000,1)}) \\\\
\end{bmatrix}
$$</div>
<p>The Jacobian has dimensions <span class="math">\((1000, 1000)\)</span>. Notice that the diagonal elements are equal to <span class="math">\(a^{[16]}_{(i,1)}(1 - a^{[16]}_{(i,1)})\)</span>, whereas every other element is equal to <span class="math">\(- a^{[16]}_{(i,1)}a^{[16]}_{(j,1)}\)</span>.</p>
<p>Substituting in our two Jacobian matricies, we get:</p>
<div class="math">$$\dfrac{\partial \mathcal{L}}{\partial \textbf{z}^{[16]}} = \dfrac{\partial \mathcal{L}}{\partial \textbf{a}^{[16]}}\dfrac{\partial \textbf{a}^{[16]}}{\partial \textbf{z}^{[16]}}$$</div>
<div class="math">$$\dfrac{\partial \mathcal{L}}{\partial \textbf{z}^{[16]}} = 
\begin{bmatrix}
-\dfrac{y_1}{a^{[16]}_{(1,1)}} &amp;
-\dfrac{y_2}{a^{[16]}_{(2,1)}} &amp;
\dots &amp;
-\dfrac{y_{1000}}{a^{[16]}_{(1000,1)}}
\end{bmatrix}
\begin{bmatrix}
\\
a^{[16]}_{(1,1)}(1 - a^{[16]}_{(1,1)}) &amp;
- a^{[16]}_{(1,1)}a^{[16]}_{(2,1)} &amp; 
\dotsc &amp;
- a^{[16]}_{(1,1)}a^{[16]}_{(999,1)} &amp;
- a^{[16]}_{(1,1)}a^{[16]}_{(1000,1)} \\\\
\vdots &amp; 
\vdots &amp;
\ddots &amp;
\vdots &amp;
\vdots \\\\
- a^{[16]}_{(1000,1)}a^{[16]}_{(1,1)} &amp; 
- a^{[16]}_{(1000,1)}a^{[16]}_{(2,1)} &amp; 
\dotsc &amp;
- a^{[16]}_{(1000,1)}a^{[16]}_{(999,1)} &amp; 
a^{[16]}_{(1000,1)}(1 - a^{[16]}_{(1000,1)}) \\\\
\end{bmatrix}
$$</div>
<p>This matrix multiplication yields a Jacobian matrix with dimensions <span class="math">\((1, 1000)\)</span>. Let's look at the calculations for the first element of this matrix, the partial derivative of <span class="math">\(\mathcal{L}\)</span> with respect to <span class="math">\(z^{[16]}_{(1,1)}\)</span>.</p>
<div class="math">$$\dfrac{\partial \mathcal{L}}{\partial z^{[16]}_{(1,1)}} = 
-\dfrac{y_1}{a^{[16]}_{(1,1)}}a^{[16]}_{(1,1)}(1 - a^{[16]}_{(1,1)}) + 
\dfrac{y_2}{a^{[16]}_{(2,1)}}a^{[16]}_{(2,1)}a^{[16]}_{(1,1)} +
\dfrac{y_3}{a^{[16]}_{(3,1)}}a^{[16]}_{(3,1)}a^{[16]}_{(1,1)} +  
\dots + 
\dfrac{y_{1000}}{a^{[16]}_{(1000,1)}}a^{[16]}_{(1000,1)}a^{[16]}_{(1,1)}
$$</div>
<div class="math">$$\dfrac{\partial \mathcal{L}}{\partial z^{[16]}_{(1,1)}} = 
-y_1(1 - a^{[16]}_{(1,1)}) + 
y_2a^{[16]}_{(1,1)} +
y_3a^{[16]}_{(1,1)} +  
\dots + 
y_{1000}a^{[16]}_{(1,1)}
$$</div>
<div class="math">$$\dfrac{\partial \mathcal{L}}{\partial z^{[16]}_{(1,1)}} = 
-y_1 + 
y_1a^{[16]}_{(1,1)}) + 
y_2a^{[16]}_{(1,1)} +
y_3a^{[16]}_{(1,1)} +  
\dots + 
y_{1000}a^{[16]}_{(1,1)}
$$</div>
<div class="math">$$\dfrac{\partial \mathcal{L}}{\partial z^{[16]}_{(1,1)}} = 
-y_1 + 
a^{[16]}_{(1,1)}\sum_i^{1000}{y_i}
$$</div>
<p>And recall because <span class="math">\(\textbf{y}\)</span> has only one class, one element is equal to <span class="math">\(1\)</span> whereas the others are equal to <span class="math">\(0\)</span>. Therefore, the sum is equal to <span class="math">\(1\)</span>.</p>
<div class="math">$$\dfrac{\partial \mathcal{L}}{\partial z^{[16]}_{(1,1)}} = 
-y_1 +
a^{[16]}_{(1,1)}*1
$$</div>
<div class="math">$$\dfrac{\partial \mathcal{L}}{\partial z^{[16]}_{(1,1)}} = 
a^{[16]}_{(1,1)} - y_1
$$</div>
<p>Notice that the partial derivative is the same with just one class that we calculated in the previous <a href="/backprop">backprop</a> blog post! All that fun work to get the same answer. <a href="https://www.ics.uci.edu/~pjsadows/notes.pdf">This is a great reference</a> if you want more softmax backprop fun.</p>
<p>Generalizing to all elements in the Jacobian matrix, we get:</p>
<div class="math">$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{z}^{[16]}} = 
\begin{bmatrix}
a^{[16]}_{(1,1)} - y_1 &amp;
a^{[16]}_{(2,1)} - y_2 &amp;
a^{[16]}_{(3,1)} - y_3 &amp;
\dots &amp;
a^{[16]}_{(1000,1)} - y_{1000} &amp;
\end{bmatrix}
$$</div>
<p>And we end up with the partial derivative of <span class="math">\(\mathcal{L}\)</span> with respect to <span class="math">\(\textbf{z}^{[16]}\)</span> with dimensions <span class="math">\((1000, 1)\)</span>. We sometimes label the transpose of this partial derivative <span class="math">\(\boldsymbol{\delta}^{[16]}\)</span>.</p>
<div class="math">$$
\boldsymbol{\delta}^{[2]} = 
\bigg( \dfrac{\mathcal{L}}{\partial{\mathbf{z}^{[16]}}}\bigg)^T
$$</div>
<p>The dimensions of <span class="math">\(\boldsymbol{\delta}^{[16]}\)</span> are <span class="math">\((1000, 1)\)</span>, which match the dimensions of <span class="math">\(\textbf{z}^{[16]}\)</span>. So we can think of <span class="math">\(\boldsymbol{\delta}^{[16]}\)</span> as the gradient for <span class="math">\(\textbf{z}^{[16]}\)</span>, although we don't use this explicitly in gradient descent since <span class="math">\(\textbf{z}^{[16]}\)</span> has no updatable parameters.</p>
<p>Next up, we need to calculate the partial derivative of <span class="math">\(\mathcal{L}\)</span> with respect to both <span class="math">\(\textbf{W}^{[16]}\)</span> and <span class="math">\(\textbf{b}^{[16]}\)</span> in order to get the gradients <span class="math">\(d\textbf{W}^{[16]}\)</span> and <span class="math">\(d\textbf{b}^{[16]}\)</span> with gradient descent. Luckily, we've already calculated this in the previous post on <a href="\backprop">backprop</a> so we can just use the results from that:</p>
<div class="math">$$
\dfrac{\partial \textbf{z}^{[16]}}{\partial \textbf{W}^{[16]}} =
\begin{bmatrix}
a^{[15]}_{(1,1)} &amp;
a^{[15]}_{(2,1)} &amp;
a^{[15]}_{(3,1)} &amp;
\dots &amp;
a^{[15]}_{(999,1)} &amp;
a^{[15]}_{(1000,1)} &amp;
0 &amp;
0 &amp;
0 &amp;
\dots &amp;
0 &amp;
0 &amp;
\dots &amp;
\dots &amp;
0 &amp;
0 &amp;
0 &amp;
\dots &amp;
0 &amp;
0 \\\\
0 &amp;
0 &amp;
0 &amp;
\dots &amp;
0 &amp;
0 &amp;
a^{[15]}_{(1,1)} &amp;
a^{[15]}_{(2,1)} &amp;
a^{[15]}_{(3,1)} &amp;
\dots &amp;
a^{[15]}_{(999,1)} &amp;
a^{[15]}_{(1000,1)} &amp;
\dots &amp;
\dots &amp;
0 &amp;
0 &amp;
0 &amp;
\dots &amp;
0 &amp;
0 &amp; \\\\
\vdots &amp;
\vdots &amp;
\vdots &amp;
\dots &amp;
\vdots &amp;
\vdots &amp;
\vdots &amp;
\vdots &amp;
\vdots &amp;
\dots &amp;
\vdots &amp;
\vdots &amp;
\dots &amp;
\dots &amp;
\vdots &amp;
\vdots &amp;
\vdots &amp;
\dots &amp;
\vdots &amp;
\vdots &amp; \\\\
0 &amp;
0 &amp;
0 &amp;
\dots &amp;
0 &amp;
0 &amp;
0 &amp;
0 &amp;
0 &amp;
\dots &amp;
0 &amp;
0 &amp;
\dots &amp;
\dots &amp;
a^{[15]}_{(1,1)} &amp;
a^{[15]}_{(2,1)} &amp;
a^{[15]}_{(3,1)} &amp;
\dots &amp;
a^{[15]}_{(999,1)} &amp;
a^{[15]}_{(1000,1)} \\\\
\end{bmatrix}
$$</div>
<p>Notice that the Jacobian Matrix for the partial derivative of <span class="math">\(\textbf{z}^{[16]}\)</span> with respect to <span class="math">\(\textbf{W}^{[16]}\)</span> is a matrix with dimensions <span class="math">\((1000, 1000000)\)</span>. Since <span class="math">\(\textbf{W}^{[16]}\)</span> has dimensions <span class="math">\((1000, 1000)\)</span>, it has a total of <span class="math">\(1000 * 1000 = 1,000,000\)</span> weights which is represented in the second dimension of the Jacobian Matrix. </p>
<p>Using the chain rule, the partial derivative of <span class="math">\(\mathcal{L}\)</span> with respect to <span class="math">\(\textbf{W}^{[16]}\)</span> is equal to:
</p>
<div class="math">$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{W}^{[16]}} = \dfrac{\partial \mathcal{L}}{\partial \textbf{z}^{[16]}}\dfrac{\partial \mathcal{\textbf{z}^{[16]}}}{\partial \textbf{W}^{[16]}}
\dfrac{\partial \mathcal{L}}{\partial \textbf{W}^{[16]}} = \boldsymbol{\delta}^{[16]T}\dfrac{\partial \mathcal{\textbf{z}^{[16]}}}{\partial \textbf{W}^{[16]}}
$$</div>
<p>Plugging in our two results, we get:</p>
<div class="math">$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{W}^{[16]}} = 
\begin{bmatrix}
\delta^{[16]}_{(1,1)} &amp;
\delta^{[16]}_{(2,1)} &amp;
\delta^{[16]}_{(3,1)} &amp;
\dots &amp;
\delta^{[16]}_{(1000,1)}
\end{bmatrix}
\begin{bmatrix}
a^{[15]}_{(1,1)} &amp;
a^{[15]}_{(2,1)} &amp;
a^{[15]}_{(3,1)} &amp;
\dots &amp;
a^{[15]}_{(999,1)} &amp;
a^{[15]}_{(1000,1)} &amp;
0 &amp;
0 &amp;
0 &amp;
\dots &amp;
0 &amp;
0 &amp;
\dots &amp;
\dots &amp;
0 &amp;
0 &amp;
0 &amp;
\dots &amp;
0 &amp;
0 \\\\
0 &amp;
0 &amp;
0 &amp;
\dots &amp;
0 &amp;
0 &amp;
a^{[15]}_{(1,1)} &amp;
a^{[15]}_{(2,1)} &amp;
a^{[15]}_{(3,1)} &amp;
\dots &amp;
a^{[15]}_{(999,1)} &amp;
a^{[15]}_{(1000,1)} &amp;
\dots &amp;
\dots &amp;
0 &amp;
0 &amp;
0 &amp;
\dots &amp;
0 &amp;
0 &amp; \\\\
\vdots &amp;
\vdots &amp;
\vdots &amp;
\dots &amp;
\vdots &amp;
\vdots &amp;
\vdots &amp;
\vdots &amp;
\vdots &amp;
\dots &amp;
\vdots &amp;
\vdots &amp;
\dots &amp;
\dots &amp;
\vdots &amp;
\vdots &amp;
\vdots &amp;
\dots &amp;
\vdots &amp;
\vdots &amp; \\\\
0 &amp;
0 &amp;
0 &amp;
\dots &amp;
0 &amp;
0 &amp;
0 &amp;
0 &amp;
0 &amp;
\dots &amp;
0 &amp;
0 &amp;
\dots &amp;
\dots &amp;
a^{[15]}_{(1,1)} &amp;
a^{[15]}_{(2,1)} &amp;
a^{[15]}_{(3,1)} &amp;
\dots &amp;
a^{[15]}_{(999,1)} &amp;
a^{[15]}_{(1000,1)} \\\\
\end{bmatrix}
$$</div>
<div class="math">$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{W}^{[16]}} = 
\begin{bmatrix}
\delta^{[16]}_{(1,1)}a^{[15]}_{(1,1)} &amp;
\delta^{[16]}_{(2,1)}a^{[15]}_{(1,1)} &amp;
\delta^{[16]}_{(3,1)}a^{[15]}_{(1,1)} &amp;
\dots &amp;
\delta^{[16]}_{(1000,1)}a^{[15]}_{(1,1)} &amp;
\delta^{[16]}_{(1,1)}a^{[15]}_{(2,1)} &amp;
\delta^{[16]}_{(2,1)}a^{[15]}_{(2,1)} &amp;
\delta^{[16]}_{(3,1)}a^{[15]}_{(2,1)} &amp;
\dots &amp;
\delta^{[16]}_{(1000,1)}a^{[15]}_{(2,1)} &amp;
\dots &amp;
\dots &amp;
\delta^{[16]}_{(1,1)}a^{[15]}_{(1000,1)} &amp;
\delta^{[16]}_{(2,1)}a^{[15]}_{(1000,1)} &amp;
\delta^{[16]}_{(3,1)}a^{[15]}_{(1000,1)} &amp;
\dots &amp;
\delta^{[16]}_{(1000,1)}a^{[15]}_{(1000,1)} &amp;
\end{bmatrix}
$$</div>
<p>So this is our Jacobian, with dimensions <span class="math">\((1,1000000)\)</span>. But we need our gradient matrix <span class="math">\(d\textbf{W}^{[16]}\)</span> to have dimensions that match <span class="math">\(\textbf{W}^{[16]}\)</span>. So we will reshape the Jacobian into a <span class="math">\((1000, 1000)\)</span> matrix.</p>
<div class="math">$$
d\textbf{W}^{[16]} = 
\begin{bmatrix}
\delta^{[16]}_{(1,1)}a^{[15]}_{(1,1)} &amp;
\delta^{[16]}_{(2,1)}a^{[15]}_{(1,1)} &amp;
\delta^{[16]}_{(3,1)}a^{[15]}_{(1,1)} &amp;
\dots &amp;
\delta^{[16]}_{(1000,1)}a^{[15]}_{(1,1)} \\\\
\delta^{[16]}_{(1,1)}a^{[15]}_{(2,1)} &amp;
\delta^{[16]}_{(2,1)}a^{[15]}_{(2,1)} &amp;
\delta^{[16]}_{(3,1)}a^{[15]}_{(2,1)} &amp;
\dots &amp;
\delta^{[16]}_{(1000,1)}a^{[15]}_{(2,1)} \\\\
\vdots &amp;
\vdots &amp;
\vdots &amp;
\dots &amp;
\vdots \\\\
\delta^{[16]}_{(1,1)}a^{[15]}_{(1000,1)} &amp;
\delta^{[16]}_{(2,1)}a^{[15]}_{(1000,1)} &amp;
\delta^{[16]}_{(3,1)}a^{[15]}_{(1000,1)} &amp;
\dots &amp;
\delta^{[16]}_{(1000,1)}a^{[15]}_{(1000,1)} \\\\
\end{bmatrix}
$$</div>
<div class="math">$$
d\textbf{W}^{[16]} = 
\begin{bmatrix}
\delta^{[16]}_{(2,1)} \\\\
\delta^{[16]}_{(2,1)} \\\\
\vdots \\\\
\delta^{[16]}_{(1000,1)}
\end{bmatrix}
\begin{bmatrix}
a^{[15]}_{(1,1)} &amp;
a^{[15]}_{(2,1)} &amp;
\vdots &amp;
a^{[15]}_{(1000,1)}
\end{bmatrix}
$$</div>
<div class="math">$$
d\textbf{W}^{[16]} = 
\boldsymbol{\delta}^{[16]}\textbf{a}^{[15]T}
$$</div>
<p>Ok great, now that we have <span class="math">\(d\textbf{W}^{[16]}\)</span> taken care of, let's move on to looking at the partial derivative of <span class="math">\(\textbf{z}^{[16]}\)</span> with respect to <span class="math">\(\textbf{b}^{[16]}\)</span>. Again, we will use calculations we did in a simpler example in the <a href="\backprop">backprop</a> post.</p>
<div class="math">$$
\dfrac{\partial \textbf{z}^{[16]}}{\partial \textbf{b}^{[16]}} =
\begin{bmatrix}
1 &amp;
0 &amp;
\dots &amp;
0 &amp;
0 \\\\
0 &amp;
1 &amp;
\dots &amp;
0 &amp;
0 \\\\
0 &amp;
0 &amp;
\dots &amp;
1 &amp;
0 \\\\
0 &amp;
0 &amp;
\dots &amp;
0 &amp;
1 \\\\
\end{bmatrix}
$$</div>
<p>Using the chain rule, we get:</p>
<div class="math">$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{b}^{[16]}} = \dfrac{\partial \mathcal{L}}{\partial \textbf{z}^{[16]}}\dfrac{\partial \mathcal{\textbf{z}^{[16]}}}{\partial \textbf{b}^{[16]}}
\dfrac{\partial \mathcal{L}}{\partial \textbf{b}^{[16]}} = \boldsymbol{\delta}^{[16]T}\dfrac{\partial \mathcal{\textbf{z}^{[16]}}}{\partial \textbf{b}^{[16]}}
$$</div>
<div class="math">$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{b}^{[16]}} = 
\begin{bmatrix}
\delta^{[16]}_{(1,1)} &amp;
\delta^{[16]}_{(2,1)} &amp;
\delta^{[16]}_{(3,1)} &amp;
\dots &amp;
\delta^{[16]}_{(1000,1)}
\end{bmatrix}
\begin{bmatrix}
1 &amp;
0 &amp;
\dots &amp;
0 &amp;
0 \\\\
0 &amp;
1 &amp;
\dots &amp;
0 &amp;
0 \\\\
0 &amp;
0 &amp;
\dots &amp;
1 &amp;
0 \\\\
0 &amp;
0 &amp;
\dots &amp;
0 &amp;
1 \\\\
\end{bmatrix}
$$</div>
<div class="math">$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{b}^{[16]}} = 
\begin{bmatrix}
\delta^{[16]}_{(1,1)} &amp;
\delta^{[16]}_{(2,1)} &amp;
\delta^{[16]}_{(3,1)} &amp;
\dots &amp;
\delta^{[16]}_{(1000,1)}
\end{bmatrix}
$$</div>
<div class="math">$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{b}^{[16]}} = \boldsymbol{\delta}^{[16]T}
$$</div>
<p>So the partial derivative of <span class="math">\(\mathcal{L}\)</span> with respect to <span class="math">\(\textbf{b}^{[16]}\)</span> is just the transpose of <span class="math">\(\boldsymbol{\delta}^{[16]}\)</span>, meaning that the gradient <span class="math">\(d\textbf{b}^{[16]}\)</span> will just be equal to:</p>
<div class="math">$$d\textbf{b}^{[16]} = \boldsymbol{\delta}^{[16]T}$$</div>
<p>Great, so in this section we've talked about how to calculate the gradients <span class="math">\(\boldsymbol{\delta}^{[16]}\)</span>, <span class="math">\(d\textbf{W}^{[16]}\)</span>, and <span class="math">\(d\textbf{b}^{[16]}\)</span> and therefore know how to calculate the gradients for the softmax layer for hopefully any architecture we will encounter in the future.</p>
<p>After this softmax layer, we have two more fully-connected layers. The only difference between these two fully connected layers and the softmax layer we calculated above is that they use a ReLU as opposed to softmax activation function. I show how to calculate the gradients for these layers in my <a href="/backprop">backprop post</a> that I'm sure you are sick of hearing about.</p>
<p>Working backwards, after the two fully connected layers we reshape our output from a column vector <span class="math">\(f^{[13]}\)</span> which has dimensions <span class="math">\((25088, 1)\)</span> to a 3D tensor <span class="math">\(m^{[13]}\)</span> of shape <span class="math">\((7, 7, 512)\)</span>. What is the partial derivative for this transition? Since we don't change the dimensions and only reshape the elements, the partial derivative of <span class="math">\(\textbf{f}^{[13]}\)</span> with respect to <span class="math">\(\textbf{m}^{[13]}\)</span> is just the identity matrix with dimensions <span class="math">\((25088, 25088)\)</span>. So this partial derivative doesn't change the calculations for the calculating the partial derivatives for the preceding layer.</p>
<p>&nbsp;</p>
<h2>Backprop for the Max Pooling Layer</h2>
<p>There are a total of <span class="math">\(5\)</span> max pooling layers in the VGG-16 architecture. Since they don't use trainable parameters, we only need to calculate their gradient <span class="math">\(\boldsymbol{\delta}^{[13]}\)</span>, which is the input into the max pooling layer and we can use to calculate the gradients for the trainable parameters of that conv layer, <span class="math">\(\textbf{W}_c^{[13]}\)</span> and <span class="math">\(\textbf{b}_c^{[13]}\)</span></p>
<p>In order to calculate its gradient, we need to find the partial derivative of <span class="math">\(\mathcal{L}\)</span> with respect to <span class="math">\(\textbf{a}^{[13]}\)</span>. Using the chain rule, we get:</p>
<div class="math">$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{a}^{[13]}} = \dfrac{\partial \mathcal{L}}{\partial \textbf{m}^{[13]}}\dfrac{\partial \textbf{m}^{[13]}}{\partial \textbf{a}^{[13]}}
$$</div>
<p>Let's focus on the partial of <span class="math">\(\textbf{m}^{[13]}\)</span> with respect to <span class="math">\(\textbf{a}^{[13]}\)</span>. <span class="math">\(\textbf{m}^{[13]}\)</span> has <span class="math">\(7 * 7 * 512 = 25088\)</span> values, and will therefore be the first dimension of the Jacobian. <span class="math">\(\textbf{a}^{[13]}\)</span> has <span class="math">\(14 * 14 * 512 = 100352\)</span>, and is the second dimension of the Jacobian. Good lord, this Jacobian has dimensions <span class="math">\((25088, 100352)\)</span> and has a total of <span class="math">\(25088 * 100352 = 2517630976\)</span> values. As you'll soon see, this Jacobian matrix is very sparse. There are computational shortcuts that libraries like tensorflow and pytorch use to handle these crazy matricies. So no worries.</p>
<p>Let's spend some time understanding how the elements of this matrix are aligned. We can start by looking at the first row of this matrix:</p>
<div class="math">$$
\begin{bmatrix}
\dfrac{\partial m^{[13]}_{(1,1,1)}}{\partial a^{[13]}_{(1, 1, 1)}}, &amp; 
\dots, &amp; 
\dfrac{\partial m^{[13]}_{(1,1,1)}}{\partial a^{[13]}_{(14, 14, 512)}}
\end{bmatrix}
$$</div>
<p>So notice that the value in the numerator stays the same and the value of the denominator starts with <span class="math">\(a^{[13]}_{(1, 1, 1)}\)</span> and finishes at <span class="math">\(a^{[13]}_{(14, 14, 512)}\)</span>. Since they are the ones chaning, let's focus on the values in the denominator:</p>
<div class="math">$$
\begin{bmatrix}
a^{[13]}_{(1, 1, 1)} &amp; 
\dots &amp;
a^{[13]}_{(14, 14, 512)}
\end{bmatrix}
$$</div>
<p>So basically what we are doing is taking the 3D tensor array <span class="math">\(\textbf{a}^{[13]}\)</span> and flattening it into a 1D vector. In math, this operation is sometimes called <span class="math">\(\textrm{vec}(\textbf{a})\)</span>. If we expanded this a little out:</p>
<div class="math">$$
\begin{bmatrix}
a^{[13]}_{(1, 1, 1)} &amp;
\dots &amp;
a^{[13]}_{(14, 1, 1)} &amp;
a^{[13]}_{(1, 2, 1)} &amp;
\dots &amp;
a^{[13]}_{(14, 14, 1)} &amp;
a^{[13]}_{(1, 1, 2)} &amp;
\dots &amp;
a^{[13]}_{(14, 14, 512)}
\end{bmatrix}
$$</div>
<p>So we think of the first dimension as the width, the second as the height, and the third as the channel. So the first <span class="math">\(14\)</span> values of the vector are all the values for width for a height of <span class="math">\(1\)</span> and channel of <span class="math">\(1\)</span>. Then we move to the next height <span class="math">\(2\)</span>, keep the channel <span class="math">\(1\)</span> the same, and go through all <span class="math">\(14\)</span> values of the width. We continue this until we finish all <span class="math">\(14\)</span> heights, and we have the first <span class="math">\(14 * 14 = 196\)</span> values. Next, we reset the height to <span class="math">\(1\)</span> and width to <span class="math">\(1\)</span> and repeat the process for channel <span class="math">\(2\)</span>. We go through all <span class="math">\(512\)</span> channels in this way, until we have a total of <span class="math">\(14 * 14 * 512 = 100352\)</span> values.</p>
<p>So that was the first row of our Jacobian matrix. We can think of the first column of our Jacobian matrix in a similar way:</p>
<div class="math">$$
\begin{bmatrix}
\dfrac{\partial m^{[13]}_{(1,1,1)}}{\partial a^{[13]}_{(1, 1, 1)}}, \\\\ 
\dots \\\\
\dfrac{\partial m^{[13]}_{(7,7,512)}}{\partial a^{[13]}_{(1, 1, 1)}}
\end{bmatrix}
$$</div>
<p>In this case, the value in the numerator is the one changing, from <span class="math">\(m^{[13]}_{(1,1,1)}\)</span> to <span class="math">\(m^{[13]}_{(7,7,512)}\)</span>. We can think of this as being similar to above, where we convert <span class="math">\(\textbf{m}^{[13]}\)</span> into a flat vector, using the <span class="math">\(vec\)</span> operation.</p>
<p>Now that we have a better intutition about the values in the Jacobian Matrix, let's take the first value in this Jacobian Matrix and see if we can figure it out. Recall that the max pooling operation was defined as being:</p>
<div class="math">$$m^{[13]}_{(i,j,k)} = \max_{i * s &lt;= l &lt; i * s + f, j * s &lt;= l &lt; j * s + f }a^{[2]}_{(l,m,k)}$$</div>
<p>Since all of our max pooling layers in VGG16 use a stride size of <span class="math">\(2\)</span> (<span class="math">\(s = 2\)</span>) and a <span class="math">\(2x2\)</span> filter (<span class="math">\(f = 2\)</span>), we get:</p>
<div class="math">$$m^{[13]}_{(i,j,k)} = \max_{i * 2 &lt;= l &lt; i * 2 + 2, j * 2 &lt;= l &lt; j * 2 + 2 }a^{[2]}_{(l,m,k)}$$</div>
<p>Let's say we are interested in calculating <span class="math">\(m^{[13]}_{(1,1,1)}\)</span>. We would look for the max value within a <span class="math">\(2\)</span> by <span class="math">\(2\)</span> window within <span class="math">\(\textbf{a}^{[2]}\)</span>. Based on the equation above, the values we will look at are:</p>
<div class="math">$$(a^{[13]}_{(1, 1, 1)}, a^{[13]}_{(2, 1, 1)}, a^{[13]}_{(1, 2, 1)}, a^{[13]}_{(2, 2, 1)})$$</div>
<p>Let's say the actual numerical values are as follows:</p>
<div class="math">$$0, 5, 1, -4$$</div>
<p>Since <span class="math">\(5\)</span> is the largest, <span class="math">\(m^{[13]}_{(1,1,1)} = a^{[13]}_{(2, 1, 1)}\)</span>, and the partial derivative of <span class="math">\(m^{[13]}_{(1,1,1)}\)</span> with respect to <span class="math">\(a^{[13]}_{(2, 1, 1)}\)</span> is equal to:</p>
<div class="math">$$\dfrac{\partial m^{[13]}_{(1,1,1)}}{\partial a^{[13]}_{(1, 1, 1)}} = 1$$</div>
<p>Note that the partial derivatives of <span class="math">\(m^{[13]}_{(1,1,1)}\)</span> with respect to <span class="math">\(a^{[13]}_{(1, 1, 1)}\)</span>, <span class="math">\(a^{[13]}_{(1, 2, 1)}\)</span>, and <span class="math">\(a^{[13]}_{(2, 2, 1)}\)</span> are equal to <span class="math">\(0\)</span>. And actually, the partial derivative of <span class="math">\(m^{[13]}_{(1,1,1)}\)</span> with respect to all the other values in the conv layer <span class="math">\(\textbf{a}^{[13]}\)</span> are also equal to 0 since they aren't in the 2 by 2 max pooling window we used. So the first row of our Jacobian matrix looks like this:</p>
<div class="math">$$
\begin{bmatrix}
\dfrac{\partial m^{[13]}_{(1,1,1)}}{\partial a^{[13]}_{(1, 1, 1)}}, &amp;
\dfrac{\partial m^{[13]}_{(1,1,1)}}{\partial a^{[13]}_{(2, 1, 1)}}, &amp; 
\dots, &amp; 
\dfrac{\partial m^{[13]}_{(1,1,1)}}{\partial a^{[13]}_{(14, 14, 512)}}
\end{bmatrix}
=
\begin{bmatrix}
0 &amp;
1 &amp;
\dots &amp; 
0
\end{bmatrix}
$$</div>
<p>And we can continue this process for <span class="math">\(m^{[13]}_{(2,1,1)}\)</span> through <span class="math">\(m^{[13]}_{(7,7,512)}\)</span> and eventually fill up our <span class="math">\(25088\)</span> rows of our Jacobian matrix. And that's all to it! Notice that this matrix is very sparse. Each row has exactly one nonzero value and each column has at most one nonzero value. So we don't have to hold this whole matrix in memory. Instead, we can just record the locations of the nonzero values.</p>
<blockquote>
<p>Notice that this means that we will only update the weights with errors that correspond with the max values in each window.</p>
</blockquote>
<p>We just figured out the partial derivative of <span class="math">\(\textbf{m}^{[13]}\)</span> with respect to <span class="math">\(\textbf{a}^{[13]}\)</span>. Since we already calculated the partial derivative of <span class="math">\(\mathcal{L}\)</span> with respect to <span class="math">\(\textbf{m}^{[13]}\)</span>, using the chain rule we can use that to calculate the partial derivative of <span class="math">\(\mathcal{L}\)</span> with respect to <span class="math">\(\textbf{a}^{[13]}\)</span> and send that result to the preceding layer and for the first time calculate the partial derivatives of the convolutional weights and biases. We'll work on that next.</p>
<p>&nbsp;</p>
<h2>Backprop for the Conv Layer</h2>
<p>The final type of layer that we need to calculate the partial derivatives for in order to get the gradients of the trainable parameters are conv layers. Let's focus on the first conv layer that we reach in VGG16, <span class="math">\(\textbf{a}^{[13]}\)</span>. Our objective is to calculate the gradients for the trainable parameters in this layer, <span class="math">\(d\textbf{W}^{[13]}_c\)</span> and <span class="math">\(d\textbf{b}^{[13]}_c\)</span>, as well as the partial derivative of <span class="math">\(\mathcal{L}\)</span> with respect to <span class="math">\(\textbf{z}^{[13]}\)</span>, which we use to calculate the gradients for the preceding layers. Let's focus on this first. We can use the chain rule to break apart this partial derivative:</p>
<div class="math">$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{z}^{[13]}} = \dfrac{\partial \mathcal{L}}{\partial \textbf{a}^{[13]}}\dfrac{\partial \mathcal{\textbf{a}^{[13]}}}{\partial \textbf{z}^{[13]}}
$$</div>
<p>We've already calculated the partial derivative of <span class="math">\(\mathcal{L}\)</span> with respect to <span class="math">\(\textbf{a}^{[13]}\)</span>, so we focus our attention on calculating the partial derivative of <span class="math">\(\textbf{a}^{[13]}\)</span> with respect to <span class="math">\(\textbf{z}^{[13]}\)</span>. <span class="math">\(\textbf{a}^{[13]}\)</span> and <span class="math">\(\textbf{z}^{[13]}\)</span> have the same dimensions <span class="math">\((14, 14, 512)\)</span>. So therefore, the Jacobian matrix is <span class="math">\(100352\)</span> by <span class="math">\(100352\)</span>. The position of the values from <span class="math">\(\textbf{a}^{[13]}\)</span> and <span class="math">\(\textbf{z}^{[13]}\)</span> is very similar to our previous max pooling example, where we can think of the changing values in each row as <span class="math">\(vec(\textbf{z}^{[13]})\)</span> and the changing values in each column as <span class="math">\(vec(\textbf{a}^{[13]})\)</span>.</p>
<p>The transition from <span class="math">\(\textbf{z}^{[13]}\)</span> to <span class="math">\(\textbf{a}^{[13]}\)</span> just consists of applying the ReLU <span class="math">\(g(z)\)</span> nonlinear activation function to each element. What is the ReLU function?</p>
<div class="math">$$
g(z) = \begin{cases}
   z &amp;\text{if } z &gt; 0  \\
   0 &amp;\text{if } z =&lt; 0
\end{cases}
$$</div>
<p>So the ReLU function just returns the value if it's greater than <span class="math">\(0\)</span>, and 0 otherwise. What is the derivative of the ReLU function?</p>
<div class="math">$$
g'(z) = \begin{cases}
   1 &amp;\text{if } z &gt; 0  \\
   \text{Undefined} &amp;\text{if } z = 0  \\
   0 &amp;\text{if } z &lt; 0
\end{cases}
$$</div>
<p>Since this function is applied elementwise, the partial derivative of <span class="math">\(\textbf{a}^{[13]}\)</span> with respect to <span class="math">\(\textbf{z}^{[13]}\)</span> is just a diagonal matrix, with the derivatives of ReLU that correspond with that element in the diagonals and <span class="math">\(0\)</span> for all the other values.</p>
<div class="math">$$
\dfrac{\partial \textbf{a}^{[13]}}{\partial \textbf{z}^{[13]}} = 
\begin{bmatrix}
g'(z^{[13]}_{(1,1,1)}) &amp; 0 &amp; \dots &amp; 0 &amp; 0 \\\\
0 &amp; g'(z^{[13]}_{(2,1,1)}) &amp; \dots &amp; 0 &amp; 0 \\\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\\\
0 &amp; 0 &amp; \dots &amp; g'(z^{[13]}_{(13,14,512)}) &amp; 0 \\\\
0 &amp; 0 &amp; \dots &amp; 0 &amp; g'(z^{[13]}_{(14,14,512)}) \\\\
\end{bmatrix}
$$</div>
<p>Great, so next we focus our attention on calculating the partial derivative of <span class="math">\(\mathcal{L}\)</span> with respect to <span class="math">\(\textbf{W}_c^{[13]}\)</span>. Using the chain rule:</p>
<div class="math">$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{W}_c^{[13]}} = 
\dfrac{\partial \mathcal{L}}{\partial \textbf{z}^{[13]}} 
\dfrac{\partial \mathcal{\textbf{z}^{[13]}}}{\partial \textbf{W}^{[13]}}
$$</div>
<p>We just calculated the partial derivative of <span class="math">\(\mathcal{L}\)</span> with respect to <span class="math">\(\textbf{z}^{[13]}\)</span> and focus on calculating the partial derivative of <span class="math">\(\textbf{z}^{[13]}\)</span> with respect to <span class="math">\(\textbf{W}_c^{[13]}\)</span>. This derivative takes it's first dimension from the values in <span class="math">\(\textbf{z}^{[13]}\)</span> <span class="math">\(7 * 7 * 512 = 25088\)</span> and second dimension from values in <span class="math">\(\textbf{W}_c^{[13]}\)</span> <span class="math">\(3 * 3 * 512 * 512 = 2359296\)</span>. Its dimensions are therefore <span class="math">\((25088 , 2359296)\)</span>. Again, we can think of getting the values for each indexed partial derivatives using the <span class="math">\(vec()\)</span> function.</p>
<p>Let's deconstruct the derivative for <span class="math">\(z^{[13]}_{(1,1,1)}\)</span>. Since this is from the first channel of <span class="math">\(\textbf{z}^{[13]}\)</span>, we use the first filter channel in <span class="math">\(\textbf{W}_c^{[13]}\)</span>. The calculation is as follows:</p>
<div class="math">$$z^{[13]}_{(1,1,1)} =
s^{[12]}_{(1, 1, 1)}W^{[13]}_{c(1,1,1,1)} + 
s^{[12]}_{(2, 1, 1)}W^{[13]}_{c(2,1,1,1)} + 
s^{[12]}_{(3, 1, 1)}W^{[13]}_{c(3,1,1,1)} +
$$</div>
<div class="math">$$ 
s^{[12]}_{(1, 2, 1)}W^{[13]}_{c(1,2,1,1)} + 
s^{[12]}_{(2, 2, 1)}W^{[13]}_{c(2,2,1,1)} + 
s^{[12]}_{(3, 2, 1)}W^{[13]}_{c(3,2,1,1)} +
$$</div>
<div class="math">$$ 
s^{[12]}_{(1, 3, 1)}W^{[13]}_{c(1,3,1,1)} + 
s^{[12]}_{(2, 3, 1)}W^{[13]}_{c(2,3,1,1)} + 
s^{[12]}_{(3, 3, 1)}W^{[13]}_{c(3,3,1,1)} + \dots
$$</div>
<p>These are the calculations when we multiply the first channel of the first filter with the same padded input <span class="math">\(\textbf{s^{[12]}}\)</span>. Next, we just move to the next channel of the first filter, and the third dimension goes from <span class="math">\(1 \rightarrow 2\)</span>. We repeat this process for the 512 channels.</p>
<div class="math">$$
\dots + 
s^{[12]}_{(1, 1, 2)}W^{[13]}_{c(1,1,2,1)} + 
s^{[12]}_{(2, 1, 2)}W^{[13]}_{c(2,1,2,1)} + 
s^{[12]}_{(3, 1, 2)}W^{[13]}_{c(3,1,2,1)} +
$$</div>
<div class="math">$$ 
s^{[12]}_{(1, 2, 2)}W^{[13]}_{c(1,2,2,1)} + 
s^{[12]}_{(2, 2, 2)}W^{[13]}_{c(2,2,2,1)} + 
s^{[12]}_{(3, 2, 2)}W^{[13]}_{c(3,2,2,1)} +
$$</div>
<div class="math">$$ 
s^{[12]}_{(1, 3, 2)}W^{[13]}_{c(1,3,2,1)} + 
s^{[12]}_{(2, 3, 2)}W^{[13]}_{c(2,3,2,1)} + 
s^{[12]}_{(3, 3, 2)}W^{[13]}_{c(3,3,2,1)} + \dots
$$</div>
<p>And then eventually we reach the channel number <span class="math">\(512\)</span> in the first filter:</p>
<div class="math">$$
\vdots
$$</div>
<div class="math">$$ 
s^{[12]}_{(1, 3, 512)}W^{[13]}_{c(1,3,512,1)} + 
s^{[12]}_{(2, 3, 512)}W^{[13]}_{c(2,3,512,1)} + 
s^{[12]}_{(3, 3, 512)}W^{[13]}_{c(3,3,512,1)} + b^{[1]}_{(1,1)}
$$</div>
<p>Recall from the blog post on <a href="/vgg_forwardprop">VGG16 forward propagation</a> that <span class="math">\(\textbf{s}^{[12]}\)</span> is the activation from the previous layer padded with one border of zeros <span class="math">\(p = 1\)</span> using same padding.</p>
<p>What happens when we take the partial derivative of <span class="math">\(z^{[13]}_{(1,1,1)}\)</span> with respect to <span class="math">\(W^{[13]}_{c(1,1,1,1)}\)</span>? Notice we just get the value for the padding layer <span class="math">\(s^{[13]}_{(1, 1, 1)}\)</span> and everything else is equal to 0. </p>
<div class="math">$$\dfrac{\partial z^{[13]}_{(1,1,1)}}{\partial W^{[13]}_{c(1,1,1,1)}} = s^{[12]}_{(1, 1, 1)} + 0 + 0 + ... + 0$$</div>
<div class="math">$$\dfrac{\partial z^{[13]}_{(1,1,1)}}{\partial W^{[13]}_{c(1,1,1,1)}} = s^{[12]}_{(1, 1, 1)}$$</div>
<p>So notice that the first row of the Jacobian Matrix will have <span class="math">\(3 * 3 * 512 = 4608\)</span> nonzero elements, which correspond to the values multiplied by the weights in the filter. Notice that this is a very sparse row, since there are a total of <span class="math">\(2359296\)</span> elements in the row.</p>
<p>How do we know which elements of the Jacobian Matrix are nonzero and which ones are zero? Can we define the derivative a little better than just saying the values in our padded layer? Yes, we can but it's going to require a little bit of matrix algebra. </p>
<p>There's a <a href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=AC39B5F4E59E77079125BB5FC6DFDF71?doi=10.1.1.727.1832&amp;rep=rep1&amp;type=pdf">great resource</a> for understanding how backpropagation works for a convolutional layer. We'll draw a lot from that resource for this explanation.</p>
<h3>im2col Operation</h3>
<p>Let's consider a simpler convolution scenario where we have an input <span class="math">\(\textbf{s}^{[12]}\)</span> with dimensions <span class="math">\((3,3,3)\)</span>, output <span class="math">\(\textbf{z}^{[13]}\)</span> with dimensions <span class="math">\((2,2,2)\)</span>, with a filter <span class="math">\(\textbf{W}_c^{[13]}\)</span> with dimensions <span class="math">\((2,2,3,2)\)</span>. We use a stride size of <span class="math">\(1\)</span> and no padding.</p>
<p>Could we somehow reshape <span class="math">\(\textbf{s}^{[12]}\)</span> and <span class="math">\(\textbf{W}_c^{[13]}\)</span> in such a way that a simple matrix multiplication would yield <span class="math">\(\textrm{vec}(\textbf{z}^{[13]})\)</span>? Remember that <span class="math">\(\textrm{vec}(\textbf{z}^{[13]})\)</span> is just <span class="math">\(\textbf{z}^{[13]}\)</span> represented as a column vector with dimensions <span class="math">\((2*2*2 = 8, 1)\)</span>). We can using Matlab's function called <code>im2col</code>.</p>
<p>First, let's define the three channels of <span class="math">\(\textbf{s}^{[12]}\)</span>:</p>
<div class="math">$$
\textbf{s}^{[12]}_{(1)} = 
\begin{bmatrix}
1 &amp; 3 &amp; 4 \\\\
0 &amp; 2 &amp; 6 \\\\
5 &amp; 1 &amp; 9 \\\\
\end{bmatrix}
\hspace{1cm}
\textbf{s}^{[12]}_{(2)} = 
\begin{bmatrix}
4 &amp; 1 &amp; 5 \\\\
0 &amp; 0 &amp; 1 \\\\
2 &amp; 6 &amp; 7 \\\\
\end{bmatrix}
\hspace{1cm}
\textbf{s}^{[12]}_{(3)} = 
\begin{bmatrix}
8 &amp; 9 &amp; 1 \\\\
3 &amp; 1 &amp; 5 \\\\
6 &amp; 2 &amp; 2
\end{bmatrix}
$$</div>
<p>If we used the function <code>im2col</code> with a stride size of 1, we would get the following result for the first column:</p>
<div class="math">$$
\begin{bmatrix}
1 \\\\
0 \\\\
3 \\\\
2 \\\\
4 \\\\
0 \\\\
1 \\\\
0 \\\\
8 \\\\
3 \\\\
9 \\\\
1
\end{bmatrix}
$$</div>
<p>Do you see what we did there? We took all of the inputs that the filter uses in the first position from all the channels, and then stacked them into a column vector. Notice that <span class="math">\(1\)</span>, <span class="math">\(0\)</span>, <span class="math">\(3\)</span>, and <span class="math">\(2\)</span> are the first <span class="math">\(4\)</span> elements in our column vector, and they come from the filter position in the first channel of <span class="math">\(\textbf{s}^{[12]}\)</span>, and so on. If we continue this process for the other convolutions, we get the following matrix:</p>
<div class="math">$$
\textrm{im2col}(\textbf{s}^{[12]}) = 
\begin{bmatrix}
1 &amp; 3 &amp; 0 &amp; 2 \\\\
0 &amp; 2 &amp; 5 &amp; 1 \\\\
3 &amp; 4 &amp; 2 &amp; 6 \\\\
2 &amp; 6 &amp; 1 &amp; 9 \\\\
4 &amp; 1 &amp; 0 &amp; 0 \\\\
0 &amp; 0 &amp; 2 &amp; 6 \\\\
1 &amp; 5 &amp; 0 &amp; 1 \\\\
0 &amp; 1 &amp; 6 &amp; 7 \\\\
8 &amp; 9 &amp; 3 &amp; 1 \\\\
3 &amp; 1 &amp; 6 &amp; 2 \\\\
9 &amp; 1 &amp; 1 &amp; 5 \\\\
1 &amp; 5 &amp; 2 &amp; 2
\end{bmatrix}
$$</div>
<p>The first dimension of <span class="math">\(\textrm{im2col}(\textbf{s}^{[12]})\)</span> is <span class="math">\(2 * 2 * 3 = 12\)</span>, which is the filter height, width, and number of channels multiplied together. The second dimension is <span class="math">\(4\)</span>, which is the number of convolutions performed times the number of channels in the output, which is also equal to the output height and width of <span class="math">\(\textbf{s}^{[12]}\)</span> (<span class="math">\(2*2\)</span>). So the final dimensions of <span class="math">\(\textbf{s}^{[12]}\)</span> is <span class="math">\((12,8)\)</span>.</p>
<p>Let's also define the three channels of the first and second filter of <span class="math">\(\textbf{W}_c^{[1]}\)</span>:</p>
<div class="math">$$
\textbf{W}^{[13]}_{c(1, 1)} = 
\begin{bmatrix}
5 &amp; 3 \\\\
0 &amp; 2
\end{bmatrix}
\hspace{1cm}
\textbf{W}^{[13]}_{(2, 1)} = 
\begin{bmatrix}
2 &amp; 3 \\\\
2 &amp; 6
\end{bmatrix}
\hspace{1cm}
\textbf{W}^{[13]}_{c(3, 1)} = 
\begin{bmatrix}
2 &amp; 7 \\\\
3 &amp; 4
\end{bmatrix}
$$</div>
<div class="math">$$
\textbf{W}^{[13]}_{c(1, 2)} = 
\begin{bmatrix}
8 &amp; 7 \\\\
3 &amp; 1
\end{bmatrix}
\hspace{1cm}
\textbf{W}^{[13]}_{(2, 2)} = 
\begin{bmatrix}
0 &amp; 0 \\\\
1 &amp; 2
\end{bmatrix}
\hspace{1cm}
\textbf{W}^{[13]}_{c(3, 2)} = 
\begin{bmatrix}
5 &amp; 5 \\\\
1 &amp; 2
\end{bmatrix}
$$</div>
<p><span class="math">\(\textbf{W}_c^{[13]}\)</span> has dimensions <span class="math">\((2,2,3,2)\)</span>. Let's flatten each of the filters and call it <span class="math">\(\textbf{F}^{[13]}\)</span>, so that each column represents one filter.</p>
<div class="math">$$
\textbf{F}^{[13]} =
\begin{bmatrix}
5 &amp; 8 \\\\
0 &amp; 3 \\\\
3 &amp; 7 \\\\
2 &amp; 1 \\\\
2 &amp; 0 \\\\
2 &amp; 1 \\\\
3 &amp; 0 \\\\
6 &amp; 2 \\\\
2 &amp; 5 \\\\
3 &amp; 1 \\\\
7 &amp; 5 \\\\
4 &amp; 2
\end{bmatrix}
$$</div>
<p><span class="math">\(\textbf{F}^{[13]}\)</span> has dimensions <span class="math">\((12, 2)\)</span>. The first dimension are the height, width, and number of channels of <span class="math">\(\textbf{W}_c^{[13]}\)</span> multiplied together (<span class="math">\(2*2*3 = 12\)</span>) and the second dimension are the number of filters in <span class="math">\(\textbf{W}_c^{[13]}\)</span>.</p>
<p>Now, if we take the transpose of <span class="math">\(\textrm{im2col}(\textbf{s}^{[12]})\)</span>, multilpy it by <span class="math">\(\textbf{F}^{[13]}\)</span>, and flatten it using <span class="math">\(\textrm{vec}\)</span>, we get <span class="math">\(\textrm{vec}(\textbf{z}^{[13]})\)</span>, which is pretty sweet.</p>
<div class="math">$$\textrm{vec}(\textbf{z}^{[13]}) = \textrm{vec}(\textrm{im2col}(\textbf{s}^{[12]})^{T}\textbf{F}^{[13]})$$</div>
<p>Let's break down the dimensions a little bit. So, the dimensions of <span class="math">\(\textrm{im2col}(\textbf{s}^{[12]})^{T}\)</span> are <span class="math">\((4, 12)\)</span> and the dimensions of <span class="math">\(\textbf{F}^{[13]}\)</span> are <span class="math">\((12, 2)\)</span>. So <span class="math">\(\textbf{s}^{[12]})^{T}\textbf{F}^{[13]}\)</span> is a matrix with dimensions <span class="math">\((4, 2)\)</span> and after flattening it, <span class="math">\(\textrm{vec}(\textrm{im2col}(\textbf{s}^{[12]})^{T}\textbf{F}^{[13]})\)</span> is a column vector with dimensions <span class="math">\((8,1)\)</span>. <span class="math">\(\textbf{z}^{[13]}\)</span> is a <span class="math">\((2,2,2)\)</span> matrix, and <span class="math">\(\textrm{vec}(\textbf{z}^{[13]})\)</span> is a <span class="math">\((8,1)\)</span> matrix. So the dimensions match up!</p>
<p>So we've succeeding in defining <span class="math">\(\textrm{vec}(\textbf{z}^{[13]})\)</span> as a simple matrix multiplication. Next up, can we use this outcome to help us calculate the partial dertivative of the loss <span class="math">\(\mathcal{L}\)</span> with respect to <span class="math">\(\textbf{W}^{[13]}_c\)</span>. Before we can do that, we need to introduce one more operation called the Kronecker product.</p>
<h3>Kronecker Product</h3>
<p>Let's say we have two matricies, <span class="math">\(\textbf{A} \in \mathbb{R}^{m x n}\)</span> and <span class="math">\(\textbf{B} \in \mathbb{R}^{p x q}\)</span>. The Kronecker product <span class="math">\(\textbf{A} \otimes \textbf{B}\)</span> is a <span class="math">\((mp x nq)\)</span> matrix and is defined as:</p>
<div class="math">$$
\textbf{A} \otimes \textbf{B} = 
\begin{bmatrix}
a_{11}\textbf{B} &amp; \dots &amp;  a_{1n}\textbf{B} \\\\
\vdots &amp; \ddots &amp; \vdots \\\\
a_{m1}\textbf{B} &amp; \dots &amp; a_{mn}\textbf{B}
\end{bmatrix}
$$</div>
<p>The Kronecker product has the following properties, which we will be useful:</p>
<div class="math">$$
(\textbf{A} \otimes \textbf{B})^T = \textbf{A}^T \otimes \textbf{B}^T
$$</div>
<div class="math">$$
\textrm{vec}(\textbf{A}\textbf{X}\textbf{B}) = (\textbf{B}^T \otimes \textbf{A})\textrm{vec}(\textbf{X})
$$</div>
<p>The second property is valid as long as the matrix multiplication <span class="math">\(\textbf{A}\textbf{X}\textbf{B}\)</span> is valid. Looking at our previous example, we can use the second property:</p>
<div class="math">$$\textrm{vec}(\textbf{z}^{[13]}) = \textrm{vec}(\textrm{im2col}(\textbf{s}^{[12]})^{T}\textbf{F}^{[13]})$$</div>
<div class="math">$$\textrm{vec}(\textbf{z}^{[13]}) = \textrm{vec}(\textrm{im2col}(\textbf{s}^{[12]})^{T}\textbf{F}^{[13]}\textbf{I})$$</div>
<div class="math">$$\textrm{vec}(\textbf{z}^{[13]}) = (\textbf{I} \otimes \textrm{im2col}(\textbf{a}^{[12]})^{T})\textrm{vec}(\textbf{F}^{[13]})$$</div>
<p>Great, so now we are ready to calculate the partial dertivative of the loss <span class="math">\(\mathcal{L}\)</span> with respect to <span class="math">\(\textbf{W}^{[13]}_c\)</span>.</p>
<h3>Calculating the Gradient</h3>
<p>So using the chain rule we can break the partial derivative into:</p>
<div class="math">$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{W}^{[13]}_c} =
\dfrac{\partial \mathcal{L}}{\partial \textbf{z}^{[13]}}
\dfrac{\partial \textbf{z}^{[13]}}{\partial \textbf{W}^{[13]}_c}
$$</div>
<p>Notice the following property:</p>
<div class="math">$$
\dfrac{\partial \textbf{z}^{[13]}}{\partial \textbf{W}^{[13]}_c} = 
\dfrac{\partial \textrm{vec}(\textbf{z}^{[13]})}{\partial \textrm{vec}(\textbf{W}^{[13]}_c)}
$$</div>
<p>The Jacobian Matrix doesn't care what the shape of the inputs are and so, the partial derivative is equal to the partial derivative of the tensors reshaped by <span class="math">\(\textrm{vec}\)</span>.</p>
<p>Using this logic, notice that:</p>
<div class="math">$$
\dfrac{\partial \textrm{vec}(\textbf{z}^{[13]})}{\partial \textrm{vec}(\textbf{W}^{[13]}_c)} =
\dfrac{\partial \textrm{vec}(\textbf{z}^{[13]})}{\partial \textrm{vec}(\textbf{F}^{[13]})}
$$</div>
<p>Since <span class="math">\(\textrm{vec}(\textbf{W}^{[13]}_c) = \textrm{vec}(\textbf{F}^{[13]})\)</span>, the partial derivatives will be equal to each other. Taking the partial derivative:</p>
<div class="math">$$
\dfrac{\partial \textbf{z}^{[13]}}{\partial \textbf{W}^{[13]}_c} =
\dfrac{\partial \textrm{vec}(\textbf{z}^{[13]})}{\partial \textrm{vec}(\textbf{F}^{[13]})} = 
\dfrac{\partial \bigg( (\textbf{I} \otimes \textrm{im2col}(\textbf{s}^{[12]})^{T})\textrm{vec}(\textbf{F}^{[13]})\bigg)}
{\partial \textrm{vec}(\textbf{F}^{[13]})} $$</div>
<div class="math">$$
\dfrac{\partial \textbf{z}^{[13]}}{\partial \textbf{W}^{[13]}_c} =
\textbf{I} \otimes \textrm{im2col}(\textbf{s}^{[12]})^{T}
$$</div>
<p>And plugging back in:</p>
<div class="math">$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{W}^{[13]}_c} =
\dfrac{\partial \mathcal{L}}{\partial \textbf{z}^{[13]}}
\textbf{I} \otimes \textrm{im2col}(\textbf{s}^{[12]})^{T}
$$</div>
<p>Using the first property of the Kronecker product:</p>
<div class="math">$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{W}^{[13]}_c} =
\dfrac{\partial \mathcal{L}}{\partial \textbf{z}^{[13]}}
\big( \textbf{I} \otimes \textrm{im2col}(\textbf{s}^{[12]}) \big)^T
$$</div>
<p>And taking the transpose of both sides:</p>
<div class="math">$$
\bigg(
\dfrac{\partial \mathcal{L}}{\partial \textbf{W}^{[13]}_c}
\bigg)^T =
\textbf{I} \otimes \textrm{im2col}(\textbf{s}^{[12]})
\dfrac{\partial \mathcal{L}}{\partial \textbf{z}^{[13]}}^T
$$</div>
<p>Notice that the transpose of the partial derivative of <span class="math">\(\mathcal{L}\)</span> with respect to <span class="math">\(\textbf{z}^{[13]}\)</span> has dimensions <span class="math">\((8, 1)\)</span> and is equivalent to the <span class="math">\(\textrm{vec}\)</span> of the partial derivative. So, notice that:</p>
<div class="math">$$
\bigg(
\dfrac{\partial \mathcal{L}}{\partial \textbf{W}^{[13]}_c}
\bigg)^T =
\textbf{I} \otimes \textrm{im2col}(\textbf{s}^{[12]})
\textrm{vec}\bigg(\dfrac{\partial \mathcal{L}}{\partial \textbf{z}^{[13]}}\bigg)
$$</div>
<p>Let's really quickly review the dimensions of each of the matricies. <span class="math">\(\textbf{I}\)</span> has dimenensions <span class="math">\((2,2)\)</span>. <span class="math">\(\textrm{im2col}(\textbf{s}^{[12]})\)</span> has dimensions <span class="math">\((12,4)\)</span>. The partial derivative of <span class="math">\(\mathcal{L}\)</span> </p>
<p><span class="math">\(\frac{\partial \mathcal{L}}{\partial \textbf{z}^{[13]}}\)</span> has dimensions $</p>
<p>And using the second property of the Kronecker product defined above:</p>
<div class="math">$$
\bigg(
\dfrac{\partial \mathcal{L}}{\partial \textbf{W}^{[13]}_c}
\bigg)^T =
\textrm{vec}\bigg(
\textrm{im2col}(\textbf{s}^{[12]})
\dfrac{\partial \mathcal{L}}{\partial \textbf{z}^{[13]}}
\textbf{I}
\bigg) 
$$</div>
<div class="math">$$
\bigg(
\dfrac{\partial \mathcal{L}}{\partial \textbf{W}^{[13]}_c}
\bigg)^T =
\textrm{vec}\bigg(
\textrm{im2col}(\textbf{s}^{[12]})
\dfrac{\partial \mathcal{L}}{\partial \textbf{z}^{[13]}}
\bigg) 
$$</div>
<p>Notice that the partial derivative of <span class="math">\(\mathcal{L}\)</span> with respect to <span class="math">\(\textbf{W}^{[13]}_c\)</span> has dimensions <span class="math">\((1, 24)\)</span>. So, notice that:</p>
<div class="math">$$
\bigg(
\dfrac{\partial \mathcal{L}}{\partial \textbf{W}^{[13]}_c}
\bigg)^T = 
\textrm{vec}\bigg(\dfrac{\partial \mathcal{L}}{\partial \textbf{W}^{[13]}_c}\bigg)
$$</div>
<p>And so we get:</p>
<div class="math">$$
\textrm{vec}\bigg(\dfrac{\partial \mathcal{L}}{\partial \textbf{W}^{[13]}_c}\bigg) =
\textrm{vec}\bigg(
\textrm{im2col}(\textbf{s}^{[12]})
\dfrac{\partial \mathcal{L}}{\partial \textbf{z}^{[13]}}
\bigg) 
$$</div>
<p>Removing the <span class="math">\(\textrm{vec}\)</span> operation from both sides, we finally get:</p>
<div class="math">$$
\dfrac{\partial \mathcal{L}}{\partial \textbf{W}^{[13]}_c} = \textrm{im2col}(\textbf{s}^{[12]})
\dfrac{\partial \mathcal{L}}{\partial \textbf{z}^{[13]}}
$$</div>
<p>Wow, that's a beautiful result! If we reshape the partial derivative into the shape of <span class="math">\(\textbf{W}^{[13]}_c\)</span>, we will get the gradient <span class="math">\(d\textbf{W}^{[13]}_c\)</span>.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
        </div>

        <div class="meta">
            <div>
                    <a href="http://www.jasonosajima.com/tag/neural-networks.html" class="tag">neural networks</a>
                    <a href="http://www.jasonosajima.com/tag/machine-learning.html" class="tag">machine learning</a>
                    <a href="http://www.jasonosajima.com/tag/convolutional-networks.html" class="tag">convolutional networks</a>
                    <a href="http://www.jasonosajima.com/tag/vgg.html" class="tag">VGG</a>
            </div>
        </div>
    </article>


</div>

<style type="text/css">
{
    max-width: 700px;
}

.text_cell .prompt {
    display: none;
}

div.cell {
    padding: 0;
}

div.text_cell_render {
    padding: 0;
}

div.prompt {
    font-size: 13px;
}

div.input_prompt {
    padding: .7em 0.2em;
}

div.output_prompt {
    padding: .4em .2em;
}

div.input_area {
    margin: .2em 0.4em;
    max-width: 580px;
}

table.dataframe {
    font-family: Arial, sans-serif;
    font-size: 13px;
    line-height: 20px;
}

table.dataframe th, td {
    padding: 4px;
    text-align: left;
}

pre code {
    background-color: inherit;
}</style>

        </div>
<!--
    <footer>
      <p>
        © 2012-2017 Jason Osajima, license <a href=""> </a>
        unless otherwise noted.
        Generated by <a href= "http://docs.getpelican.com/">Pelican</a>.
      </p>
    </footer>
-->
    </body>
</html>